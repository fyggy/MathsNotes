---
tags:
  - chapter
  - linear_algebra1
date: 2023-11-06
---
[[Directory]], [[Linear Algebra and Geometry 1|Subject Directory]]
# Linear Maps
## Introduction
### Definition 1.1: Linear Maps
Let $V$ and $W$ be vector spaces over a field $\mathbb{F} {}$. Then a map (or transformation, or even operator, strictly a function) ${} T:V\to{}W {}$ is linear if 
1. For all ${} \mathbf{u},\, \mathbf{v} \in V {}$, ${} T(\mathbf{u}+\mathbf{v})=T(\mathbf{u})+T(\mathbf{v})$.
2. For all $\alpha \in \mathbb{F} {}$, and ${} \mathbf{v} \in V {}$, $T(\alpha \mathbf{v})=\alpha T(\mathbf{v}) {}$.

These conditions can be combined into a single condition:
1. For all ${} \mathbf{u}, \mathbf{v} \in V {}$, and ${} \alpha,\, \beta \in \mathbb{F} {}$, we have $T(\alpha \mathbf{u}+\beta \mathbf{v})=\alpha T(\mathbf{u})+\beta T(\mathbf{v})$.
#### Observation:
If $T:V\to{}W {}$ is linear, then by $(2)$, we have
$$
T(\alpha\cdot \mathbf{0})=\alpha\cdot T(\mathbf{0})\quad\forall \alpha \in \mathbb{F}
$$
But
$$
T(\alpha\cdot \mathbf{0})=T(\mathbf{0})
$$
so
$$
\begin{align}
\forall \alpha \in  \mathbb{F}: T(\mathbf{0})=\alpha T(\mathbf{0})
\end{align}
$$
Therefore
$$
T(\mathbf{0})=\mathbf{0}
$$
Usually, this is the first thing to check to see if a transformation $T$ is linear
### Examples:
1. 
The identity map
${} I:V\to{}V {}$ given by $I(\mathbf{v})=\mathbf{v}$ for all $\mathbf{v} \in  V$. Clearly, this is linear, as 
$$
T(\alpha \mathbf{v}+\beta \mathbf{w})=\alpha \mathbf{v}+\beta \mathbf{w}=\alpha T(\mathbf{v})+\beta T(\mathbf{w})
$$
2. The zero map
$T:V\to{}W {}$ given by ${} T(\mathbf{v})=\mathbf{0} {}$ for all ${} \mathbf{v} \in V$
This is also linear, as
$$
T(\alpha \mathbf{v}+\beta \mathbf{w})=\mathbf{0}=\alpha \mathbf{0}+\beta \mathbf{0}=\alpha T(\mathbf{v})+\beta T(\mathbf{w})
$$
3. Reflection
${} T:\mathbb{R}^{2}\to{}\mathbb{R}^{2} {}$ given by reflecting in the $x$-axis:
$$
T\begin{pmatrix} x \\ y \end{pmatrix} =\begin{pmatrix} x \\ -y \end{pmatrix} 
$$
This is linear. For all ${} \mathbf{v}=(x_{1},\, y_{1}) {}$ and ${} \mathbf{w}=(x_{2},\, y_{2}) {}$, we have
$$
T(\mathbf{v}+\mathbf{w})=T \begin{pmatrix} x_{1}+x_{2} \\ y_{1}+y_{2} \end{pmatrix} =\begin{pmatrix} x_{1}+x_{2} \\ -(y_{2}+y_{2}) \end{pmatrix} =\begin{pmatrix} x_{1} \\ -y_{1} \end{pmatrix} +\begin{pmatrix} x_{2} \\ -y_{2} \end{pmatrix} =T(\mathbf{v})+T(\mathbf{w})
$$
For all ${} \mathbf{v}=(x,\, y) {}$ and ${} \alpha \in  \mathbb{F} {}$, we have
$$
T(\alpha \mathbf{v})=T \begin{pmatrix} \alpha x \\ \alpha y \end{pmatrix} =\begin{pmatrix} \alpha \\ -\alpha y \end{pmatrix} =\alpha\begin{pmatrix} x \\ -y \end{pmatrix} =\alpha T(\mathbf{v})
$$
4. Scaling
Let ${} \beta \in  \mathbb{F}$. Let $T:V\to{}V {}$ be given by ${} T(\mathbf{v})=\beta \mathbf{v} {}$. This is linear, as for all ${} \alpha,\, \gamma \in \mathbb{F} {}$ and ${} \mathbf{u},\, \mathbf{v} \in  V {}$, we have
$$
	T(\alpha \mathbf{u}+\gamma \mathbf{v})=\beta(\alpha \mathbf{u}+\gamma \mathbf{v})=\alpha\beta \mathbf{u}+\gamma\beta \mathbf{v}=\alpha T(\mathbf{u})+\gamma T(\mathbf{v})
$$
5. Translations
Fix ${} \mathbf{a}\in V {}$, $\mathbf{a}\neq \mathbf{0}$. Define ${} T: V\to{}V {}$ by $T(\mathbf{v})=\mathbf{v}+\mathbf{a} {}$. This is ***not*** linear, as
$$
T(\mathbf{0})=\mathbf{a}\neq \mathbf{0}
$$
6. Differentiation
Let ${} \mathbb{P} {}$ be the vector space of all polynomials. Let ${} D:P\to{}P {}$ be given by, for some ${} f \in  \mathbb{P}$, $D(f)=f'. {}$This is linear. For all ${} f, g \in \mathbb{P} {}$, and $\alpha \in  \mathbb{F}$, we have
$$
D(f+g)=\frac{d}{dt} (f+g)=\frac{df}{dt} +\frac{dg}{dt} =D(f)+D(g)
$$
and
$$
D(\alpha f)=\frac{d}{dt} (\alpha f)=\alpha \frac{df}{dt} =\alpha D(f)
$$
7. "Squaring"
Define ${} T:\mathbb{R}^{2}\to{}\mathbb{R}^{2}$ by ${} T \begin{pmatrix} x \\ y \end{pmatrix} =\begin{pmatrix} x^{2} \\ y^{2} \end{pmatrix}  {}$. This is ***not*** linear. Take $\alpha \in  \mathbb{R}$ and $\mathbf{v} \in  (x,\, y) \in  \mathbb{R}^{2}$. Then we have
$$
T(\alpha \mathbf{v})=T \begin{pmatrix} \alpha x \\ \alpha y \end{pmatrix} =\begin{pmatrix} (\alpha x)^{2} \\ (\alpha y)^{2} \end{pmatrix} =\alpha^{2}\begin{pmatrix} x^{2} \\ y^{2} \end{pmatrix} =\alpha^{2}T(\mathbf{v})\neq\alpha T(\mathbf{v})
$$
In general. 
## Linear Maps and Bases
Let ${} T:V\to{}W {}$ be linear. Let ${} \mathbf{e}_{1},\, \mathbf{e}_{2},\,\dots,\,\mathbf{e}_{n} {}$ be a basis of $V$. Suppose we know $T(\mathbf{e}_{1}),\, T(\mathbf{e}_{2}),\,\dots,\,T(\mathbf{e}_{n})$. Let ${} \mathbf{v}\in V {}$. Then we have
$$
\mathbf{v}=\alpha_{1}\mathbf{e}_{1}+\alpha_{2}\mathbf{e}_{2}+\dots+\alpha_{n}\mathbf{e}_{n}
$$
Now we have
$$
\begin{align}
 T(\mathbf{v}) & =T(\alpha_{1}\mathbf{e}_{1}+\dots+\alpha_{n}\mathbf{e}_{n})   \\
 & =T(\alpha_{1}\mathbf{e}_{1})+\dots+T(\alpha_{n}\mathbf{e}_{n}) \\
 & =\alpha_{1}T(\mathbf{e}_{1})+\dots+\alpha_{n}T(\mathbf{e}_{n})
 \end{align}
$$
So we now know the value of ${} T(\mathbf{v}) {}$ for all $\mathbf{v} \in  V$, as there we're no conditions placed on $\mathbf{v}$ above.
In particular
### Proposition 2.1
Let $T:V\to{}W$ and ${} S:V\to{}W {}$ be linear maps, with ${} \mathbf{e}_{1},\, \mathbf{e}_{2},\,\dots,\,\mathbf{e}_{n} {}$ being a basis of $V {}$. If we have
$$
T(\mathbf{e}_{1})=S(\mathbf{e}_{1}),\, \quad T(\mathbf{e}_{2})=S(\mathbf{e}_{2}),\, \quad\dots ,\, \quad T(\mathbf{e}_{n})=S(\mathbf{e}_{n})
$$
Then $T=S {}$.
#### Proof:
Let $\mathbf{v} \in V$. Therefore, there exist scalars ${} \alpha_{1},\,\dots,\,\alpha_{n} {}$ such that
$$
\mathbf{v}=\alpha_{1}\mathbf{e}_{1}+\alpha_{2}\mathbf{e}_{2}+\dots+\alpha_{n}\mathbf{e}_{n}
$$
Now we have
$$
\begin{align}
 T(\mathbf{v}) & =T(\alpha_{1}\mathbf{e}_{1}+\dots+\alpha_{n}\mathbf{e}_{n})  \\
 & = T(\alpha_{1}\mathbf{e}_{1})+\dots+T(\alpha_{n}\mathbf{e}_{n}) \\
 & =\alpha_{1}T(\mathbf{e}_{1})+\dots+\alpha_{n}T(\mathbf{e}_{n}) \\
 & =\alpha_{1}S(\mathbf{e}_{1})+\dots+\alpha_{n}S(\mathbf{e}_{n}) \\
 & =S(\alpha_{1}\mathbf{e}_{1})+\dots+S(\alpha_{n}\mathbf{e}_{n}) \\
 & =S(\alpha_{1}\mathbf{e}_{1}+\dots+\alpha_{n}\mathbf{e}_{n}) \\
 & =S(\mathbf{v})
 \end{align}
$$
As required.
## Matrix of a Linear Map
### Linear Maps ${} \mathbb{F}^{n}\to{}\mathbb{F}^{m} {}$
Let ${} \mathbf{e}_{1},\, \mathbf{e}_{2},\,\dots,\,\mathbf{e}_{n} {}$ be the standard basis in ${} \mathbb{F}^{n} {}$. Let ${} T: \mathbb{F}^{n}\to{}\mathbb{F}^{m} {}$ be a linear map. Let
$$
T(\mathbf{e}_{1})=\begin{pmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1} \end{pmatrix} ,\, T(\mathbf{e}_{2})=\begin{pmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m 2} \end{pmatrix} ,\,\dots,\, T(\mathbf{e}_{n})= \begin{pmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn} \end{pmatrix}
$$
Really, these vectors contain all the information of $T$, by ${} 2.1 {}$. So, we *adjoin* them into a matrix $A: {}$
$$
A=\begin{pmatrix}
\uparrow & \uparrow &  & \uparrow \\
	T(\mathbf{e}_{1}) & T(\mathbf{e}_{2}) & \dots & T(\mathbf{e}_{n}) \\
\downarrow & \downarrow &  & \downarrow
\end{pmatrix}
=\begin{pmatrix}
a_{11} & a_{12} & \dots & a_{1 n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots &  \\
a_{m1} & a_{m 2} & \dots & a_{mn}
\end{pmatrix}
$$
We call this the matrix of $T$. We want ${} T(\mathbf{v}) {}$ to be the product $A\mathbf{v} {}$. 
If ${} T(\mathbf{v})=A\mathbf{v} {}$ for ${} \mathbf{v}=(v_{1},\, v_{2},\, \dots,\, v_{n}) {}$, then
$$
\begin{align}
A\mathbf{v} & =T(v_{1}\mathbf{e}_{1}+v_{2}\mathbf{e}_{2}+\dots+v_{n}\mathbf{e}_{n}) \\
 & = v_{1} T(\mathbf{e}_{1})+v_{2}T(\mathbf{e}_{2})+\dots+v_{n}T(\mathbf{e}_{n}) \\
 & =v_{1} \begin{pmatrix} a_{11} \\ a_{21} \\ \vdots  \\ a_{m1} \end{pmatrix} +v_{2} \begin{pmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m 2} \end{pmatrix} +\dots+v_{n} \begin{pmatrix} a_{1 n} \\ a_{2 n} \\ \vdots \\ a_{mn} \end{pmatrix}  \\
 & = \begin{pmatrix} v_{1}a_{11}+v_{2}a_{12}+\dots+v_{n}a_{1n} \\ v_{1}a_{21}+v_{2}a_{22}+\dots+v_{n}a_{2n} \\ \vdots \\ v_{1}a_{m_{1}}+v_{2}a_{m 2}+\dots+v_{n}a_{mn} \end{pmatrix} 
\end{align}
$$
Hence we compute the product $A\mathbf{v}$ according to the (vauge) rule
$$
k^{\text{th}} \text{ entry of } A\mathbf{v}=(k^{\text{th}} \text{ row of } A)\cdot V
$$
or more precisely
$$
(A\mathbf{v})_{k}=\sum_{j=1}^{n} a_{kj}v_{j}
$$
(we denote the ${} k^{\text{th}} {}$ entry of a vector $\mathbf{v} {}$ as $(v)_{k}$)
#### Remark
We can only multiply an $m\times n$ matrix with a vector of length $n$
#### Notation:
Since applying a linear map is essentially a multiplication, we usually write $T\mathbf{v} {}$ instead of $T(\mathbf{v})$ (we will see later this is because every linear map has a matrix representation)