---
tags:
  - chapter
  - linear_algebra1
date: 2023-11-13
---
[[Directory]], [[Linear Algebra and Geometry 1]]
# Operations on Linear Maps
## Composition
Let ${} T: \mathbb{F}^{n}\to{}\mathbb{F}^{m} {}$, and ${} S: \mathbb{F}^{m}\to{}\mathbb{F}^{k} {}$ (note that the range of $T {}$ is the domain of $S {}$)
If ${} T,\, S {}$ are both linear, then ${} S\circ T: \mathbb{F}^{n}\to{}\mathbb{F}^{k} {}$ is also linear. That is, this diagram commutes:
![[Pasted image 20231113123058.png|450]]
Let ${} A {}$ be the matrix of $S$ and $B$ be the matrix of $T$, with respect to the standard basis. Then $A$ is a ${} k \times m {}$ matrix and $B$ is a ${} m\times n$ matrix

For ${} \mathbf{v}\in \mathbb{F}^{n} {}$, we have $S\circ T(\mathbf{v})=S(B\mathbf{v})=A(B\mathbf{v})$. So conceptually, we want to define a multiplication between the 2 matrices $A$ and $B$ such that $AB$ is the matrix of $S\circ T {}$.
### Definition 1.1: Matrix Multiplication
Let 
$$
B=\begin{pmatrix}
\uparrow & \uparrow &  & \uparrow \\
\mathbf{b}_{1} & \mathbf{b}_{2} & \dots & \mathbf{b}_{n} \\
\downarrow & \downarrow &  & \downarrow
\end{pmatrix}
$$
So $\mathbf{b}_{i}=T\mathbf{e}_{i}$ (with $\mathbf{e}_{1},\,\dots,\,\mathbf{e}_{n}$ as the standard basis)
Then $S\circ T(\mathbf{e}_{i})=S(\mathbf{b}_{i})=A\mathbf{b}_{i}$. So
$$
[S\circ T]=\begin{pmatrix}
\uparrow & \uparrow &  & \uparrow \\
A\mathbf{b}_{1} & A\mathbf{b}_{2} & \dots & A\mathbf{b}_{n} \\
\downarrow & \downarrow &  & \downarrow
\end{pmatrix} = AB
$$

We compute this product $AB$ by the rule:
$$
\begin{align}
 (AB)_{ij} & =(i^{\text{th}}\text{ row of } A)\cdot (j^{\text{th}} \text{ column of } B)  \\
 & =a_{i 1}b_{1}+a_{i 2}b_{2j}+\dots+a_{im}b_{mi}  \\
& = \sum_{k=1}^{m} a_{ik}b_{kj}
 \end{align}
$$

#### Example
$\begin{pmatrix}1 & 2 & 3 \\ 1 & -1 & 0 \end{pmatrix} \begin{pmatrix}0 & -1 \\ 1 & 1 \\ 2 & 3 \end{pmatrix} =\begin{pmatrix}1\cdot 0+2\cdot 1+2\cdot 3 & 1\cdot (-1)+2\cdot 1+3\cdot 3 \\ 1\cdot 0+(-1)\cdot 1+0\cdot 2 & 1\cdot (-1)+(-1)\cdot 1+0\cdot 3 \end{pmatrix} =\begin{pmatrix}8 & 10 \\ -1 & -2 \end{pmatrix} {}$
#### Remark: 
For ${} AB {}$ to be defined we need the number of columns of ${} A {}$ to be the same as the number of rows of $B {}$

### Properties of Matrix Multiplication
In the following properties, we hold that the products are defined
For all ${} A,\, B,\, C {}$ matrices
1. ${} A(BC)=(AB)C {}$ (associativity). 
2. ${} A(B+C)=AB+AC {}$ (distributivity). 
3. ${} A(\alpha B)=(\alpha A)B=\alpha(AB) {}$ for all $\alpha \in \mathbb{F} {}$
4. ${} (AB)^{\mathrm{T}}=B^{\mathrm{T}}A^{\mathrm{T}} {}$ (note that the order is reversed)
#### Warning:
Matrix multiplication is ***<u>not</u>*** commutative, almost always. Given 2 random matrices, they are not commutative with probability 1.
## Invertibility
Let $V$ be a vector space and let ${} I_{V}: V\to{}V {}$ be the identity map of ${} V {}$. For some ${} A : V\to{}W {}$ and ${} B: W\to{}V {}$ be linear maps, we have
$$
AI_{V}=A
$$
and
$$
I_{V}B=B
$$
### Definition 2.1: The Inverse of a matrix
Let ${} A:V \to{}W {} {}$ be a linear map. $A$ is *invertible* (or *non-singular*) if there exists a linear map ${} A^{-1}: W\to{}V {}$ such that
$$
AA^{-1}=I_{W}
$$
and
$$
A^{-1}A=I_{V}
$$
#### Remark: 
If an inverse exist, it is unique
#### Warning:
It is possible that there is $B: W\to{}V$ such that ${} AB=I_{W} {}$, but ${} BA\neq I_{V}$.
However, if $AB=I_{W}$ and $CA=I_{V} {}$, then $A$ is invertible and $B=C=A^{-1}$.
## Properties of Inverse
### Proposition 2.2: 
Let $A$ and $B$ be linear maps (or more usefully, matrices) with $AB$ well defined. Then the following hold:
1. If $A$ is invertible, so it the transpose ${} A^{\mathrm{T}} {}$ and ${} ( A^{\mathrm{T}} )^{-1}=(A^{-1})^{T}$.
2. If $A,\, B$ are both invertible, so is $AB$, and ${} (AB)^{-1}=B^{-1}A^{-1} {}$
#### Proof:
1. 
Recall we have ${} (AB)^{T}=B^{\mathrm{T}}A^{\mathrm{T}} {}$. Taking ${} B=A^{-1} {}$ we get
$$
\begin{align}
(A^{-1})^{\mathrm{T}}A^{\mathrm{T}}=(AA^{-1})^{\mathrm{T}}=I
\end{align}
$$
and
$$
A^{\mathrm{T}}(A^{-1})^{\mathrm{T}}=(A^{-1}A)^{\mathrm{T}}=I
$$
2. 
We check that
$$
(AB)(B^{-1}A^{-1})=A(B B^{-1})A^{-1}=AIA^{-1}=A A^{-1}=I
$$
Exact same computation again to show that
$$
(B^{-1}A^{-1})(AB)=I
$$
so
$$
( AB )^{-1}=B^{-1}A^{-1}
$$
As required. 

### Key Idea:
Let ${} A: V\to{}W {}$ be an invertible linear map
$A$ and $A^{-1}$ let us convert properties of $V$ to properties of $W$, and vice versa.

If such a $A$ exists, we say $V$ and $W$ are isomorphic. *You know what this means*. 
#### Examples:
1. 
Consider the map ${} A: \mathbb{P}_{2}\to{}\mathbb{F}^{3} {}$ given by
$$
a+bt+ct^{2} \mapsto(a,\, b,\, c)
$$
This is clearly linear. It's also clear that the map ${} A^{-1} : \mathbb{F}^{3}\to{}\mathbb{P}_{2}$ given by
$$
{} (a,\, b,\, c)\mapsto a+bt+ct^{2} {}
$$
Is the inverse of $A$. Therefore, $A {}$ is an isomorphism.
##### Intuition: 
${} \mathbb{P}_{2} {}$ and ${} \mathbb{F}^{3}$ are different representations of the same vector space. 

2. Let ${} \mathcal{E}=\{ \mathbf{e}_{1},\,\dots,\,\mathbf{e}_{n} \}$ be a basis of $V$ (so ${} \dim V=n {}$)
Then 
$$
V \ni \mathbf{v}=\sum_{k=1}^{n} \alpha_{k}\mathbf{e}_{k},\, \quad [\mathbf{v}]_{\mathcal{E}}=\begin{pmatrix} \alpha_{1} \\ \alpha_{2} \\ \vdots \\ \alpha_{n} \end{pmatrix} \in  \mathbb{F}^{n}
$$
The map ${} \mathbf{v}\mapsto[\mathbf{v}]_{\varepsilon}$ is an invertible linear map. Therefore, ${} V {}$ and $\mathbb{F}^{n} {}$ are isomorphic. 
### Theorem 2.3: Invertibility Preserves Bases
Let ${} A: V\to{}W {}$ be an invertible linear map, and let ${} \mathcal{E}=\{ \mathbf{e}_{1},\, \mathbf{e}_{2},\,\dots,\,\mathbf{e}_{n} \} {}$ be a basis of ${} V$. Then $A\mathbf{e}_{1},\, A\mathbf{e}_{2},\,\dots,\,A\mathbf{e}_{n} {}$ is a basis of ${} W {}$. 
Imprecisely, these vectors are the columns of the matrix of ${} A {}$, converted back to $W {}$.
#### Remark:
Applying the theorem to $A^{-1}$ shows ${} \mathbf{e}_{1},\, \mathbf{e}_{2},\,\dots,\,\mathbf{e}_{n} {}$ is a basis of $V {}$ iff ${} A\mathbf{e}_{1},\, A\mathbf{e}_{2},\,\dots,\,A\mathbf{e}_{n} {}$ is a basis of $W$
#### Proof:
We check 2 things:
1. Linear Independence:
Take ${} \alpha_{1},\,\dots,\,\alpha_{n}\in \mathbb{F} {}$ such that ${} \sum_{k=1}^{n} \alpha_{k}A\mathbf{e}_{k}=\mathbf{0} {}$ 
Applying $A^{-1} {}$ gives
$$
\begin{align}
A^{-1}=\left( \sum_{k=1}^{n} \alpha_{k}A\mathbf{e}_{k} \right)=\sum_{k=1}^{n} \alpha_{k}A^{-1}A\mathbf{e}_{k}=\sum_{k=1}^{n} \alpha_{k}\mathbf{e}_{k}=\mathbf{0}
\end{align}
$$
But ${} \mathbf{e}_{1},\,\dots,\,\mathbf{e}_{n} {}$ is a basis of $V$, so they are linearly independant. Therefore, we must have ${} \alpha_{1}=\dots=\alpha_{n}=0 {}$, as required. Therefore, ${} A\mathbf{e}_{1},\,\dots,\,A\mathbf{e}_{n} {}$ are linearly independent.
2. Spanning
Take $\mathbf{w} \in W$. then ${} A^{-1}\mathbf{w} \in V {}$. So ${} \exists \alpha_{1},\,\dots,\,\alpha_{n}\in \mathbb{F} {}$ such that 
$$
A^{-1}\mathbf{w}=\sum_{k=1}^{n} \alpha_{n}\mathbf{e}_{n}
$$
Then
$$
\mathbf{w}=\sum_{k=1}^{n} \alpha_{n}A\mathbf{e}_{n}
$$
So ${} A\mathbf{e}_{1},\,\dots,\,A\mathbf{e}_{n} {}$ span $W$
Therefore, ${} A\mathbf{e}_{1},\,\dots,\,A\mathbf{e}_{n} {}$ are a basis of $W$.
