---
tags:
  - chapter
  - linear_algebra1
date: 2023-12-08
---
[[Directory]], [[Linear Algebra and Geometry 1|Subject Directory]]
# 
## 
### Definition 1.1: Determinant
Fix ${} n \in \mathbb{N} {}$. A determinant of a matrix ${} M\in M_{n} (\mathbb{F}) {}$ is a map ${} \det:M_{n}(\mathbb{F})\to{}\mathbb{F} {}$ satisfying these basic properties:
1. Multilinearity: Linear as a function of each column separately 
2. Antisymmetry: Interchanging two columns multiplies the result by ${} -1 {}$
3. Normalisation: Output corresponding to the identity matrix is ${} 1 {}$
### Theorem 1.2:
For each ${} n \in \mathbb{N} {}$, there is a *unique* determinant.
### Proposition 1.3:
For any square matrix ${} A {}$, and if $\det {}$ is a determinant, the following hold:
1. if ${} A$ has a zero column, then ${} \det(A)=0 {}$
2. If the columns are linearly dependant, ${} \det(A)=0 {}$
3. Adding a multiple of one column to another #todo

#todo: clean this up
## Determinants of Diagonal and Triangular Matrices
### Diagonal matricies
A diagonal matrix is a matrix of the form:$$
A=\begin{pmatrix}
a_{1} & 0 & \dots & 0 \\
0 & a_{2} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & a_{n}
\end{pmatrix}
$$
With ${} a_{1},\,\dots,\,a_{n} \in \mathbb{F} {}$. Now we must have, by applying multilinearity
$$
\det(A)=a_{1} \det \begin{pmatrix}
1 & 0 & \dots & 0 \\
0 & a_{2} & \dots & 0 \\
\vdots & \vdots & \ddots & \dots \\
0 & 0 & \dots & a_{n}
\end{pmatrix}
=a_{1} a_{2} \det \begin{pmatrix}
0 & 0 & \dots & 0 \\
0 & 0 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & a_{n}
\end{pmatrix}
$$
Continuing this, we find that ${} \det (A)=a_{1} a_{2}\dots a_{n} \det(I_{n}) {}$, where ${} I_{n}$ is the nth dimensional identity matrix. Therefore, 
$$
\det (A)=a_{1}a_{2}\dots a_{n}
$$
if $A {}$ is a diagonal matrix.
### Triangular Matrices
A upper triangular matrix is a matrix of the form
$$
A= \begin{pmatrix}a_{11} & 0 & 0 & \dots & 0\\ a_{21} & a_{22}  & 0& \dots & 0\\ a_{31} & a_{32} & a_{33} & \dots & 0\\
\vdots &  \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & a_{n3} &  \dots & a_{nn} \end{pmatrix}
$$
and a lower triangular matrix is a matrix of the form
$$
A=\begin{pmatrix}a_{11} & a_{12}  & a_{13}& \dots & a_{1n} \\ 0 & a_{22}  & a_{23}& \dots & a_{2n}  \\
0 & 0 & a_{33} & \dots & a_{3n}\\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0  & 0& \dots & a_{nn} \end{pmatrix}
$$
Observe that every matrix in echelon form is an upper triangular matrix. If a matrix is either upper or lower triangular, then it is triangular
### Observations:
If $A$ is triangular
1. If ${} a_{ii}=0 {}$, $A$ is not invertible, so ${} \det(A)=0 {}$.
2. Otherwise ${} A {}$ can be made diagonal without changing diagonal entries by adding multiples of one column to another.
#### Example:
Since adding one column to another does not change the determinant, then we can do column reduction, similar to row reduction
$$
\begin{pmatrix}1 & -1 & 4 \\ 0 & 2 & 1 \\ 0 & 0 & 3 \end{pmatrix} \to{\text{ C2+C1},\, \text{ C3-4C1}} \begin{pmatrix}1 & 0 & 0 \\ 0 & 2 & 1 \\ 0 &0  & 3 \end{pmatrix} \to{\text{ C3-1/2 C2}}\begin{pmatrix}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 3
\end{pmatrix}
$$
So the determinant of a triangular matrix is the product of it's diagonal entries. 
### Elementary Column Operations
1. Interchanging two columns
2. Multiply a column by some constant $\alpha\neq 0$
3. Add a multiple of one column to another

These operations have the following effects on $\det(A) {}$:
1. Multiplies determinant by $-1 {}$
2. Multiplies determinant ${} \alpha$
3. Multiplies determinant by $1$

Let $E {}$ be a matrix corresponding to some ERO. Applying this operation to columns gives the following.
${} (EA^{\mathrm{T}})^{\mathrm{T}}=AE^{\mathrm{T}} {}$
As we perform the operations on the rows, then we swap them back to columns
### Key point:
Every elementary column operation is given by *right* multiplication by some elementary matrix.

Let $E {}$ be some ${} n\times n {}$ elementary matrix. Then since $E {}$ is some composition of elementary column operations, then ${} \exists \beta \in \mathbb{F} {}$ such that
$$
\forall A \in M_{n}(\mathbb{F}):\det(AE)=\beta \det(A)
$$
But ${} \det (IE)=\beta \det(I) {}$, so ${} \det(E)=\beta {}$. Therefore, ${} \det(AE)=\det (E)\det(A) {}$
### Theorem 1.4:
Let $A$ and $B$ be ${} n\times n {}$ matrices. Then 
$$
\det(AB)=\det (A)\det(B)
$$
(i.e. $\det {}$ is a homomorphism from ${} ( M_{n}(\mathbb{F}) )^{\times } {}$ to ${} \mathbb{F}^{\times } {}$)
#### Proof:
If ${} B {}$ is not invertible, then ${} AB$ is not invertible, so both sides are $0 {}$

Otherwise, if ${} B$ is invertible, there exist some elementary matrices ${} E_{1},\, E_{2},\,\dots,\,E_{k} {}$ such that ${} B=E_{1} E_{2}\dots E_{k} {}$. Then
$$
\begin{align}
 \det(AB) & =\det(AE_{1} E_{2} \dots E_{k})  \\
 & = \det (AE_{1} E_{2}\dots E_{k-1})\det(E_{k}) \\
 & =\det(A)\det(E_{1})\det(E_{2})\dots \det(E_{k}) \\
 & =\det(A)\det(E_{1} E_{2}\dots E_{k}) \\
 & =\det(A)\det(B)
 \end{align}
$$
Consequence, $A$ is invertible iff $\det(A) {}$