{"path":".obsidian/plugins/text-extractor/cache/474d115ac2f3784bb31d8ac8568ab990.json","text":"4CCM113A Linear Algebra and Geometry I Nazar Miheisi King’s College London 2022 2 Preface These notes were written in the Autumn term 2018 to accompany the lectures for the module 4CCM113A Linear Algebra and Gemetry I. Much of the material and presen- tation is based on the excellent book Linear algebra Done Wrong by Sergei Treil and I am grateful to him for making this book freely available. These notes were also partly inspired by the lecture notes for the previous iteration of this module Linear Methods by George Papadopolous and the book Introduction to Linear Algebra by Gilbert Strang. I would strongly recommend these sources to students of this module. I take this opportunity to thank Kwok-Wing Tsoi for careful reading of these notes and many valuable suggestions throughout their preparation. I also wish to thank the many students, in particular Tamanna Seghal and Senan Sekhon, that took the time to read through earlier drafts of the notes and pointed out typos. Nazar Miheisi, September 2019 Contents 0 Foundational Material 5 0.1 Summation Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 0.2 Complex Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1 Vectors in R n and C n 29 1.1 The space R n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 1.2 Linear combinations and span . . . . . . . . . . . . . . . . . . . . . . . . 31 1.3 Geometry of R n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 1.4 Complex vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 2 Vector Spaces 43 2.1 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 2.2 Digression. Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 2.3 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 2.4 Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 3 Linear Maps 63 3.1 Operations on linear maps . . . . . . . . . . . . . . . . . . . . . . . . . . 66 3.2 Matrix of a linear map . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 3.3 Invertible linear maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 4 Systems of Linear Equations 79 3 4 CONTENTS 4.1 Geometry of linear equations . . . . . . . . . . . . . . . . . . . . . . . . . 79 4.2 Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 4.3 Principle of linearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 4.4 Image and kernel of a linear map . . . . . . . . . . . . . . . . . . . . . . 98 4.5 Change of bases. Similarity . . . . . . . . . . . . . . . . . . . . . . . . . 103 5 Determinants 109 5.1 Geometric interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 5.2 Generalisation to n × n matrices . . . . . . . . . . . . . . . . . . . . . . . 112 5.3 Cofactor formula for A−1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 6 Introduction to eigenvalues and canonical forms 125 6.1 Eigenvalues and eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . 125 6.2 Canonical forms of 2 × 2 matrices . . . . . . . . . . . . . . . . . . . . . 128 6.3 Application to systems of ODEs . . . . . . . . . . . . . . . . . . . . . . . 133 Chapter 0 Foundational Material Notation We will use the following sets of numbers: N = The natural numbers = {1, 2, 3, 4, . . . }, Z = The integers = {. . . , −3, −2, −1, 0, 1, 2, 3, . . . }, R = The real numbers. 0.1 Summation Notation Throughout this course (and in many other courses) we will perform some standard manipulations of summations. Here we quickly review the sigma notation for summation that is used; it is necessary to be familiar with this. Let a0, a1, . . . , an be a collection of numbers. Recall that n∑ k=0 ak = a0 + a1 + · · · + an. Notice that while the summation variable (or summation index) k appears on the left hand side, once the sum is written out in full there is no appearance of k in the sum. Consequently the summation variable k is often called a ‘dummy variable’ because it does not appear in the full expression. You might think of the summation variable as a label for each of the terms on the right 5 6 CHAPTER 0. FOUNDATIONAL MATERIAL Remark 0.1.1. It is a good rule of thumb when working with sigma notations: “if in doubt, write it out”. Example 0.1.2. Evaluate the sum 5∑ k=2(−1) k(k + 1)2. Solution. 5∑ k=2(−1) k(k + 1)2 = (−1)2 × 3 2 + (−1) 3 × 4 2 + (−1)4 × 5 2 + (−1) 5 × 6 2 = 9 − 16 + 25 − 36 = −18. Exercise. Express the sum 1 − 3 2 + 54 − 7 6 + 98 − 11 10 using sigma notation. 0.1.1 Relabelling the indices Since the summation index (k in the example above) is just used to label the terms in the sum, we can choose any index variable we like, i.e n∑ k=0 ak = n∑ j=0 aj = a0 + a1 + · · · + an. i, j, k, etc. are usually popular choices for index variables, but any symbol can be used. In addition to renaming indices, you can also shift them, provided you shift the bounds to match. For example, rewriting n∑ k=1(k − 1) as n−1∑ k=0 k can make the sum more convenient to work with. 0.1. SUMMATION NOTATION 7 0.1.2 Linearity of sums Let a0, a1, . . . , an and b0, b1, . . . , bn be collections of numbers. Then n∑ k=0(ak + bk) = (a0 + b0) + (a1 + b1) + . . . (an + bn) = (a0 + a1 + . . . an) + (b0 + b1 + · · · + bn) = n∑ k=0 ak + n∑ k=0 bk. This means that sums inside sums can be split. Let C be a constant. Then n∑ k=0 Cak = Ca0 + Ca1 + . . . Can = C(a0 + a1 + . . . an) = C n∑ k=0 ak, so constant factors can be pulled out of the sum. 0.1.3 Some standard sums ‹ Sum of ones: n∑ k=1 1 = 1 + 1 + . . . 1| {z } n terms = n. ‹ Sum of integers up to n: n∑ k=1 k = 1 + 2 + · · · + n = 1 2 n(n + 1). 8 CHAPTER 0. FOUNDATIONAL MATERIAL Indeed we can check that n∑ k=1 k = 1 + 2 + · · · + n = 1 2{(1 + 2 + · · · + n) + (n + (n − 1) + · · · + 1)} = 1 2{(n + 1) + (n + 1) + · · · + (n + 1) | {z } n terms } = 1 2n(n + 1). ‹ Sum of a geometric series: n∑ k=1 rk = r − rn+1 1 − r . Indeed n∑ k=1 rk = r + r2 + · · · + rn and r n∑ k=1 rk = r2 + r3 + · · · + rn+1. Taking the diﬀerence we get (1 − r) n∑ k=1 rk = r − rn+1, and so n∑ k=1 rk = r − rn+1 1 − r . Example 0.1.3. Use the properties above to evaluate the sum 5∑ k=1(2 k − 4k + 3). 0.1. SUMMATION NOTATION 9 Solution. 5∑ k=1(2 k − 4k + 3) = 5∑ k=1 2 k + 5∑ k=1 k + 4 5∑ k=1 1 = 2 − 2 6 1 − 2 − (4 × 1 2 × 5 × 6) + (3 × 5) = 62 − 60 + 15 = 17. Exercise. Derive a formula for the sum of the ﬁrst n odd numbers. 0.1.4 Double sums Sometimes the collection of numbers that we are summing is indexed by two variables, i.e we want to sum up a00, a01 . . . , a0n, a10, a11, . . . a1n . . . am0, am1, . . . amn. Then m∑ j=0 n∑ k=0 ajk = m∑ j=0 (aj0 + aj1 + · · · + ajn) = a00 + a01 + · · · + a0n + a10 + a11 + · · · + a1n ... ... + am0 + am1 + · · · + amn. Here, as well as changing summation index, we can also swap the order of summation without changing the sum, i.e m∑ j=0 n∑ k=0 ajk = n∑ k=0 m∑ j=0 ajk. A common way of getting a double sum in a calculation occurs when we have to multiply 10 CHAPTER 0. FOUNDATIONAL MATERIAL two sums: ( n∑ k=0 ak ) ( m∑ k=0 bk ) = (a0 + a1 + . . . an)(b0 + b1 + · · · + bm) = a0(b0 + b1 + · · · + bm) + a1(b0 + b1 + · · · + bm) + · · · + an(b0 + b1 + · · · + bm) = a0b0 + a0b1 + · · · + a0bm + a1b0 + a1b1 + · · · + a1bm ... ... + anb0 + anb1 + · · · + anbm = n∑ j=0 m∑ k=0 ajbk. Observe that while the summation indices on the left hand side are the same, they are diﬀerent on the right. This is necessary because when we multiply out, we have to sum over all possible pairs of ak’s and bk’s, not just the when the indices are the same. In summary we have the identities 1 ( n∑ k=0 ak ) ( m∑ k=0 bk ) = n∑ j=0 aj m∑ k=0 bk = m∑ k=0 bk n∑ j=0 aj = n∑ j=0 m∑ k=0 ajbk. Remark 0.1.4. We can also have triple sums, quadruple sums etc. Exercise. (i) Evaluate the sum n∑ j=1 m∑ k=1 1. (ii) Show that n∑ j=1 m∑ k=1(aj + bk) = m n∑ j=1 aj + n m∑ k=1 bk. 1These identities do not generally hold for inﬁnite sums. You will do more on inﬁnite sums in Sequences and Series. 0.2. COMPLEX NUMBERS 11 0.2 Complex Numbers The real numbers R have a striking shortcoming: there are many quadratic equations that have no solutions. In particular, the equation x2 + 1 = 0 has no solution in R. This can be remedied by ‘inventing’ a new number which is a solution to this equation, and so we introduce 2 i = √−1. Remarkably, with the addition of this one number, we can solve all quadratic equations, and indeed all polynomials of any degree! Deﬁnition 0.2.1. The set of all numbers of the form a + ib, where a, b ∈ R are call the complex numbers and denoted C, i.e. C = {a + ib : a, b ∈ R}. Remark 0.2.2. 1. Note that z = −i is also a solution of z2 + 1 = 0. 2. i = √ −1, i2 = −1, i 3 = −i, i4 = 1. In particular, i 3 + i = 0 (i3 is the additive inverse of i) and i3 × i = 1 (i 3 is the multiplicative inverse of i). 3. C contains all real numbers since every real number is of the form a + i0. 4. A complex number of the form 0 + ib is called an imaginary number. 5. There is no natural way to order the complex numbers. This sometimes makes them more diﬃcult to work with than the real numbers. Deﬁnition 0.2.3. Let z = a + ib. We deﬁne Re (z) = a; this is called the real part of z Im (z) = b; this is called the imaginary part of z Note that both Re (z) and Im (z) are real numbers. 2Engineers usually use j rather than i to denote √−1. 12 CHAPTER 0. FOUNDATIONAL MATERIAL 0.2.1 Fundamental theorem of algebra As we mentioned earlier, every non-constant polynomial equation has a solution in C. In fact we can say more. Let P be a polynomial of degree n, i.e P (z) = anzn + an−1zn−1 + . . . a1z + a0, where a0, . . . , an ∈ C. Then the equation P (z) = 0 has n solutions w1, w2, . . . , wn (some of the solutions may be repeated). This means that P (z) can be factorised as P (z) = an(z − w1)(z − w2) . . . (z − wn). This fact is known as the fundamental theorem of algebra. Unfortunately the proof is beyond the scope of this course, but you are encouraged to look it up. A number system (or ﬁeld ) that satisﬁes the above property is called algebraically closed. Thus C is algebraically closed whereas R is not. 0.2.2 Arithmetic of complex numbers When manipulating complex numbers, we simply follow the rules for real numbers to- gether with the additional rule ‘i 2 = −1’. Let z = a + ib and w = c + id, where a, b, c, d ∈ R. Then we have Addition/Substraction : z ± w = (a + ib) ± (c + id) = (a ± c) + i(b ± d); Multiplication : zw = (a + ib)(c + id) = ac + ibc + iad + i 2bd = (ac − bd) + i(bc + ad). Example 0.2.4. Write each of the following complex numbers in the form a + ib. (i) (9 + i) + (11 − i) (ii) (−1 − i)(−8 + i) (iii) 2 + i 3 × 1 + 2i 5 Solution. (i) (9 + i) + (11 − i) = 20 (= 20 + 0i). (ii) (−1 − i)(−8 + i) = 8 − i + 8i − i 2 = 8 + 7i + 1 = 9 + 7i. (iii) 2 + i 3 × 1 + 2i 5 = (2 + i)(1 + 2i) 15 = 2 + 5i + 2i2 15 = 5i 15 = 1 3i. 0.2. COMPLEX NUMBERS 13 Division is a little trickier. If we want to express z w = a + ib c + id in terms of its real and imaginary parts, we need to make the denominator real. We do this by multiplying the numerator and denominator by c − id (because (c + id)(c − id) = c2 + d2 which is a real number). Hence we have Division : z w = (a + ib)(c − id) (c + id)(c − id) = ac + bd c2 + d2 + i bc − da c2 + d2 . This looks complicated but it is easy to use, as the next example shows. Example 0.2.5. Write each of the following complex numbers in the form a + ib. (i) 5 + 2i 4 − 3i (ii) i − 4 3i − 7 Solution. (i) 5 + 2i 4 − 3i = (5 + 2i)(4 + 3i) (4 − 3i)(4 + 3i) = 14 + 23i 25 = 14 25 + 23 25i. (ii) i − 4 3i − 7 = (i − 4)(−3i − 7) (3i − 7)(−3i − 7) = 31 + 5i 58 = 31 58 + 5 58i. Exercise. Write 1/i in the form a + ib. 0.2.3 Complex conjugate and modulus When dividing by c + id, it was useful to invoke the number c − id. This operation of changing the sign of the imaginary part of a complex number will prove to be useful more generally; we call this complex conjugation. Deﬁnition 0.2.6. Let z = a + ib, where a, b ∈ R. The complex conjugate of z is denoted z (or sometimes z∗), and is deﬁned by z = a − ib. 14 CHAPTER 0. FOUNDATIONAL MATERIAL Remark 0.2.7. If x ∈ R, then x = x. Properties of complex conjugation: Let z = a + ib and w = c + id, where a, b, c, d ∈ R. 1. z + w = (a + ib) + (c + id) = (a + c) + i(b + d) = (a + c) − i(b + d) = z + w. 2. zw = (a + ib)(c + id) = (ac − bd) − i(bc + ad) = (a − ib)(c − id) = z w. 3. z = a − ib = a + ib = z. 4. z + z = (a + ib) + (a − ib) = 2a = 2Re (z). This gives the formula Re (z) = 1 2(z + z). 5. z − z = (a + ib) − (a − ib) = 2ib = 2iIm (z). This gives the formula Im (z) = 1 2i (z − z). Deﬁnition 0.2.8. Let z = a + ib. The modulus or absolute value of z is denoted |z| and deﬁned by |z| = √a2 + b2 = √zz. Properties of the modulus: Let z = a + ib and w = c + id, where a, b, c, d ∈ R. 1. |z| is always a non-negative real number. 2. |z| = |z| since a2 + b2 = a2 + (−b) 2. 3. |Re (z)| ≤ |z| and |Im (z)| ≤ |z| since a2 ≤ a2 + b2 and b2 ≤ a2 + b2. 4. |zw| = |z||w| since |zw| = √zw.zw = √zwzw = √zzww = √zz√ww = |z||w|. Exercise. Compute the modulus of ( 1 + i 2 )10 . . 0.2. COMPLEX NUMBERS 15 Proposition 0.2.9. For every z, w ∈ C, |z + w| ≤ |z| + |w|. This is known as the triangle inequality. When we discuss the complex plane, we will see that this has a geometric interpretation which makes this proposition ‘obvious’, but for now we give an algebraic proof. Proof. Take z, w ∈ C. Then |z + w| 2 = (z + w)(z + w) = (z + w)(z + w) = zz + zw + wz + ww = |z| 2 + zw + wz + |w| 2. However, wz = zw and so zw + wz = 2Re (zw). Substituting back we get |z + w| 2 = |z| 2 + 2Re (zw) + |w| 2 ≤ |z| 2 + 2|zw| + |w| 2 (since Re (zw) ≤ |zw|) = |z| 2 + 2|z||w| + |w| 2 (since |zw| = |z||w|) = (|z| + |w|) 2. Hence |z + w| ≤ |z| + |w| as required. Using the conplex conjugate and modulus, we can write a more succinct formula for the quotient of two complex numbers: z w = z w w w = zw |w|2 . In particular, note that 1 w = w |w|2 . Exercise. Which of the following statements are true for all complex numbers z and w? In each case either explain why it is true or give a counterexample to show that it is false. 1. |z + w| = |z| + |w| 2. Re (z − w) = Re (z) − Re (w) 3. Im (zw) = Im (z)Im (w) 16 CHAPTER 0. FOUNDATIONAL MATERIAL 0.2.4 The complex plane or Argand diagram Recall that the real numbers R can be represented by points on a line. Analogously, the complex numbers C can be represented by points on a two dimensional plane. Speciﬁ- cally, we can represent each z = a + ib ∈ C by the point (a, b) in the Cartesian plane. When we use the Cartesian plane to represent complex numbers we refer to it as the complex plane or Argand diagram. 34 −5 −4 −3 −2 −1 1 2 3 4 5 −5 −4 −3 −2 −1 1 2 3 4 5 4 − 7 2i 3 + 4i 2 − i −5 + 2i 1 + i √2 −3 − πi Re (z) Im (z) Figure 1: The complex numbers 3 + 4i, −5 + 2i, 2 − i, 4 − 7 2i, 1 + i √2, −3 − πi plotted on the complex plane. Remark 0.2.10. The x-axis is called the real axis or real line and the y-axis is called the imaginary axis or imaginary line. The correspondence a + ib ↔ (a, b) 3This is similar to the fact that when we use a line to represent real numbers, we usually call it ‘the number line’. 4The idea of visualising complex numbers on the complex plane was ﬁrst describe by the Norwegian- Danish mathematician and cartographer Caspar Wessel in 1797 and then independently rediscovered by the amateur mathematician Jean-Robert Argand in 1806. 0.2. COMPLEX NUMBERS 17 lets us visualise many algebraic operations with complex numbers as geometric operations of the plane. ‹ Addition of complex numbers can be visualised by the parallelogram rule: the sum z + w is the fourth vertex on the parallelogram with vertices z, w and 0 (see Figure 5.1). Thus if w ∈ C, we can understand the operation of adding w as translating the complex plane by the “vector” w. z w z + w Re (z) Im (z) Figure 2: The points 0, z, w and z + w for the vertices of a parallelogram. ‹ Complex conjugation maps z = a + ib to z = a − ib. In the plane, this maps (a, b) to (a, −b). Geometrically this is reﬂection in the real axis (see Figure 3). ‹ By Pythagoras’ theorem, the modulus of z = a + ib is the the distance from z to 0. Also, for z, w ∈ C, |z − w| is the distance between z and w in the complex plane. Example 0.2.11. Find solutions z ∈ C to the equation |z + 1| = |z − 1|. Solution. This can be done using two diﬀerent ways. We present them both here. Method 1: We approach this by taking real and imaginary parts and then solving the resulting equation. 18 CHAPTER 0. FOUNDATIONAL MATERIAL z = a + ib a b z = a − ib−b | z | | z | = | z | Figure 3: Complex conjugation corresponds to reﬂection in the real axis and thus it preserves the modulus. Remember that |w| = √ ww = √c2 + d2 if w = c + id. So it might help if we solve |z + 1| 2 = |z − 1| 2 instead. Let z = a + ib so that |z + 1| 2 = (a + 1)2 + b2 and |z − 1| 2 = (a − 1) 2 + b2. Then equating these we get that (a + 1) 2 + b2 = (a − 1)2 + b2 and so we must have (a + 1) 2 = (a − 1)2 Multiplying out both sides we see that a2 + 2a + 1 = a2 − 2a + 1, which is equivalent to a = −a. Therefore a = 0. A quick check veriﬁes that any z ∈ C with Re (z) = 0 is indeed a solution. Thus the set of solutions is the imaginary axis Re (z) = 0. Method 2: We think about this geometrically rather than algebraically. Recall that for z, w ∈ C, |z − w| is the distance between z and w in the complex plane. Thus |z + 1| = |z − (−1)| is the distance between z and −1, and |z − 1| is the distance between z and 1. So we need to ﬁnd all points z that are equidistant from 1 and −1. This is the perpendicular bisector of the straight line joining 1 and −1. Hence the set of solutions is the imaginary axis Re (z) = 0. (If you do not see this straight away, draw a sketch and think about it!) Remark 0.2.12. Although the second approach above is more elegant, it is useful to be able to think about complex numbers both algebraically and geometrically. If you haven’t seen complex numbers before, it is worth trying so solve these kinds of problems both ways. 0.2. COMPLEX NUMBERS 19 Exercise. Find all solutions z ∈ C to the equation |z + 1| = 2. Draw a sketch of this set. 0.2.5 Argument of a complex number A point in the plane can also be speciﬁed by giving its distance from the origin and the angle it makes with the positive x-axis (these are known as its polar coordinates). By the correspondence a + ib ↔ (a, b), we can do the same with complex numbers. If z ̸= 0, |z| = r and z makes an angle θ with the positive real axis (see Figure 4), then Re (z) = r cos θ and Im (z) = r sin θ. Therefore z = r(cos θ + i sin θ). z r cos θ r sin θr θ Re (z) Im (z) Figure 4: If the complex number z has modulus r and makes an angle θ with the positive real axis, then z = r(cos θ + i sin θ). Remark 0.2.13. Every complex number of modulus one is of the form cos θ + i sin θ for some −π < θ ≤ π. Deﬁnition 0.2.14. Let z = r(cos θ + i sin θ). The angle θ is called the argument of z and is sometimes denoted arg z. Remark 0.2.15. Although we have deﬁned θ as “the” argument, this is misleading, it would be more accurate to say an argument. This is because if θ is an argument of z, 20 CHAPTER 0. FOUNDATIONAL MATERIAL then θ + 2π is also an argument of z. However, there is a unique value of the argument θ such that −π < θ ≤ π. This is known as the principal argument, and is usually denoted by Arg z (capital ’A’ rather than small ’a’). If z = a + ib with a, b ∈ R, the argument θ of z satisﬁes the following equation : tan(θ) = b a . Since tan(θ) is periodic with period π, the equation tan(θ) = b/a has two solutions in the interval −π < θ ≤ π. To ensure we take the correct one, we have to choose tan −1(b/a) such that 0 < θ < π if b > 0, −π < θ < 0 if b < 0, θ = 0 if b = 0 and a > 0, and θ = π if b = 0 and a < 0. It is usually easy to see which value of tan−1(b/a) to take after drawing a sketch. Exercise. Determine the principal argument of the complex number z = −3 − i √3. Understanding the geometric interpretation of multiplying or dividing complex numbers is most easily done using the modulus and argument of a complex number. However, we will delay this discussion and return to it after introducing one of the most useful formulae in mathematics: Euler’s formula. 0.2.6 Euler’s formula and polar form Recall from Calculus I that the exponential function x 7→ e x, x ∈ R, can be deﬁned as the inﬁnite series ex = ∞∑ k=0 xk k! = 1 + x + x2 2 + x3 3! + x4 4! + . . . . This deﬁnition can be extended to z ∈ C (although we don’t consider issues of conver- gence, this series does converge for all z ∈ C). Then as is the case for real numbers, we have that for arbitrary z, w ∈ C e z+w = eze w. We also have the following power series for sin(x) and cos(x): sin(x) = ∞∑ k=0 (−1)kx2k+1 (2k + 1)! = x − x3 3! + x5 5! − . . . cos(x) = ∞∑ k=0 (−1)kx2k (2k)! = 1 − x2 2! + x4 4! − . . . 0.2. COMPLEX NUMBERS 21 Theorem 0.2.16 (Euler’s formula). For every θ ∈ R, e iθ = cos θ + i sin θ. Proof. Substituting x = iθ into the series for e x gives e iθ = ∞∑ k=0 (iθ)k k! = 1 + iθ + (iθ) 2 2 + (iθ)3 3! + x4 4! + . . . . Since i 2 = −1, i 3 = −i and i4 = 1, this simpliﬁes to e iθ = 1 + iθ − θ2 2 − iθ3 3! + x4 4! + . . . = { 1 − x2 2! + x4 4! − . . . } + i {x − x3 3! + x5 5! − . . . } = cos θ + i sin θ. Remark 0.2.17. 1. When θ = 0, We have e0 = cos(0) + i sin(0) = 1 as expected. 2. When θ = π/2 we have e iπ/2 = cos(π/2) + i sin(π/2) = i. 3. When θ = π we have e iπ + 1 = 0. This is known as Euler’s identity 5. 4. For every θ ∈ R, ei(θ+2π) = eiθ – that is, the function θ → eiθ is periodic with period 2π. In particular, e 2πi = 1. We saw in the last section that every complex number z ̸= 0 can be written in the form z = r(cos θ + i sin θ). By Euler’s theorem it follows that we can write z in the form z = reiθ, where r = |z| and θ = arg z. This is known as the polar form of a complex number. Complex conjugation is easily carried out in polar form: if z = reiθ, then z = cos θ + i sin θ = cos θ − i sin θ = cos(−θ) + i sin(−θ) = re−iθ. 5This is frequently celebrated as one of the most beautiful identities in mathetics – it relates 1, e, i, π in one elegant equation. 22 CHAPTER 0. FOUNDATIONAL MATERIAL Exercise. Express the complex number z = −4 − 4i in polar form. Recall that for z ∈ C, Re (z) = 1 2(z + z) and Im (z) = 1 2i (z − z). (0.2.1) If z = eiθ then Euler’s formula states that Re (z) = cos θ and Im (z) = sin θ. We also have that z = e −iθ. Substituting these into (0.2.1) gives us the formulae cos θ = 1 2 ( eiθ + e −iθ) ; sin θ = 1 2i (e iθ − e−iθ) . Compare these with the hyperbolic functions when you meet them in Calculus I. 0.2.7 Multiplication and division using polar form Let z = reiθ and w = ρeiϕ. Then zw = reiθρeiϕ = rρeiθe iϕ = rρei(θ+ϕ). Thus |zw| = rρ = |z||w| and arg(zw) = θ + ϕ = arg z + arg w. This shows that multiplying two complex numbers involves multiplying their moduli and summing their arguments. This gives a nice geometric interpretation of multiplying by w = ρeiϕ: it corresponds to ‘stretching’ (or ‘squashing’ if |w| < 1) the complex plane outwards (away from 0) by a factor of ρ and rotating the plane anticlockwise by an angle ϕ. Similarly, if z = reiθ and w = ρeiϕ, then z w = reiθ ρeiϕ = r ρ e i(θ−ϕ). Hence \f \f \f z w \f \f \f = r ρ = |z| |w| and arg ( z w ) = θ − ϕ = arg z − arg w. Remark 0.2.18. As a rule of thumb addition and subtraction of complex numbers is simpler using the z = a + ib notation while multiplication and division are simpler in the polar notation z = reiθ. 0.2. COMPLEX NUMBERS 23 0.2.8 De Moivre’s theorem Another consequence of Euler’s formula is De Moivre’s theorem: 6 Theorem 0.2.19 (De Moivre’s theorem). For every θ ∈ R and n ∈ N, (cos θ + i sin θ)n = cos(nθ) + i sin(nθ). Proof. By Euler’s formula, (cos θ + i sin θ)n = (eiθ) n = e inθ = cos(nθ) + i sin(nθ). De Moivre’s theorem and Euler’s formula can be used to derive many trigomometric identities. For example, taking n = 2 in De Moivre’s theorem gives (cos θ + i sin θ) 2 = cos(2θ) + i sin(2θ). Then multiplying out the left hand side we get cos 2 θ + 2i cos θ sin θ − sin2 θ = cos(2θ) + i sin(2θ). The real part of the left side must equal the real part of the right side; the same is true for the imaginary parts. Equating the real and imaginary parts of each side we ﬁnd cos(2θ) = cos2 θ − sin2 θ; sin(2θ) = 2 cos θ sin θ. Thus we have derived the familiar double angle formulae. Similar identities can be derived for larger n. Exercise. Use Euler’s theorem to derive expressions for cos(θ + ϕ) and sin(θ + ϕ) in terms of cos θ, sin θ, cos ϕ and sin ϕ. 6Although we present De moivre’s theorem follows easily from Euler’s formula, it was actually proved about 30 years earlier 24 CHAPTER 0. FOUNDATIONAL MATERIAL 0.2.9 Roots of unity A root of unity is a solution in C of the equation zn = 1, (0.2.2) for some n ∈ N. For each particular n these are known as nth roots of unity. This equation has n solutions which possess a delightful symmetry in the complex plane. Let us try to ﬁnd the solutions of (0.2.2). First one notes that 1 = |zn| = |z| n. Therefore |z| = 1. This means that each solution is of the form z = e iθ for some −π < θ ≤ π. Plugging this back into the (0.2.2) gives zn = (e iθ)n = cos(nθ) + i sin(nθ) = 1. Equating real and imaginary parts we get cos(nθ) = 1 and sin(nθ) = 0. (0.2.3) Thus θ is a solution of (0.2.3) if and only if nθ is a multiple of 2π, i.e θ = 2πk n for some k ∈ Z. In summary, we have showed that z = ei2πk/n is a solution of (0.2.2) for every k ∈ Z. However, many of these solutions are actually the same. When k is a multiple of n, say k = mn, then z = e i2πmn/n = e i2πm = 1, and so all solutions with k = . . . , −3n, −2n, −n, 0, n, 2n, 3n, . . . are the same. Similarly, any two integers that diﬀer by a multiple of n give the same solution, so the only distinct solutions are z = e i2πk/n, k = 0, 1, . . . , n − 1. Geometrically, the n-th roots of unity form the vertices of a regular n-polygon, symmet- rically arranged around the origin in the complex plane, with one root at z = 1 (see Figure 5). To summarise, we have proved the following: 0.2. COMPLEX NUMBERS 25 Proposition 0.2.20. Let n be a positive integer. The equation zn = 1 has exactly n distinct roots in the complex numbers and they are z = e i2πk/n, k = 0, 1, 2, ..., n − 1. Remark 0.2.21. More generally, any z ∈ C, z ̸= 0, has n distinct n-th roots in C and they are all evenly spaced on the circle of radius |z| 1/n in the complex plane. Exercise. Find all solutions of the equation z3 = 1 + i. Figure 5: Plot of the n-th roots of unity for n = 2, 3, 4, 5, 6, 7. The unit circle (the circle of radius one) is shown as a dotted line for reference. Note that they are evenly spaced around the unit circle. Recall that the square roots of a real number x are √x and −√ x, so that the sum of the square roots of x is zero. The fact that the sum of the square roots is zero remains true for all complex numbers z. In fact, for every integer n ≥ 2 and every complex number z, the sum of the n-th roots of z is equal to zero. The symmetry of the set of n-th roots in the complex plane allows us to give a particularly elegant proof of this. Although, we prove this only for n-th roots of unity, the same proof can be adapted to prove the general case. 26 CHAPTER 0. FOUNDATIONAL MATERIAL Proposition 0.2.22. Let n be an integer with n ≥ 2. The sum of the n-th roots of unity is equal to zero, i.e. n−1∑ k=0 e 2πik/n = 0. Proof. Let ω = e 2πi/n, so that the n-th roots of unity are 1, ω, ω2, . . . , ωn−1. Let S = n−1∑ k=0 e 2πik/n = n−1∑ k=0 ωk. Observe that since ωn = ω0 = 1, we have that ωS = ω(1 + ω + · · · + ωn−1) = (ω + ω2 + · · · + ωn) = S. Since ω ̸= 1 whenever n ≥ 2, we must have S = 0. 0.2.10 Logarithms and complex powers We know that if x ∈ R and x > 0, then the equation e y = x has a unique solution which we call log x (or sometimes ln x but we won’t use this notation). Similarly, for z ∈ C, z ̸= 0 we can deﬁne log z to be a solution w of the equation e w = z. To understand this better, let w = u + iv for some u, v ∈ R and z = reiθ for some r > 0, θ ∈ R. Then if e w = z we have ew = eu+iv = eue iv = reiθ. Since |eiv| = 1, we must have r = eu. Dividing both sides by r (recall that r > 0), we see that we must also have eiv = e iθ. This happens if and only if v = θ + 2πk for some k ∈ Z. combining these we see that ew = z if and only if w = log r + iθ + i2πk for some k ∈ Z. This shows that log z has many values. Indeed if w is a solution of ew = z, then w + 2πi is also a solution. To make log z single valued, we insist that θ is the principal argument i.e. −π < θ ≤ π. 0.2. COMPLEX NUMBERS 27 Deﬁnition 0.2.23. Let z ∈ C, z ̸= 0. Then the principal logarithm of z is denoted Log z (capital ’L’) and deﬁned by Log z = log |z| + i Arg z. Remark 0.2.24. The principal logarithm is the unique value of log z such that −π < Im (log z) ≤ π. We also wish to make sense of expresions of the form zw with z, w ∈ C. We can use the logarithm to do this. For z, w ∈ C, z ̸= 0 we deﬁne zw = e w log z. Remark 0.2.25. Since log z is multi-valued so is zw. In particular, if v ∈ C is one value of zw, e2πiwv is another value. For example, in Section 0.2.9 we saw that for any z ̸= 0, z1/n has n possible values. Exercise. Find the values of i i. 28 CHAPTER 0. FOUNDATIONAL MATERIAL Chapter 1 Vectors in Rn and Cn 1.1 The space Rn The set of real numbers R can be visualised as points on a line. Similarly, pairs of real numbers (x, y) can be visualised as points on a plane (once an origin and unit of length have been ﬁxed). The set of all such pairs is denoted R 2. An element (v1, v2) of R 2 can be written as a column or row : Column vector : v = (v1 v2 ) Row vector : v = (v1 v2) or (v1, v2) We will predominantly use column vectors in this course, but sometimes write them as rows to save space. 1 The set of all triples of real numbers is denoted R3. As before we will consider these as columns of length 3: v =  v1 v2 v3   . Elements of R 3 can be visualised as points or arrows in ‘3-d space’ (see Figure 1.2). Although we can no longer draw a picture, there is nothing to prevent us from considering quadruples of real numbers, quintuples etc. 1Although there is no mathematical reason to prefer column vectors over row vectors, columns are slightly more natural from the point of view of linear equations (which will be covered later in the course). 29 30 CHAPTER 1. VECTORS IN R N AND CN v = ( v1 v2 ) v1 v2 v = ( v1 v2 ) v1 v2 Figure 1.1: An elelmentv ∈ R 2 can be visualised as a point in the plane (left) or as an arrow to the point with coordinates (v1, v2) (right). v =  v1 v2 v3   v1 v2 v3 v =  v1 v2 v3   v1 v2 v3 Figure 1.2: Visualisation of v ∈ R 3 as a point in space (left) and as an arrow (right). Deﬁnition 1.1.1. Let n be a positive integer. The space R n is the set of all columns of size n, v =      v1 v2 ... vn  | | |  , whose entries v1, v2, . . . , vn are real numbers. We will see later in the course that R n is an example of a vector space and so we will call elements of R n vectors. 2 Remark 1.1.2. 1. The numbers v1, . . . , vn are called components, coordinates or en- tries. 2We will use boldface latin letters to denote elements of Rn in the notes, and during lectures we will use underlined latin letters. We will continue with this convention when we consider general vector spaces later in the course. 1.2. LINEAR COMBINATIONS AND SPAN 31 2. We think of elements of R n as coordinates of points in “n-dimensional space”. We can add elements of R n together and we can multiply them by constants. More precisely, we can perform the following operations: Vector addition      v1 v2 ... vn  | | |  +      w1 w2 ... wn  | | |  =      v1 + w1 v2 + w2 ... vn + wn  | | |  ; Scalar multiplication α      v1 v2 ... vn  | | |  =      αv1 αv2 ... αvn  | | |  (α ∈ R). In this context real numbers are called scalars (because when a real number multiplies a vector, it ‘scales’ the vector). 3 Remark 1.1.3. 1. For v ∈ R n, we write −v for the element (−1)v. 2. We will use 0 to denote the origin, i.e. 0 =      0 0 ... 0  | | |  . 1.2 Linear combinations and span We can combine the operations of addition and scalar multiplication to form linear combinations. Deﬁnition 1.2.1. Let v1, v2, . . . , vm be vectors in Rn. A linear combination of v1, v2, . . . , vm is an expression of the form α1v1 + α2v2 + · · · + αmvm = m∑ k=1 αkvk, where α1, . . . , αn are scalars (i.e. α1, . . . , αn ∈ R). 3We will usually use small Greek letters, such as α, β, . . . to denote scalars. 32 CHAPTER 1. VECTORS IN R N AND CN Example 1.2.2. Let u = ( 3 −2 ) , v = ( 1 1 ) and w = (−5/2 4 ). Evaluate the linear combination 1 6(2u − 5v + 2w) Solution. 1 6(2u − 5v + 2w) = 1 6 {( 6 −4 ) − ( 5 5 ) + (−5 8 )} = 1 6 ( −4 −1 ) = (−2/3 −1/6 ) . Deﬁnition 1.2.3. Let v1, . . . , vm be vectors in R n. The linear span (or sometimes just span) of v1, . . . , vm is the set of all vectors which are linear combinations of v1, . . . , vm. We denote the linear span of v1, . . . , vm by span{v1, . . . , vm}, i.e. span{v1, . . . , vm} = { m∑ k=1 αkvk : α1, . . . , αm ∈ R } . Remark 1.2.4. We always have 0 ∈ span{v1, . . . , vn}, since we can take α1 = 0, α2 = 0, . . . , αn = 0. Example 1.2.5. Determine whether (3, 5, −8) is in the linear span of the vectors (1, 3, 0) and (0, −2, 4). Solution. (3, 5, −8) is in the linear span of (1, 3, 0) and (0, −2, 4) if and only if there exist α, β ∈ R such that   3 5 −8   = α   1 3 0   + β   0 −2 4   . (1.2.1) Equating each component, we see that this holds if and only if 3 = α, (1.2.2) 5 = 3α − 2β, (1.2.3) −8 = 4β. (1.2.4) From (1.2.2) and (1.2.4) we must have α = 1 and β = −2. Substituting these into the right hand side of (1.2.3) we get 3α − 2β = 13 ̸= 5. Hence (1.2.3) does not hold. We have shown that there are no α, β ∈ R such that (1.2.1) holds and so (3, 5, −8) is not in the linear span of (1, 3, 0) and (0, −2, 4). 1.3. GEOMETRY OF R N 33 The following examples will be particularly important. ‹ Let v ∈ R n be a non-zero vector. Then span{v} = {αv : α ∈ R}. Geometrically, this is a line. In fact it is the line through the origin in the direction of the vector v. Moreover, every line through the origin is of this form. ‹ Let v, w ∈ R n be non-zero vectors. Then span{v, w} = {αv + βw : α, β ∈ R}. If w is a multiple of v, then span{v, w} = span{v} and so we get a line again. If w is not a multiple of v (so that w and v are not in the “same direction”) then span{v, w} is a plane. In particular it is the plane containing v, w and the origin. Moreover, every plane through the origin is of this form. 1.3 Geometry of Rn Lengths and Dot products In two and three dimensions we can compute the length of a vector v - that is, the distance between the origin and the point v - using Pythagoras’ theorem. More precisely, the length of v = (v1, v2, v3) ∈ R3 is √ v2 1 + v2 2 + v2 3. We can generalize this to deﬁne the length of a vector in Rn. Deﬁnition 1.3.1. Let v = (v1, v2, . . . , vn) ∈ R n. The length or norm of v is denoted ∥v∥ and deﬁned by ∥v∥ = ( n∑ k=1 v2 k ) 1 2 = √ v2 1 + v2 2 + · · · + v2 n. We can deﬁne the distance between two points u and v in R n to be ∥v − u∥. 34 CHAPTER 1. VECTORS IN R N AND CN v v w Figure 1.3: The linear span of a single vector is a line (left) and the linear span of two vectors is a plane (right). Proposition 1.3.2 (Properties of the norm). For every u, v ∈ Rn and every scalar α we have (i) ∥v∥ ≥ 0 and ∥v∥ = 0 if and only if v = 0, (ii) ∥αv∥ = |α|∥v∥, (iii) ∥u + v∥ ≤ ∥u∥ + ∥v∥. Proof. (i) Since v2 1 ≥ 0, v2 2 ≥ 0, . . . , v2 n ≥ 0, it follows that ∥v∥ = √v2 1 + · · · + v2 n ≥ 0. Moreover, if ∥v∥ = 0 then we must have v2 1 = v2 2 = · · · = v2 n = 0 and so v = 0. (ii) This is a direct computaion: ∥αv∥ = √(αv1)2 + · · · + (αvn)2 = √ α2(v2 1 + · · · + v2 n) = √α2√ v2 1 + · · · + v2 n = |α|∥v∥. 1.3. GEOMETRY OF R N 35 (iii) This is the triangle inequality that we saw for complex numbers. The proof of this will be left as an assignment question. Example 1.3.3. Let u = ( 3 −2 ) and v = ( −1 5 ) . Compute the distance between the points u and v. Solution. The distance between u and v is ∥u − v∥ = \r \r \r \r ( 3 −2 ) − (−1 5 )\r \r \r \r = \r \r \r \r ( 4 −7 )\r \r \r \r = √ 65. Deﬁnition 1.3.4. Let u = (u1, . . . , un), v = (v1, . . . vn) ∈ R n. Then the dot product (or scalar product or inner product) of u and v is denoted u · v and deﬁned by u · v = n∑ k=1 ukvk = u1v1 + u2v2 + · · · + unvn. Remark 1.3.5. For v = (v1, . . . , vn) we have v · v = v2 1 + · · · + v2 n = ∥v∥2. Thus ∥v∥ = √ v · v. Proposition 1.3.6 (Properties of the dot product). For every u, v, w ∈ Rn and every scalar α we have (i) v · v ≥ 0 and v · v = 0 if and only if v = 0, (ii) u · v = v · u, (iii) u · (v + w) = u · v + u · w, (iv) (αu) · v = α(u · v) = u · (αv). Proof. (i) Since v · v = ∥v∥2, the claim follows from Proposition 1.3.2(i). (ii) This is immediate since u1v1 + · · · + unvn = v1u1 + · · · + vnun. (iii) We compute that u · (v + w) = u1(v1 + w1) + u2(v2 + w2) + · · · + un(vn + wn) = u1v1 + u2v2 + . . . unvn + u1w1 + u2w2 + . . . unwn = u · v + u · w. 36 CHAPTER 1. VECTORS IN R N AND CN (iv) We have that (αu) · v = αu1v1 + · · · + αunvn = α(u1v1 + · · · + unvn) = α(u · v). The same steps also prove that u · (αv) = α(u · v). When n = 2 or 3, the dot product allows us to determine the angle between two vectors. Proposition 1.3.7. Let u, v ∈ R n with n = 2, 3 and let θ be the angle between u and v. Then u · v = ∥u∥∥v∥ cos θ. Proof. The triangle spanned by u and v (i.e the triangle with vertices u, v and 0) has side lengths ∥u∥, ∥v∥ and ∥u − v∥ (see Figure 1.4). By the cosine rule, we have that ∥u − v∥2 = ∥u∥2 + ∥v∥ 2 − 2∥u∥∥v∥ cos θ. However, ∥u − v∥ 2 = (u − v) · (u − v). We can simplify this using the properties of the dot product above to get ∥u − v∥2 = (u − v) · (u − v) = u · u + v · v − u · v − v · u = ∥u∥2 + ∥v∥ 2 − 2u · v. Substituting this into the above equation we get ∥u∥ 2 + ∥v∥2 − 2u · v = ∥u∥2 + ∥v∥ 2 − 2∥u∥∥v∥ cos θ, and therefore u · v = ∥u∥∥v∥ cos θ. Remark 1.3.8. 1. In particular, we have that two vectors non-zero u and v are perpendicular (or orthogonal ) if and only if u · v = 0. 2. Since | cos θ| ≤ 1, we have that |u · v| ≤ ∥u∥∥v∥. This is known as the Cauchy-Schwartz inequality and will be proved in much more generality in Linear Algebra and Geometry II. 1.3. GEOMETRY OF R N 37 v u − v u ∥v∥ ∥u∥ ∥u − v∥ θ Figure 1.4: The triange spanned by u and v. Example 1.3.9. Find the angle between u = ( 1 −5 ) and v = (−3 2 ) . Solution. Let ϕ denote the angle u and v. Then by Proposition 1.3.7, cos ϕ = u · v ∥u∥∥v∥. We compute that u · v = −3 − 10 = −13; ∥u∥ = √ 12 + 52 = √26; ∥v∥ = √(−3)2 + 22 = √ 13. This gives cos ϕ = −13 √26 √13 = −1 √2, and hence ϕ = 3π/4. 1.3.1 Lines and planes We saw earlier that the line through the origin in the direction of a vector p is of the form {αp : α ∈ R}. Let L be a line through the point v0 and parallel to p. Then every 38 CHAPTER 1. VECTORS IN R N AND CN point v on L is of the form v = v0 + tp. (1.3.1) for some t ∈ R. 4As the real parameter t varies, we get every point on the line. (1.3.1) is known as the parametric equation of the line. v0 p tp v = v0 + tp Figure 1.5: Any point v on the line can be expressed in the form v = v0 + tp for some choice of t ∈ R. Remark 1.3.10. The parametric equation of a line is not unique. In particular, ‹ replacing p with any non-zero multiple of p gives the same line; ‹ replacing v0 with any other point on the line gives the same line. Example 1.3.11. Find a parametric equation of the line in R3 passing through the points (1, −3, 1) and (−2, 4, 5). Solution. For a parametric equation, we need a point v0 on the line; we will take v0 =   1 −3 1   . 4We use t here rather than α because we think of this parameter as ‘time’. 1.3. GEOMETRY OF R N 39 We also need a vector p parallel to the line. Since the diﬀerence of any two vectors on the line will be parallel to it, we can take p =   1 −3 1   −  −2 4 5   =   3 −7 −4   Then the parametric equation of the line is v =   1 −3 1   + t   3 −7 −4   . Exercise. Let L1 be the line in R 2 with parametric equation v = (1 1 ) + t ( 4 −2 ) . Let L2 be the line in R 2 with parametric equation v = (−3 3 ) + t ( −2 1 ) . Determine whether or not L1 and L2 are parallel or equal. Let p and q be two non-zero vectors in Rn and suppose p is not a multiple of q. Then the plane spanned by p and q (i.e. the plane passing through p, q and 0) is the set span{p, q} = {αp + βq : α, β ∈ R}. Fix a point v0 ∈ R n and let Π be the plane parallel to span{p, q} and pasing through v0. Then any point v on Π is of the form v = v0 + tp + sq for some t, s ∈ R. As the real parameters t, s vary, we get every point on the plane. This is called the parametric equation of the plane. Remark 1.3.12. As with the parametric equation of a line, the parametric equation of a plane is also not unique. In particular, ‹ replacing p and q with any non-zero vectors in span{p, q} (provided they are not multiples of each other) gives the same plane; 40 CHAPTER 1. VECTORS IN R N AND CN ‹ replacing v0 with any other point on the plane gives the same plane. Proposition 1.3.13. A plane in R 3 is the set of points v = (v1, v2, v3) satisfying an equation av1 + bv2 + cv3 = d, (1.3.2) where a, b, c, d ∈ R are constants and a, b, c not all zero. (1.3.2) is the Cartesian equation of the plane. Proof. Consider the plane in R3 with parametric equation v = v0 + tp + sq. We will show that there exist a, b, c, d ∈ R with a, b, c not all zero such that (1.3.2) holds. To do this we take any non-zero vector u which is perpendicular to both p and q (the existence of such a point is geometrically obvious and can be proved rigorously once we study linear equations later in the course). Set a, b, c to be the components of u, so that av1 + bv2 + cv3 = u · v. Then if v lies on the plane, so that v = v0 + tp + sq for some t, s ∈ R, we have u · v = u · (v0 + tp + sq) = u · v0 + tu · p + su · q = u · v0. Setting d = u · v0 we see that if v lies on the plane then av1 + bv2 + cv3 = d. We also need to show that the set of all vectors satisfying (1.3.2) with a, b, c not all zero is a plane. Suppose without loss of generality that a ̸= 0. Then if v1, v2, v3 satisfy (1.3.2), we can rearrange to get av1 = d − bv2 − cv3 and hence v1 = d/a − v2b/a − v3c/a. We conclude that  v1 v2 v3   =  d/a − v2b/a − v3c/a v2 v3   =   d/a 0 0   + v2  −b/a 1 0   + v3  −c/a 0 1   , which is the equation of a plane. To make it look more familiar we can set v2 = t and v3 = s to get v =  d/a 0 0   + t  −b/a 1 0   + s  −c/a 0 1   . 1.4. COMPLEX VECTORS 41 More generally, an equation of the form a1v1 + a2v2 + · · · + anvn = b where a1, . . . , an, b are given numbers is called a linear equation in the unknowns v1, . . . , vn. The set of solutions in R n of such an equation is called a hyperplane in R n. It is a ﬂat “n − 1 dimensional surface” in R n. 1.4 Complex vectors Much of the theory that we have developed extends to vectors with complex entries. Deﬁnition 1.4.1. Let n be a positive integer. The space Cn is the set of all columns of size n, z =      z1 z2 ... zn  | | |  , whose entries z1, z2, . . . , zn are complex numbers. As in the real case, we can add vectors and multiply by scalars; the only diﬀerence being that now we are allowed to multiply by complex numbers and so the complex numbers are called scalars. Example 1.4.2. Express (−7 − 3i, i, −5i) as a linear combination of the vectors (i, 1, 0) and (−3, 0, 1 + 2i). Solution. We need to ﬁnd α, β ∈ C such that  −7 − 3i i 5i   = α  i 1 0   + β   −3 0 1 + 2i   . Equating each component, we see that we must have 5 + 3i = iα − 3β, (1.4.1) i = α, (1.4.2) 5i = (1 + 2i)β. (1.4.3) 42 CHAPTER 1. VECTORS IN R N AND CN It follows from (1.4.2) that α = i. Substituting this into (1.4.1) we get −7−3i = −1−3β, and so β = 2 + i. We can check that with this choice of α and β (1.4.3) is also satisﬁed: (1 + 2i)β = (1 + 2i)(2 + i) = 5i. We have shown that  −7 − 3i i 5i   = i  i 1 0   + (2 + i)   −3 0 1 + 2i   . 1.4.1 Length and dot product in Cn The deﬁnitions of the length and the dot product in R n cannot be extended directly to C n. For example, by that deﬁnition the length of the vector (1, i) would be √12 + i2 =√1 − 1 = 0. So a non-zero vector would have zero length. In order to retain the desirable geometric properties we have to modify the deﬁnitions of the length and dot product. Deﬁnition 1.4.3. Let z = (z1, z2, . . . , zn) ∈ Cn. The length or norm of z is denoted ∥z∥ and deﬁned by ∥z∥ = ( n∑ k=1 |zk| 2) 1 2 = √ |z1|2 + |z2|2 + · · · + |zn|2. We can the deﬁne distance between two points z and w in C n to be ∥z − w∥. With this deﬁnition the norm on C n satisﬁes the properties in Proposition 1.3.2. Exercise. Explain why the triangle inequality in C n is a consequence of the triangle inequality in R n. Deﬁnition 1.4.4. Let z = (z1, . . . , zn), w = (w1, . . . wn) ∈ C n. Then the dot product (or scalar product or inner product) of z and w is denoted z · w and deﬁned by z · w = n∑ k=1 zkwk = z1w1 + z2w2 + · · · + znwn. Remark 1.4.5. If the entries of z and w are real, the deﬁnition of the dot product (or length) in C n and R n coincide. This is because in this case wk = wk and so n∑ k=1 zkwk = n∑ k=1 zkwk. Chapter 2 Vector Spaces In order to deﬁne a vector space, we ﬁrst need to choose a set of scalars, which for us will always be either R or C. Most of the theory we will develop works equally well for both so we will usually write F to mean either R or C (or neglect to mention the set of scalars at all) when it doesn’t matter. For example, we will write Fn to mean either R n or Cn. Informally, a vector space is a set of objects which can be added together and multiplied by scalars. In addition, we want these operations to obey the usual rules of algebra that we are used to from R n. Deﬁnition 2.0.1. A vector space is any collection of objects V (called vectors) for which two operations can be performed: ‹ Vector addition, which takes two vectors v, w ∈ V and returns another vector v + w ∈ V (in this case we say that V is closed under addition). ‹ Scalar multiplication, which takes a vector v ∈ V and scalar α ∈ F and returns a vector αv ∈ V (in this case we say that V is closed under scalar multiplication). Furthermore, the following properties (or axioms) must be satisﬁed. The ﬁrst four properties relate to addition: 1. Commutativity: v + w = w + v for all v, w ∈ V ; 2. Associativity: (u + v) + w = u + (v + w) for all u, v, w; 3. Zero vector: there exists a special vector, denoted 0, such that v + 0 = v for all v ∈ V ; 4. Additive inverse: For every vector v ∈ V , there is a vector −v ∈ V such that −v + v = 0; 43 44 CHAPTER 2. VECTOR SPACES The next two properties relate to scalar multiplication: 5. Multiplicative identity: 1v = v for all v ∈ V ; 6. Multiplicative associativity: α(βv) = (αβ)v for all v ∈ V and all α, β ∈ F. The last two distributive properties connect vector addition and scalar multiplication: 7. α(v + w) = αv + αw fora all v, w ∈ V and all α ∈ F; 8. (α + β)v = αv + βv for all v ∈ V and all α, β ∈ F. The list of properties above seems daunting but it can be summarised as saying that the usual laws of algebra work. Verifying that all of the properties hold can be tedious, but in many cases we don’t have to verify all of them. Remark 2.0.2. If the scalars are the real numbers we say that V is a real vector space or V is a vector space over R. Similarly, if the scalars are the complex numbers we say that V is a complex vector space or that V is vector space over C. Note that every complex vector space is also a real vector space because if we can multiply by complex numbers then we can certainly multiply by real numbers. 2.1 Examples Column vectors. For each n ≥ 1, the spaces R n and C n (with the standard addition and scalar multiplication) are vector spaces. R n is a real vector space but not a complex vector space, whereas C n can be either a real or complex vector space. In particular, the set of scalars R = R 1 or C = C 1 can itself be thought of as a vector space (although this is not a very interesting vector space). Zero vector space. An even less interesting space is the space {0} consisting of only the zero vector. Polynomials. For n ≥ 0, let Pn denote the space of all polynomials in one variable with degree at most n, i.e. Pn consists of all polynomials p of the form p(t) = a0 + a1t + a2t 2 + · · · + ant n, where t is the independent variable and a0, a1, . . . , an ∈ F. Note that some, or all of the coeﬃcients ak can be 0. Addition and scalar multiplication are deﬁned in the standard manner, e.g. (4t 3 − 2t 2 + 5t + 1) + (−6t 3 + t + 4) = −2t 3 − 2t 2 + 6t + 5 2.1. EXAMPLES 45 and 3(4t 3 − 2t 2 + 5t + 1) = 12t 3 − 6t 2 + 15t + 3. Then Pn is a vector space. In particular, if we insist the coeﬃcients are real we have a real vector space, if the coeﬃcients are complex we have a complex vector space. Functions. Let F(R, R) denote the set of all functions from R to R. As before, we deﬁne addition and scalar multiplication in the usual way. Then F(R, R) is a real vector space. Similarly, the set of all functions from R to C, F(R, C), is a complex vector space. Solutions of diﬀerential equations. An ordinary diﬀerential equation, or ODE, is an equation involving some unknown function and its derivatives. The order of the ODE is the highest derivative to appear in the equation. A linear ODE of order n of the unknown function u is an ODE of the form an(x)d nu dxn + an−1(x)d n−1u dxn−1 + · · · a1(x)du dx + a0(x)u = b(x), (2.1.1) where a0(x), a1(x), · · · , an(x) and b(x) are known functions. (2.1.1) is called homogeneous if b(x) = 0. For example ‹ du dx + 4u = 2 + e−x is a ﬁrst order linear ODE of the unknown which is not homogeneous; ‹ 3d2u dx2 + x du dx + u sin x = 0 is a second order linear homogeneous ODE. The set of all solutions of linear homogeneous ODE is a vector space. Let us illustrate this with an example. Consider the ﬁrst order ODE x du dx + sin(x)u = 0. (2.1.2) Suppose that u and v are both solutions of (2.1.2) so that x du dx + sin(x)u = 0 and x dv dx + sin(x)v = 0. Then x d(u + v) dx + sin(x)(u + v) = x ( du dx + dv dx ) + sin(x)(u + v) (2.1.3) = xdu dx + sin(x)u + x dv dx + sin(x)v = 0. (2.1.4) 46 CHAPTER 2. VECTOR SPACES So u + v is also a solution. If α is a scalar, we also have x d(αu) dx + sin(x)(αu) = α (xdu dx + sin(x)u ) = 0. Hence αu is also a solution. We have shown that the set of solutions of (2.1.2) is closed under addition and scalar multiplication. One can easily verify that the remaining properties of a vector space hold. 2.1.1 Examples that are not vector spaces Integers. The set of integers Z with the usual addition and scalar multiplication is not a vector space since it is not closed under scalar multiplication. For example 1 23 /∈ Z. More generally, the set of columns of length n with integer entries, denoted Z n, is not a vector space for the same reason. Unit vectors. A vector v ∈ Fn is a unit vector if ∥v∥ = 1. The space of all unit vectors in Fn with the usual addition and scalar multiplication is not a vector as it is neither closed under addition nor scalar multiplication. For example the sum of the unit vectors (1, 0) and (0, 1) in R2 is not a unit vector. Positive real axis. The space R+ = (0, ∞) of all positive real numbers, with the usual addition and scalar multiplication from R, is closed under addition and satisﬁes most of the properties of vector spaces, but is not a vector space because we can’t multiply by negative scalars. In addition, it does not have a zero vector since it does not contain 0. Exercise. Does the set of all functions f : [0, 1] → [0, 1], with the usual operations, form a vector space? 2.2 Digression. Matrices A matrix is a rectangular array of numbers. For example ( 1 2 3 0 6 9 )     1 0 0 1  | |        1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4  | | | |  ( 1 5 7 9 )  1 + i 2√3 −2i 1 − i iπ/2   . 2.2. DIGRESSION. MATRICES 47 The individual numbers are called the entries, elements or components of the matrix. If the matrix has m rows and n columns we say that it is an m × n matrix (pronounced ‘m by n’). The above examples have respective sizes 2 × 3, 4 × 1, 5 × 5, 2 × 2 and 3 × 2. If m = n we say that it is a square matrix. The set of all m × n matrices with entries in F will be denoted Mm,n(F). If m = n then we will simply write Mn(F). Remark 2.2.1. Elements of R n or C n can be thought of as n × 1 matrices (or 1 × n matrices if you are working with row vectors). 2.2.1 Matrix notation It is often convenient to use subscripts to label the entries of a matrix. Given a matrix A, we typically write aij to denote the entry in the ith row and jth column. 1 In this notation a typical 3 × 4 matrix is A =  a11 a12 a13 a14 a21 a22 a23 a24 a31 a32 a33 a34   . At times we may also write Aij to denote the the entry in the ith row and jth column (i.e. we may use the same letter rather than a the lowercase version). Moreover we may write A = (aij) without specifying its size. The columns (or rows) of an m × n matrix can be regarded as vectors in Fm (respectively Fn). Let the columns of A be a1, a2, . . . , an ∈ Fm. We may informally write A =   ↑ ↑ . . . ↑ a1 a2 . . . an ↓ ↓ . . . ↓   when we wish to regard A as an ordered list of its columns. We may occasionally use analogous notation when regarding a matrix as a list of its rows. Deﬁnition 2.2.2. Let A be a matrix. Then the transpose of A, denoted A T , is obtained by interchanging the rows and columns of A, i.e. if A = (aij) then AT = (aji). 1We typically use an uppercase Latin letter to denote a matrix and the same lowercase letter (with subscripts) to denote its entries. 48 CHAPTER 2. VECTOR SPACES For example (1 2 3 0 6 9 )T =  1 0 2 6 3 9   ,     1 0 0 1  | |  T = (1 0 0 1 ) ,   1 3 5 7 9 11 2 4 6   T =  1 7 2 3 9 4 5 11 6   . Remark 2.2.3. Observe that (AT )T = A. 2.2.2 Matrices as vectors Addition and scalar multiplication are deﬁned entry-wise, i.e if A = (aij) and B = (bij) are two m × n matrices, and α is a scalar, then A + B = (aij + bij) =      a11 + b11 a12 + b12 . . . a1n + b1n a21 + b21 a22 + b22 . . . a2n + b2n ... ... . . . ... am1 + bm1 am2 + bm2 . . . amn + bmn  | | |  ; αA = (αaij) =      αa11 αa12 . . . αa1n αa21 αa22 . . . αa2n ... ... . . . ... αam1 αam2 . . . αamn  | | |  . With these operations Mm,n(F) is a vector space. To be precise, the set of m×n matrices with real entries is a real vector space and the set of m × n matrices with complex entries is a complex vector space. Exercise. Is the set of all 2 × 2 matrices with integer entries, denoted M2,2(Z), a real vector space? 2.3 Subspaces Deﬁnition 2.3.1. Let V be a vector space. A non-empty set W ⊆ V is a subspace of V if (1) for every u, v ∈ W , u + v ∈ W (W is closed under addition); 2.3. SUBSPACES 49 (2) for every α ∈ F and v ∈ W , αv ∈ W (W is closed under scalar multiplication). In other words, a subspace of V is a subset of V which is also a vector space with the same operations of addition and scalar multiplication inherited from V . Remark 2.3.2. The conditions in Deﬁnition 2.3.1 can be replaced by the condition (1’) for every α, β ∈ F and every u, v ∈ W , αu + βv ∈ W . A consequence of the conditions of the conditions (1) and (2) above (or condition (1’)) is the following key point: If W is a subspace, it must contain all linear combinations of vectors in W . In particular, W must contain 0. 2.3.1 Examples (and non-exmaples) of subspaces (1) Let V be a vector space. Then {0} and V itself are subspaces of V . These are usually referred to as trivial subspaces. (2) P2, the space of polynomials of degree at most 2, is a subspace of P3. (3) Let W denote the subset of R 2 consisting of all (x, y) ∈ R 2 such that x = 1. Let us check if W is closed under addition: Take v1, v2 ∈ W with v1 = (x1, y1) and v2 = (x2, y2), then v1 + v2 = ( x1 y1 ) + ( x2 y2 ) = (x1 + x2 y1 + y2 ) . However x1 + x2 = 1 + 1 = 2. So v1 + v2 /∈ W . Thus W is not closed under addition, and hence not a subspace of R 2. Similarly, one can show that W is not closed under scalar multiplication. However, the easiest way to see that W is not a subspace is to observe that it does not contain 0. (4) Let U be the set of all (x, y, z) ∈ R 3 such that x = 2y + z. Let us check if U is closed under addition: Take v1, v2 ∈ U with v1 = (x1, y1, z1) and v2 = (x2, y2, z2), then v1 + v2 =  x1 y1 z1   +  x2 y2 z2   =   x1 + x2 y1 + y2 z1 + z2   . 50 CHAPTER 2. VECTOR SPACES We have that x1 + x2 = (2y1 + z1) + (2y2 + z2) = 2(y1 + y2) + (z1 + z2). Hence v1 + v2 ∈ U . Let us check if U is closed under scalar multiplication: Take v ∈ U with v = (x, y, z) and α ∈ R, then αv =  αx αy αz   . We have that αx = α(2y + z) = 2αy + αz. Hence αv ∈ U . We have shown that U is a subspace of R 3. (5) Let E be the subset of Pn consisting of all even polynomials, i.e. all p ∈ Pn satisfying p(t) = p(−t). Let us check if E is closed under addition: Take p, q ∈ Pn. Then p(t) + q(t) = p(−t) + q(−t) and so p + q ∈ E. Let us check if E is closed under scalar multipli- cation: Take p ∈ E and α ∈ F. Then αp(t) = αp(−t) and so αp ∈ E. Hence E is a subspace of Pn. (6) Let S be the subset of Pn consisting of all polynomials p ∈ Pn such that p(0) = 1. Then S doesn not contain the zero function so cannot be a subspace. One can also show that S is not closed under addition or scalar multiplication. Exercise. For each of the following conditions, determine if the set of all vectors (x, y) ∈ R 2 which satisﬁes it is a subspace of R2. (a) x + y = 1 (b) x + 2y = 0 (c) x = y (d) x2 = y2 Proposition 2.3.3. Let v1, v2, . . . , vn be vectors in V . Then span{v1, v2, . . . , vn} is a subspace of V . Moreover, if W is any subspace of V containing v1, v2, . . . , vn, then W must contain span{v1, v2, . . . , vn}. Proposition 2.3.3 says that span{v1, v2, . . . , vn} is the smallest subspace of V which contains v1, v2, . . . , vn. 2.4. BASES 51 Proof. First let us show that span{v1, v2, . . . , vn} is a subspace. Take u, w ∈ span{v1, v2, . . . , vn} so that u = n∑ k=1 αkvk and w = n∑ k=1 βkvk. Then u + w = n∑ k=1 αkvk + n∑ k=1 βkvk = n∑ k=1(αk + βk)vk ∈ span{v1, v2, . . . , vn}. Take β ∈ F. Then β n∑ k=1 αkvk = n∑ k=1(βαk)vk ∈ span{v1, v2, . . . , vn}. Hence span{v1, v2, . . . , vn} is a subspace of V . Now suppose that W is a subspace of V containing v1, v2, . . . , vn. Then since W is a subspace, it must also contain all linear combinations of v1, v2, . . . , vn, so it contains span{v1, v2, . . . , vn}. Example 2.3.4. The only subspaces of R 3 are {0}, lines through the origin (the linear span of a single vector), planes through the origin (the linear span of two vectors) and all of R 3. The only subspaces of R 2 are {0}, lines through the origin and all of R 2. 2.4 Bases Deﬁnition 2.4.1. A collection of vectors v1, v2, . . . , vn ∈ V is a basis for V if every vector v ∈ V admits a unique representation as a linear combination v = α1v1 + α2v2 + · · · + αnvn = n∑ k=1 αkvk. The coeﬃcients α1, α2, . . . , αn are called coordinates of v with respect to the basis v1, v2, . . . , vn. Example 2.4.2. Let V = Fn and consider the vectors e1 =        1 0 0 ... 0  | | | | |  , e2 =        0 1 0 ... 0  | | | | |  , e3 =        0 0 1 ... 0  | | | | |  , . . . , en =        0 0 0 ... 1  | | | | |  , 52 CHAPTER 2. VECTOR SPACES The vectors e1, e2, . . . , en form a basis in Fn. Indeed if v =      v1 v2 ... vn  | | |  , then v = v1e1 + v2e2 + · · · + vnen = n∑ k=1 vkek, and this representation is unique. The basis e1, e2, . . . , en is called the standard basis in Fn. There are many other bases in Fn. Example 2.4.3. Let V = Pn be the space of polynomials in one variable with degree at most n and consider the vectors (polynomials) e0(t) = 1, e1(t) = t, e2(t) = t 2, . . . , en(t) = t n. Then any polynomial p(t) = a0 + a1t + · · · + ant n admits a unique representation p = a0e0 + a1e1 + · · · + anen = n∑ k=1 akek. Hence e1, e2, . . . , en form a basis of Pn. We call this the monomial basis of Pn. Remark 2.4.4. Every vector space has a (possibly inﬁnite) basis. The proof of this fact is beyond the scope of this course. Exercise. Find a basis for the real vector space M2,2(R) (the space of all 2 × 2 matrices with real entries). 2.4.1 Coordinate vector Let V be a vector space with basis E = {e1, e2, . . . , en}. Then any v is uniquely determined its coordinates with respect to E, i.e. the coeﬃcients in the expansion v = α1e1 + α2e2 + · · · + αnen. It is convenient to write these coordinates in a col- umn vector. 2.4. BASES 53 Deﬁnition 2.4.5. Let V be a vector space with basis E = {e1, e2, . . . , en} ane let v = α1e1 + α2e2 + · · · + αnen ∈ V . Then the coordinate vector of v with respect to E is the column vector [v]E :=      α1 α2 ... αn  | | |  ∈ Fn. Thus we have a one-to-one correspondence between V and Fn: v ←→ [v]E. Moreover if w = β1e1 + · · · + βnen then v + w = (α1 + β1)e1 + (α2 + β2) + · · · + (αn + βn)en = n∑ k=1(αk + βk)ek. and if β ∈ F, βv = βα1e1 + βα2e2 + · · · + βαnen = n∑ k=1 βαkek. Hence [v + w]E = [v]E + [w]E and [βv]E = β[v]E. In other words, addition and scalar multiplication in V corresponds to addition and scalar multiplication of the coordinate vectors in Fn. Remark 2.4.6. The coordinate vector of v depends on the choice of basis. If we choose a diﬀerent basis, the coordinates of v will change. 2.4.2 Spanning and linearly independent sets The deﬁnition of a basis states that each vector v ∈ V admits a unique representation as a linear combination of vectors in our basis. Thus in order to determine if a set of vectors is a basis we need to determine two distinct facts: (1) that such a representation exists for each vector; (2) that it is unique. First we consider only the existence part. 54 CHAPTER 2. VECTOR SPACES Deﬁnition 2.4.7. We say that a collection of vectors v1, v2, . . . , vn ∈ V spans V , or that it is a spanning set (or generating set), if every vector v ∈ V admits a representation as a linear combination v = α1v1 + α2v2 + · · · + αnvn = n∑ k=1 αkvk. Equivalently, v1, v2, . . . , vn span V if span{v1, v2, . . . , vn} = V. The diﬀerence between the deﬁnitions of a basis and a spanning set is that for a basis, the representation of each vector as a linear combination must be unique whereas for a spanning set, there may be many diﬀerent representations for the same vector. Example 2.4.8. Consider the following vectors in R 2: v1 = ( 1 0 ) , v2 = ( 0 1 ) , v3 = (−1 0 ) , v4 = ( 0 −1 ) , Then v1, v2, v3, v4 spans R2. Indeed if v = ( x y ) then v = x ( 1 0 ) + y ( 0 1 ) + 0 ( −1 0 ) + 0 ( 0 −1 ) = xv1 + yv2 + 0v3 + 0v4. However, we also have v = 0 (1 0 ) + 0 (0 1 ) − x ( −1 0 ) − y ( 0 −1 ) = 0v1 + 0v2 − xv3 − yv4, and so v1, v2, v3, v4 is not a basis. Remark 2.4.9. As in the example above, one can always add additional vectors to a spanning set and still get a spanning set. However removing a vector from a spanning set may cause it to stop spanning. For example, the vectors (1, 0) and (0, 1) span R 2 but either one on its own does not. Example 2.4.10. Show that the vectors (1, 0, 0), (1, 1, 0) and (1, 1, 1) span R3. Solution. Take an arbitrary vector v = (x, y, z) ∈ R3. We need to show that it can be expressed as a linear combination of the form  x y z   = α   1 0 0   + β  1 1 0   + γ  1 1 1   . 2.4. BASES 55 Equating components we get x = α + β + γ, (2.4.1) y = β + γ, (2.4.2) z = γ. (2.4.3) Substituting (2.4.3) into (2.4.2) we get β = y −z, and then substituting these into (2.4.1) we get α = x − y. Hence  x y z   = (x − y)  1 0 0   + (y − z)   1 1 0   + z  1 1 1   . Since v was arbitrary, this shows that every v ∈ R3 is in the linear span of (1, 0, 0), (1, 1, 0) and (1, 1, 1). Exercise. Let u, v, w ∈ R 2. Suppose span{u, v} = R2. Which of the following state- ments are true? (a) span{u} = R2 or span{v} = R 2. (b) span{u, v, w} = R2. (c) span{u, u + v} = R 2 Example 2.4.11. Determine if the vectors (1, 1, 1), (1, −2, 2) and (2, −1, 3) span R 3. Solution. As before we start by taking an arbitrary vector v = (x, y, z) ∈ R 3 and trying to solve the equation   x y z   = α  1 1 1   + β   1 −2 2   + γ   2 −1 3   . Equating components we must have x = α + β + 2γ, (2.4.4) y = α − 2β − γ, (2.4.5) z = α + 2β + 3γ. (2.4.6) Subtracting (2.4.5) from (2.4.4) and (2.4.6) from (2.4.4) gives x − y = 3β + 3γ, x − z = −β − γ. 56 CHAPTER 2. VECTOR SPACES Hence x − y = −3(x − z), and so 4x − y + 3z = 0 (2.4.7) The set of all v = (x, y, z) that satisfy (2.4.7) is a plane. Thus (1, 1, 1), (1, −2, 2) and (2, −1, 3) cannot span all of R3. Next we consider only the uniqueness of of a representation of a vector as a linear combination. Deﬁnition 2.4.12. A collection of vectors v1, v2, . . . , vn ∈ V are linearly dependent if there exist α1, α2, . . . , αn ∈ F, not all zero, such that α1v1 + α2v2 + · · · + αnvn = 0. The vectors v1, v2, . . . , vn ∈ V are linearly independent if they are not linearly dependent. Of course by taking all the coeﬃcients αk to be 0, we get one representation of 0 as a linear combination of v1, v2, . . . , vn. So the deﬁnition above is saying that v1, v2, . . . , vn are ‹ linearly independent if there is precisely one representation of 0 as a linear combi- nation of v1, v2, . . . , vn (i.e. it is unique for 0); ‹ linearly dependent if there is more than one such representation. Remark 2.4.13. Clearly any basis is linearly independent; since every vector has a unique representation as a linear combination of its elements, the zero vector certainly has a unique representation. Proposition 2.4.14. The vectors v1, v2, . . . , vn ∈ V are linearly dependent if and only if one of the vectors can be represented as a linear combination of the others. Proof. Suppose that v1, v2, . . . , vn are linearly dependent. This means that there are scalars α1, α2, . . . , αn ∈ F, which are not all zero, such that 0 = α1v1 + α2v2 + · · · + αnvn. Assume without loss of generality that α1 ̸= 0 (we can relabel the terms if we wish so that α1 ̸= 0). Then α1v1 = −α2v2 − · · · − αnv2, and so v1 = β2v2 + · · · + βnvn, (2.4.8) 2.4. BASES 57 where βk = −αk/α1. Conversely, suppose that (2.4.8) holds. Then we can write 0 = −v1 + v1 = −v1 + β2v2 + · · · + βnvn, which is a linear combination of v1, v2, . . . , vn where the coeﬃcient of v1 is −1. Thus v1, v2, . . . , vn are linearly dependent. Remark 2.4.15. Suppose that v1, v2, . . . , vn are linearly dependent. Then by Proposi- tion 2.4.14 one of the vectors, say v1, is a linear combination of the others. Then any vector that can be written as a linear combination of v1, v2, . . . , vn can also be written as a linear combination of v2, . . . , vn, i.e. span{v1, v2, . . . , vn} = span{v2, . . . , vn}. In particular, if v1, v2, . . . , vn are spanning, so are v2, . . . , vn. To summarise: If v1, . . . , vn are linearly dependent, then one of the vk is a linear combination of the others and we can remove this vector without changing their linear span. Example 2.4.16. Show that the vectors u = (1 2 ) and v = (2 3 ) are linearly independent. Solution. Suppose that αu + βv = 0. We need to show that α = β = 0. Equating components gives α + 2β = 0, (2.4.9) 2α + 3β = 0. (2.4.10) From (2.4.9) we get that α = −2β. Substituting this into (2.4.10) gives −4β + 3β = 0 and so β = 0 and α = −2β = 0. Proposition 2.4.17. A collection of vectors v1, v2, . . . , vn ∈ V is a basis if and only if it is linearly independent and spans V . 58 CHAPTER 2. VECTOR SPACES Proof. We have already seen that every basis of V is linearly independent and spans V so it only remains to show the converse. Suppose that v1, v2, . . . , vn are linearly independent and span V . Then, since they are spanning, any v ∈ V admits a representation v = n∑ k=1 αkvk (2.4.11) We only need to show that this representation is unique. Suppose that we have another representation of v: v = n∑ k=1 βkvk. Then 0 = v − v = n∑ k=1 αkvk − n∑ k=1 βkvk = n∑ k=1(αk − βk)vk. Since v1, v2, . . . , vn are linearly independent we must have αk − βk = 0 for each k = 1, . . . , n. Hence the representation (2.4.11) is unique. Exercise. Let V be a vector space. Which of the following statements are true? (a) Removing a vector from a basis of V produces another basis of V . (b) Adjoining another vector to a basis of V produces another basis of V . (c) Adjoining the zero vector to a basis of V produces another basis of V . Example 2.4.18. Find a basis for the subspace S of R 3 consisting of all (x, y, z) ∈ R3 such that x + y = 2z. Solution. Suppose v = (x, y, z) ∈ S. Then x = 2z − y and so v =  x y z   =  2z − y y z   =  −y y 0   +  2z 0 z   = y   −1 1 0   + z  2 0 1   . Here y, z can be arbitrary real numbers. Therefore S is the linear span of (−1, 1, 0) and (2, 0, 1). Since we also have that (−1, 1, 0) and (2, 0, 1) are linearly independent, the set {(−1, 1, 0), (2, 0, 1)} is a basis of S. Finally we show that we can always ﬁnd a basis from a spanning set. 2.4. BASES 59 Proposition 2.4.19. Any ﬁnite spanning set contains a basis. Proof. Let v1, v2, . . . , vn ∈ V be a spanning set in V . If they are linearly independent then we are done, so we can suppose that they are linearly dependent. In this case, by Proposition 2.4.14 and Remark 2.4.15, we can remove one of the vectors and the new set will still span V . If this new set is linearly independent then we are done. Otherwise we repeat this process. Continuing in this way, we must eventually come to a linearly independent spanning set, because otherwise we would remove all the vectors. 2.4.3 Dimension Theorem 2.4.20 (Dimension theorem). Let V be a vector space. Then every basis of V has the same number of elements. Moreover, if V has a basis of n elements then (a) any set of vectors in V with less than n elements doesn’t span V ; (b) any set of vectors in V with more than n elements is linearly dependent. We will prove this later in the course. Deﬁnition 2.4.21. The dimension of a vector space V is denoted dim V and deﬁned to be the number of elements in a basis for V . If V consists of only the zero vector we set dim V = 0 and if V does not have a ﬁnite basis we set dim V = ∞. Exercise. What is the dimesion of Pn, the vector space of all polynomials of degree at most n? In a ﬁnite dimensional space, any linearly independent collection of vectors can be com- pleted to a basis. Proposition 2.4.22. Let v1, v2, . . . , vm ∈ V be linearly independent vectors in a ﬁ- nite dimensional vector space V . If v1, v2, . . . , vm do not span V , there exist vectors vm+1, vm+2, . . . , vn such that v1, v2, . . . , vn is a basis of V . Proof. Suppose that dim V = n. Then since v1, v2, . . . , vm are linearly independent but do not span V , Theorem 2.4.20 implies that m < n. Take any vector vm+1 which is not in the linear span of v1, v2, . . . , vm. Then v1, v2, . . . , vm, vm+1 are still linearly independent (see Question 6 on Assignment sheet 5). We can repeat the process to get vm+2, and so on, until we have a spanning set. Observe that, by Theorem 2.4.20, this process must terminate when we have n vectors since dim V = n. Thus the collection v1, v2, . . . , vn will be a basis for V . 60 CHAPTER 2. VECTOR SPACES Theorem 2.4.20 and Proposition 2.4.22 have the following important consequence: Proposition 2.4.23. A collection of n vectors in an n dimensional vector space is spanning if and only if it is linearly independent. Proof. Let V be an n dimensional vector space and let v1, . . . , vn ∈ V . Assume that they span V . If they are not linearly independent, we can remove one of the vk without aﬀecting the span (see Remark 2.4.15). This would leave a collection of n − 1 vectors which span V . However, by Theorem 2.4.20, any collection with less than n vectors cannot span V . This is a contradiction, and so v1, . . . , vn must be linearly independent. Conversely, suppose that v1, . . . , vn ∈ V are linearly independent. If they do not span V , by Proposition 2.4.22 we can ﬁnd a vector vn+1 ∈ V such that v1, . . . , vn, vn+1 are linearly independent. However, by Theorem 2.4.20, any collection with more than n vectors cannot be linearly independent. This is a contradiction, and so v1, . . . , vn must span V . A useful consequence of Proposition 2.4.23 is the following: If we want to check if a collection of n vectors in F n is a basis, we only need to check that it is linearly independent (or that it is spanning). In general, linear independence is easier to check, because you are solving a system of equations where one side is zero. Example 2.4.24. Determine if the vectors (1, −3, 2), (2, 1, −3) and (−3, 2, 1) form a basis of R 3. Solution. Since there are 3 vectors in R3, we only need to check whether they are linearly independent. Suppose that α   1 −3 2   + β   2 1 −3   + γ  −3 2 1   = 0. (2.4.12) This is equivalent to the linear equations α + 2β − 3γ = 0, (2.4.13) −3α + β + 2γ = 0, (2.4.14) 2α − 3β + γ = 0. (2.4.15) 2.4. BASES 61 Adding 3×(2.4.13) to (2.4.14) and substracting 2×(2.4.13) from (2.4.15) to eliminate α gives 7β − 7γ = 0, (2.4.16) −7β + 7γ = 0. (2.4.17) Thus β = γ. Substituting this into (2.4.13) gives α − β = 0. Thus any choice of α, β, γ such that α = β = γ will satisfy (2.4.12). This shows that (1, 3, 2), (2, 1, 3) and (3, 2, 1) are not linearly independent and hence not a basis. (Observe that this also shows that they do not span R3). Exercise. What about the vectors (1, 3, 2), (2, 1, 3) and (3, 2, 1)? Do they form a basis of R 3? 62 CHAPTER 2. VECTOR SPACES Chapter 3 Linear Maps Deﬁnition 3.0.1. Let V and W be vector spaces over F (either both real or both complex). Then a map (or transformation) T : V → W is linear if it satisﬁes the following two conditions: (1) for every u, v ∈ V , T (u + v) = T (u) + T (v); (2) for every v ∈ V and every α ∈ F, T (αv) = αT (v). Once again, the two conditions in Deﬁnition 3.0.1 can be combined and replaced by the condition (1’) for every u, v ∈ V and every α, β ∈ F, T (αu + βv) = αT (u) + βT (v). Remark 3.0.2. Observe that we have two diﬀerent addition rules in the above deﬁnition: u and v are vectors in V so u + v uses the addition in V , but T (u) and T (v) are vectors in W so T (u)+T (v) uses the addition in W . One should keep in mind that an expression of the form v + T (v) would not necessarily make sense, unless V and W were equal. 3.0.1 Examples (and non-examples) of linear maps (1) Identity map. Let V be a vector space and let I : V → V be the map given by I(v) = v. This is a linear map and is called the identity map. (2) Zero map. Let V be a vector space and let T : V → V be the map given by T (v) = 0. This is a linear map and is called the zero map. 63 64 CHAPTER 3. LINEAR MAPS (3) Reﬂection. Let T : R 2 → R2 be the map given by reﬂecting in the x-axis, i.e. T : (x y ) 7→ ( x −y ) . Then this is a linear map. Indeed for any v1 = (x1, y1), v2 = (x2, y2) we have T (v1 + v2) = ( x1 + x2 −(y1 + y2) ) = ( x1 −y1 ) + ( x2 −y2 ) = T (v1) + T (v2), and for and v = (x, y) and α ∈ R we have T (αv) = ( αx −αy ) = α ( x −y ) = αT (v). It can also be shown geometrically that this transformation is linear. (4) Rotation. Let Tϕ : R 2 → R2 be the map that takes a vector in R 2 and rotates it anti-clockwise (about the origin) by an angle ϕ. Since Tϕ rotates the whole plane, it rotates the parallelogram used to deﬁne the sum of two vectors (parallelogram law). Therefore Tϕ preserves addition. It is also easy to see that it preserves scalar multiplication. Hence Tϕ is linear. (5) Scaling. Let V be a vector space and let α be a scalar. Let T : V → V be the map given by T (v) = αv. Then T is linear. Indeed for u, v ∈ V we have T (u + v) = α(u + v) = αu + αv = T (u) + T (v), and for every scalar β we have T (βv) = α(βv) = β(αv) = βT (v). The identity map and the zero map are both special cases of this (scaling by 1 and 0 respectively). (6) Diﬀerentiation. Let P denote the vector space of all polynomials in one variable t. Let D : P → P be the map given by D(p) = dp dt . This is a linear map. Indeed for any p, q ∈ P we have D(p + q) = d(p + q) dt = dp dt + dq dt = D(p) + D(q), 65 and for every scalar α we have D(αp) = d(αp) dt = α dp dt = αD(p). (7) Translation. Let V be a vector space. Fix a non-zero vector v0 ∈ V . T : V → V be the map given by T (v) = v0 + v (this is usually called translation by v0). Take u, v ∈ V . Then T (u + v) = v0 + u + v. But T (u) + T (v) = v0 + u + v0 + v = 2v0 + u + v. So T (u + v) ̸= T (u) + T (v), and hence T is not linear. Example 3.0.3. Determine if the map T : R 2 → R2 given by T : ( x y ) 7→ ( x2 y2 ) is linear. Solution. Let v = (x, y) and α ∈ R. Then T (αv) = ( (αx) 2 (αy) 2 ) = α2 (x2 y2 ) but αT (v) = α ( x2 y2 ) . Hence T (αv) ̸= αT (v) unless α = 1. Thus T is not linear. We could have also shown that T does not preserve addition. Example 3.0.4. Determine if the map T : R 3 → R2 given by T :  x y z   7→ ( x + y z − y ) is linear. Solution. Let v = (x, y, z) ∈ R 3 and α ∈ R. Then T (αv) = (αx + αy αz − αy ) = α ( x + y z − y ) = αT v. 66 CHAPTER 3. LINEAR MAPS So T preserves scalar multiplication. Let v1 = (x1, y1, z1) and v2 = (x2, y2, z2). Then T (v1 + v2) = T    x1 + x2 y1 + y2 z1 + z2     = ( (x1 + x2) + (y1 + y2) (z1 + z2) − (y1 + y2) ) = (x1 + y1 z1 − y1 ) + ( x2 + y2 z2 − y2 ) = T (v1) + T (v2). So T also preserves addition. This shows that T is linear. Remark 3.0.5. Let 0U and 0V denote the zero vectors in the vectors spaces U and V respectively, and let T : U → V be a linear map. Then T (0U ) = 0V . Exercise. Which of the following deﬁnitions would make f : R 2 → R linear? (a) f (x, y) = x (b) f (x, y) = xy (c) f (x, y) = x + y (d) f (x, y) = ex+y 3.0.2 Linear maps and bases Let T : V → W be a linear map and let e1, e2, . . . , en be a basis for V . Suppose we know the vectors T (e1), T (e2), . . . , T (en). Then if v = α1e1 + α2e2 + · · · + αnen, we have T (v) = T (α1e1 + α2e2 + · · · + αnen) = α1T (e1) + α2T (e2) + · · · + αnT (en). Therefore, if we know the vectors T (e1), T (e2), . . . , T (en), we can compute T (v) for every v ∈ V . In particular, if T : V → W and S : V → W are linear maps such that T (e1) = S(e1), T (e2) = S(e2), . . . , T (en) = S(en), then T = S. We can summarize this as follows: A linear map is completely determined by how it acts on a basis. 3.1 Operations on linear maps Addition and scalar multiplication Let V and W be vectors spaces (over F) and let S : V → W and T : V → W be linear maps (note that they have the same domain and range). We can add S and T by the 3.1. OPERATIONS ON LINEAR MAPS 67 rule (S + T )(v) = S(v) + T (v) for all v ∈ V. Then S + T is also a linear map from V to W . It’s clear that S + T maps V to W so we just need to check that it is linear. To this end, we calculate that if u, v ∈ V then (S + T )(u + v) = S(u + v) + T (u + v) (by the deﬁnition of S + T ) = S(u) + S(v) + T (u) + T (v) (because S and T are both linear) = (S + T )(u) + (S + T )(v) (by the deﬁnition of S + T again). Using the same reasoning we check that if v ∈ V and α ∈ F then (S + T )(αv) = S(αv) + T (αv) = αS(v) + αT (v) = α(S + T )(v). This shows that S + T is linear. We can also multiply a linear map T : V → W by a scalar α ∈ F according to the rule (αT )(v) = αT (v). One can similarly check that the map αT is also linear. Indeed if u, v ∈ V and β ∈ F then (αT )(u + v) = αT (u + v) = αT (u) + αT (v) = (αT )(u) + (αT )(v), and (αT )(βv) = αT (βv) = αβT (v) = β(αT )(v). So αT is a linear map. We have shown that the sum of two linear maps is linear and any scalar multiple of a linear map is linear. So with these operations the set of linear maps from V to W is closed under addition and scalar multiplication. Observe that these operations are deﬁned pointwise - that is, they are deﬁned at each point of the domain V separately using the addition and scalar multiplication in W (this is just the usual deﬁnition of addition and scalar multiplication of functions). It follows that they must satisfy the axioms of a vector space (because addition and scalar multiplication in W satisfy the axioms). Hence we have the following: If we ﬁx vector spaces V and W , then the set of all linear maps from V to W is itself a vector space. 68 CHAPTER 3. LINEAR MAPS Composition Let U, V and W be vector spaces and let S : U → V and T : V → W be linear maps (note that the range of S is the domain of T ). We can consider the map we obtain by ﬁrst applying S and then applying T , i.e. the map T ◦ S : U → W deﬁned by (T ◦ S)(u) = T (S(u)) for all u ∈ U. Note that even though it is written T ◦ S, with T ﬁrst, S is applied ﬁrst then T . We can visualize how these maps relate to each other using the following diagram: U W V T ◦S S T It turns out that the map T ◦ S is also linear. The proof of this is left as an assignment question. Notation: For linear maps, we usually write T S instead of T ◦ S for the composition of T with S. 3.2 Matrix of a linear map Example 3.2.1. What are all the linear maps from R to R? Let T : R → R be a linear map. Then for any x ∈ R we have T (x) = T (x × 1) = xT (1). Setting α = T (1) we see that T (x) = αx for each x ∈ R. We have show that every linear map from R to R is just multiplication by a constant. It will turn out that every linear map can be represented as a multiplication, not by a scalar, but by a matrix. 3.2. MATRIX OF A LINEAR MAP 69 3.2.1 Linear maps Fn → Fm. Matrix-vector multiplication Let e1, . . . , en be the standard basis in Fn, i.e e1 =      1 0 ... 0  | | |  , e2 =      0 1 ... 0  | | |  , . . . , en =      0 0 ... 1  | | |  . Let T : Fn → Fm be a linear map. Recall that if v = (v1, . . . , vn) ∈ Fn then n∑ k=1 vkT (ek). Let T (e1) =      a11 a21 ... am1  | | |  , T (e2) =      a12 a22 ... am2  | | |  , . . . , T (en) =      a1n a2n ... amn  | | |  and let A be the matrix with T (ek) as its kth column: A =   ↑ ↑ . . . ↑ T (e1) T (e2) . . . T (en) ↓ ↓ . . . ↓   =      a11 a12 . . . a1n a21 a22 . . . a2n ... ... . . . ... am1 am2 . . . amn  | | |  ∈ Mm,n(F). Then A contains all the information necessary to evaluate T and so we say that A is the matrix of the linear map T . We wish to deﬁne a multiplication rule between matrices and vectors so that we can represent T (v) as the product Av. If T (v) = Av then we have to deﬁne Av = n∑ k=1 vkT (ek) = v1      a11 a21 ... am1  | | |  + v2      a12 a22 ... am2  | | |  + · · · + vn      a1n a2n ... amn  | | |  =      v1a11 + v2a12 + · · · + vna1n v1a21 + v2a22 + · · · + vna2n ... v1am1 + v2am2 + · · · + vnamn  | | |  . 70 CHAPTER 3. LINEAR MAPS Hence the product Av must be computed according to the following rule: The kth entry of Av is the dot product of the kth row of A with v (without complex conjugation if the vectors are complex): (Av)k = n∑ j=1 akjvj = ak1v1 + ak2v2 + · · · + aknvn. Remark 3.2.2. Observe that the product of an m × n matrix and a column vector of length n is a column vector of length m. Example 3.2.3.  1 0 2 6 3 9   ( 4 5 ) =   1 · 4 + 0 · 5 2 · 4 + 6 · 5 3 · 4 + 9 · 5   =   4 38 57   ,  1 3 5 7 9 11 2 4 6     1 2 −1   =   1 · 1 + 3 · 2 + 5 · (−1) 7 · 1 + 9 · 2 + 11 · (−1) 2 · 1 + 4 · 2 + 6 · (−1)   =   2 14 4   . Example 3.2.4. Let T : R 3 → R 2 be the linear map given by T :  x y z   7→ ( 3x + y x − y − 2z ) . Find the matrix of T . Solution. Let e1, e2, e3 be the standard basis of R3. To ﬁnd the matrix for T we need to compute T (e1), T (e2), T (e3) and put them as the columns of the matrix. We have T (e1) = (3 1 ) , T (e2) = ( 1 −1 ) , T (e1) = ( 0 −2 ) . Hence the matrix for T is ( 3 1 0 1 −1 −2 ) . We can check that ( 3 1 0 1 −1 −2 )  x y z   = ( 3x + y x − y − 2z ) as expected. 3.2. MATRIX OF A LINEAR MAP 71 3.2.2 Matrix of linear map between general vector spaces Let V and W be vector spaces with dim V = n and dim W = m. Suppose that we ﬁx bases E = {e1, e2, . . . , en} and F = {f1, f2, . . . , fm} of V and W respectively. Then we can repeat the arguments above to get a matrix for a linear map T : V → W with respect to the bases E and F. Recall that if v = α1e1 + α2e2 + · · · + αnen, the coordinate vector of v with respect to E is the column vector [v] = [v]E =      α1 α2 ... αn  | | |  ∈ Fn. Since F is a basis of W , we can write each of the vectors T (e1), T (e2), . . . , T (en) as a linear combination of f1, f2, . . . , fm. Let T (e1) = a11f1 + a21f2 + · · · + am1fm T (e2) = a12f1 + a22f2 + · · · + am2fm ... ... ... T (en) = a1nf1 + a2nf2 + · · · + amnfm. As before the coeﬃcients ajk determine T completely. We can put them into a matrix A, so that the kth column of A is the coordinate vector of T (ek) with respect to the basis F: A =   ↑ ↑ . . . ↑ [T (e1)]F [T (e2)]F . . . [T (en)]F ↓ ↓ . . . ↓   =      a11 a12 . . . a1n a21 a22 . . . a2n ... ... . . . ... am1 am2 . . . amn  | | |  ∈ Mm,n(F). Then with v = α1e1 + α2e2 + · · · + αnen, we have that [T (v)]F = α1[T (e1)]F + α2[T (e2)]F + · · · + αn[T (en)]F and thus [T (v)]F = A[v]E. We say that A is the matrix of T with respect to the bases E and F. Let us summarise: 72 CHAPTER 3. LINEAR MAPS To get the matrix for a linear map T with respect to the bases E = {e1, e2, . . . , en} and F = {f1, f2, . . . , fm}, we have to 1. express each of T (e1), T (e2), . . . , T (en) as a linear combination of f1, f2, . . . , fm; 2. write the coeﬃcients in a matrix so that the kth column consists of the coeﬃ- cients of T (ek). We will sometimes denote this matrix by [T ] E F . When the bases are ﬁxed we may just write [T ]. More often, when it does not lead to confusion, we will not distinguish between a linear map and its matrix and use the same symbol for both. This will particularly be the case for linear maps F n to Fm considered with the standard basis. Remark 3.2.5. Since a linear map is essentially a multiplication, we often write T v instead of T (v). Similarly, we often write T S for the composition T ◦ S of the linear maps T and S. Note that the expression T v + u means T (v) + u and not T (v + u). Example 3.2.6. Let D : P3 → P3 denote the derivative map Df = df dt . Determine the matrix of D with respect to the monomial basis 1, t, t2, t 3. Solution. We have that D(1) = 0 = 0 + 0t + 0t 2 + 0t3 D(t) = 1 = 1 + 0t + 0t 2 + 0t3 D(t 2) = 2t = 0 + 2t + 0t 2 + 0t 3 D(t 3) = 3t 2 = 0 + 0t + 3t 2 + 0t 3 Hence the matrix for D is [D] =     0 1 0 0 0 0 2 0 0 0 0 3 0 0 0 0  | |  . 3.2. MATRIX OF A LINEAR MAP 73 Let us verify that this is correct. If f (t) = a0 + a1t + a2t 2 + a3t 3, its coordinate vector is [f ] = (a0, a1, a2, a3). Then [D][f ] =     0 1 0 0 0 0 2 0 0 0 0 3 0 0 0 0  | |      a0 a1 a2 a3  | |  =     a1 2a2 3a3 0  | |  . The right hand side is the coordinate vector of the function a1 + 2a2t + 3a3t 2. This is indeed the the derivative of f as expected. Keep in mind the following key point: The matrix a linear map T : V → W depends on the choice of bases for V and W . If one of the bases is changed, then the matrix will be diﬀerent. Example 3.2.7. Recall that the identity map I : Fn → Fn is given by Iv = v for all v ∈ Fn. Clearly the matrix for this map with respect to the standard basis is I = In =      1 0 . . . 0 0 1 . . . 0 ... ... . . . ... 0 0 . . . 1  | | |  Unsurprisingly, this is know as the identity matrix. Let n = 3 and consider the following basis of F3: F =   f1 =  1 0 0   , f2 =  1 1 0   , f3 =  1 1 1    ‖  . What is the matrix of I : F3 → F3 when we consider the domain with the standard basis E = {e1, e2, e3} but the range with the basis F? We have that I(e1) = e1 = f1, I(e2) = e2 = f2 − f1, I(e3) = e3 = f3 − f2. We conclude that [I]E F =  1 −1 0 0 1 −1 0 0 1   . This highlights the fact that the same map can have diﬀerent matrices when the spaces are considered with diﬀerent bases. 74 CHAPTER 3. LINEAR MAPS 3.2.3 Composition and matrix multiplication Let S : Fm → Fk and T : Fn → Fm be linear maps. Then the composition S ◦T : Fn → Fk is well-deﬁned (since the range of T is the domain of S) and linear. Let A be the k × m matrix for S and B be the m × n matrix for T . Observe that for v ∈ Fn S ◦ T (v) = S(T (v)) = S(Bv) = A(Bv). So to enable us to do algebra in an easy way, we want the matrix for S ◦ T to be the “product” of A and B; we just need to deﬁne this product appropriately. Let bj = T (ej) so that bj is the jth column of B: B =   ↑ ↑ . . . ↑ b1 b2 . . . bn ↓ ↓ . . . ↓   . Then the jth column in the matrix of S ◦ T is S ◦ T (ej) = S(T (ej)) = S(bj) = Abj. If we want the matrix of S ◦ T to be the product AB we have to deﬁne matrix multipli- cation in the following way: The product AB is obtained by multiplying each column in B by A and adjoining the results into a matrix. More formally, if A is a k × m and B is an m × n matrix, (AB)ij = (ith row of A) · (jth column of B) = m∑ l=1 ailblj = ai1b1j + ai2b2j + · · · + aimbmj. Remark 3.2.8. In order for the product AB to be deﬁned we must have that the number of columns in A is the same as the number of rows in B. We have the following properties of matrix multiplication: 1. A(BC) = (AB)C for all matrices A, B, C provided the products AB and BC are deﬁned. 3.3. INVERTIBLE LINEAR MAPS 75 2. A(B + C) = AB + AC for all matrices A, B, C provided the products AB and AC are deﬁned. 3. A(αB) = (αA)B = α(AB) for all matrices A, B and all scalars α provided the product AB is deﬁned. These follow from the corresponding properties for linear maps (or alternatively by ma- nipulating the sums in the deﬁnition of each product). Exercise. Prove each of the properties above. Warning: Matrix multiplication is not commutative! It is possible that AB is deﬁned whereas BA is not. Even when they are both deﬁned we may have AB ̸= BA (in fact it is very rare that two matrices commute). Careful analysis of the rule matrix multiplication also gives the following property relat- ing to the transpose: 4. (AB) T = BT A T for all matrices A, B provided the product AB is deﬁned. Note that the order of multiplication changes when we transpose. Example 3.2.9. Let A = ( −1 2 0 1 0 3 ) , B = (−1 0 3 4 ) , C = ( 1 1 1 −2 ) . Then A T (B + C) =   −1 1 2 0 0 3   ((−1 0 3 4 ) + ( 1 1 1 −2 )) =   −1 1 2 0 0 3   ( 0 1 4 2 ) =   4 1 0 2 12 6   . Exercise. Let A be an m × n matrix. Is the product AAT well deﬁned? If so, what are the dimensions of AAT ? 3.3 Invertible linear maps Let V be a vector space. Recall that the identity map I : V → V is given by Iv = v for all v ∈ V (we will write IV when we want to emphasize the space on which it acts). Then for linear maps A : U → V and B : V → U we have IA = A and BI = B. 76 CHAPTER 3. LINEAR MAPS Deﬁnition 3.3.1. Let A : V → W be a linear map. Then A is invertible or non-singular if there exist a linear map A −1 : W → V such that AA−1 = IW and A −1A = IV . In this case, we call A −1 the inverse of A. The inverse of an invertible linear map is unique. Indeed if B is another inverse of A then B = BI = B(AA −1) = (BA)A −1 = IA −1 = A −1. Suppose that dim V = n and dim W = m. Recall that In is the n × n identity matrix. Then if A is the m × n matrix of an invertible linear map V → W and A−1 is the n × m matrix corresponding to its inverse (with respect to the same bases), then AA −1 = Im and A −1A = In. In this case we say that that matrix A is invertible and that A−1 is the inverse of A. Observe that a linear map is invertible if and only if its matrix (with respect to any bases) is invertible. Warning: It is possible that there exists B : W → V such that AB = I but BA ̸= I or vice-versa. For example, ( 1 0 0 0 1 0 )  1 0 0 1 0 0   = ( 1 0 0 1 ) but  1 0 0 1 0 0   ( 1 0 0 0 1 0 ) =  1 0 0 0 1 0 0 0 0   . However, if there exists linear maps B, C : W → V such that AB = I and CA = I then B = IB = CAB = CI = C, so A is invertible and B = C = A −1. Proposition 3.3.2 (Properties of the inverse). Let A and B be linear maps (matrices) such that the product AB is deﬁned. Then the following properties hold: (i) if A is invertible then AT is invertible and (AT ) −1 = (A −1) T ; (ii) if A and B are invertible then AB is invertible and (AB) −1 = B−1A−1. Proof. (i) Using the fact that (AB) T = BT A T with B = A −1 we get that A T (A −1) T = (A −1A) T = I and (A −1) T A T = (AA−1) T = I. 3.3. INVERTIBLE LINEAR MAPS 77 (ii) This is a direct computation: (AB)(B−1A −1) = A(BB−1)A −1 = AA−1 = I and (B−1A −1)(AB) = B−1(A −1A)B = B−1B = I. Exercise. Let A : Fn → Fn be an invertible linear map. If A is symmetric, i.e A = AT , is A −1 also symmetric? An invertible linear map (and its inverse) can be used to convert properties of its domain to properties of its range and vice versa. An example of this is the following theorem. Theorem 3.3.3. Let A : V → W be an invertible linear map and let e1, e2, . . . , en be a basis of V . Then Ae1, Ae2, . . . , Aen is a basis of W. Proof. First we will show that Ae1, Ae2, . . . , Aen are linearly independent. To this end, we take α1, α2, . . . , αn ∈ F such that n∑ k=1 αkAek = 0. Then applying A −1 gives A −1 ( n∑ k=1 αkAek ) = n∑ k=1 αkA −1Aek = n∑ k=1 αkek = 0. Since e1, e2, . . . , en are linearly independent we have that α1 = α2 = · · · = αn = 0. Hence Ae1, Ae2, . . . , Aen are linearly independent. Next, we will show that Ae1, Ae2, . . . , Aen span W . For any w ∈ W , A−1w ∈ V , and since e1, e2, . . . , en span V , there exist α1, α2, . . . , αn ∈ F such that A −1w = n∑ k=1 αkek. Therefore w = AA −1w = A ( n∑ k=1 αkek ) = n∑ k=1 αkAek. This shows that Ae1, Ae2, . . . , Aen span W . 78 CHAPTER 3. LINEAR MAPS Remark 3.3.4. (1) Applying the previous theorem to A −1 we see that in fact e1, e2, . . . , en is a basis of V if and only if Ae1, Ae2, . . . , Aen is a basis of W. (2) Inspecting the proof shows that we can also replace “is a basis of” with “is linearly independent in” or “spans” in the statement and the result will still hold. If there exists an invertible linear map between vector spaces V and W we say that V and W are isomorphic. Isomorphic spaces have almost identical vector space properties (even though they may be very diﬀerent in other respects). For example, consider the map from Pn to Fn+1 given by a0 + a1t + · · · + ant n 7→      a0 a1 ... an  | | |  . Then this is invertible and linear. This identiﬁcation reveals that addition and scalar multiplication in Pn corresponds to addition and scalar multiplication in Fn+1 (since we just perform the operations on the coeﬃcients). Therefore, even though these are diﬀerent vector spaces, we can think of them informally as two ways of representing the same space. More generally, let E = {e1, e2, . . . , en} be a basis of V (so that V is n-dimensional). Recall that for v = α1e1 + α2e2 + · · · + αnen ∈ V , the coordinate vector of v with respect to E is the column vector [v]E =      α1 α2 ... αn  | | |  ∈ Fn. Then the map v 7→ [v]E is an invertible linear map from V to Fn. As a result, any statements that can be proved that can be proved for Fn can be converted into statements about V . Chapter 4 Systems of Linear Equations The main motivation behind the development of linear algebra and matrices was the study and solution of systems of linear equations. Recall that a linear equation in the unknowns x1, x2, . . . , xn is an equation of the form a1x1 + a2x2 + · · · + anxn = b, and a system of linear equations (or sometimes just linear system) is a ﬁnite collection of linear equations. 4.1 Geometry of linear equations In this section, all the equations are real and so we consider only real solutions (the discussion below extends to C n but it is diﬃcult to visualise and so we avoid it here). Consider two linear equations in two unknowns: a1x + b1y = c1 a2x + b2y = c2. Suppose that the coeﬃcients ak and bk on each row are not both zero. Then the set of solutions of each row is a line in R 2. The set of solutions of this linear system is the intersection of these two lines. There are three possible arrangements of three lines in R 2: (1) Lines intersect at a point. The point of intersection is the unique solution. 79 80 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS (2) Lines are parallel. Since the lines do not intersect, the system has no solutions. (3) Lines are equal. Every point on the line is a solution and so there are inﬁnitely many solutions (sometimes caled a 1-parameter family of solutions). Example 4.1.1. Consider the linear system x − 2y = 3 −3x + 6y = 7. This system belongs to case (2) above and hence it has no solutions (it is inconsistent). Next, let us consider a system with two equations in three unknowns: a1x + b1y + c1z = d1 a2x + b2y + c2z = d2. In this case, assuming the coeﬃcients on each row are not all zero, each equation de- termines a plane in R 3. As before, we can determine the number of possible solutions by considering the possible arrangements of two planes in R3. Now we can have the following arrangements: (1) Planes intersect at a line. There are inﬁnitely many solutions (1-parameter family of solutions). (2) Planes are parallel. The system has no solutions. (3) Planes are equal. There are inﬁnitely many solutions (a 2-parameter family of solutions). In particular, this shows that two equations in three unknowns can never have a unique solution. With three equations however, we can have a unique solution, but there is also more variety of things that could go wrong. To be precise, the possible arrangements of three planes in R 3 are: (1) Planes intersect at a point. There is a unique solution. (2) Two parallel planes. The system has no solutions. (This includes the case where exactly two planes are equal.) (3) No parallel planes but no intersection. The intersection of any two planes will be a line but the intersection of all three will be empty so there are no solutions. (This is the most common case when we don’t have a unique solution.) 4.2. ELIMINATION 81 (4) All three planes intersect at a line. There are inﬁnitely many solutions (1- parameter family). (5) All three planes are parallel. The system has no solutions. (6) All three planes are equal. There are inﬁnitely many solutions (2-parameter family). We can extend this reasoning to higher dimensions but we will have diﬃculty imagining 7-dimensional hyperplanes in R8! To analyse higher dimensional analogues we need the techniques of linear algebra, but it is helpful to keep these pictures in mind. 4.2 Elimination 4.2.1 Notation. Matrix Form Consider a linear system of m equations with n unknowns x1, x2, . . . , xn: a11x1 + a12x2 + · · · + a1nxn = b1 a21x1 + a22x2 + · · · + a2nxn = b2 ... ... ... am1x1 + am2x2 + · · · + amnxn = bm. To solve this system is to ﬁnd all collections x1, x2, . . . , xn ∈ F such that these equations hold simultaneously. Let A =      a11 a12 . . . a1n a21 a22 . . . a2n ... ... . . . ... am1 am2 . . . amn  | | |  , x =      x1 x2 ... xn  | | |  , b =      b1 b2 ... bm  | | |  . Then the above system of linear equations can be written in matrix form: Ax = b. To solve this is to ﬁnd all x ∈ Fn satisfying Ax = b. The set of solutions is generally referred to as the general solution when it is given in parametric form. We call A the coeﬃcient matrix and b the vector of constants. 82 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS Observe that it does not matter what we call the unknowns; we could call them xk, yk or anything else. So all the information necessary to solve the system is contained in the the matrix A and the vector b. As a result, we often combine the coeﬃcient matrix and the constant vector to form the augmented matrix of the system:      a11 a12 . . . a1n b1 a21 a22 . . . a2n b2 ... ... . . . ... ... am1 am2 . . . amn bm  | | |  . It is customary to put a vertical line separating A and b to distinguish between the coeﬃcient matrix and the augmented matrix. 4.2.2 The idea of elimination Linear systems are solved by elimination (or row reduction). This is just a systematic form of the technique that you already know for solving simultaneous linear equations. Example 4.2.1. Let us solve the system x + 2y = −2 (4.2.1) 2x + y = 7. (4.2.2) We start by subtracting 2 × (4.2.1) from (4.2.2): x + 2y = −2 (4.2.3) −3y = 11. (4.2.4) This instantly gives y = −11/3. Then substituting back we get x = −2 − 2y = −2 + 22/3 = 16/3. Alternatively, instead of back substitution, we could eliminate y from the ﬁrst equation. In particular, we multiply (4.2.4) by −1/3: x + 2y = −2 (4.2.5) y = −11/3; (4.2.6) then subtract 2 × (4.2.6) from (4.2.5): x = 16/3 y = −11/3. 4.2. ELIMINATION 83 Observe that we could have carried out this sequence of operations directly on the aug- mented matrix for the system. The ﬁrst step would be ( 1 2 −2 2 1 7 ) R2→R2−2R1 −−−−−−−→ (1 2 −2 0 −3 11 ) . (4.2.7) and the ﬁnal two steps would be (1 2 −2 0 −3 11 ) R2→− 1 3 ×R2 −−−−−−−→ ( 1 2 −2 0 1 −11/3 ) R1→R1−2R2 −−−−−−−→ (1 0 16/3 0 1 −11/3 ) . (4.2.8) This is the idea of elimination, to perform operations on the rows of the augmented matrix (or equivalently on the equations themselves) until we reduce it to a simple form where one can write down the solutions. In general, we simplify our system by performing elementary row operations of the fol- lowing types: I. interchange two rows of the matrix; II. multiply a row by a non-zero scalar; III. add a multiple of one row to a diﬀerent row. These operations do not change the set of solutions - that is, the solutions of the linear system obtained after applying an elementary row operation are the same as the solutions of the original system. 4.2.3 Echelon and reduced echelon form Recall that the idea of elimination is to perform row operations to an augmented matrix of a linear system to reduce it to a simple form where we can easily write down the solutions. Here we make precise what is meant by ‘simple form’. In particular we will introduce matrices of two forms: ‹ Echelon form - for augmented matrices in this form we can use back substitution to determine the general solution. For example the matrix on the right of (4.2.7) is in echelon form. ‹ Reduced echelon form - for augmented matrices in this form we can read oﬀ the general solution directly without the need for back substitution. For example, the matrix on the right of (4.2.8) 84 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS Deﬁnition 4.2.2. A matrix is in echelon form if 1. all zero rows, if there are any, are below all non-zero rows; 2. for each non-zero row, its left-most non-zero entry is strictly to the right of the left-most non-zero entry of the row above. The left-most non-zero entry in each row in echelon form is called a pivot entry or simply pivot. A matrix is in reduced echelon form (or REF for short) if it is in echelon form and 3. all pivot entries are equal to 1; 4. all entries above the pivots are zero. Example 4.2.3. Consider the following matrices: A =  0 0 1 4 0 0 0 0 0 0 0 0   , B =  0 0 5 1 0 0 0 0 0   , C =     1 1 0 0  | |  , D =  3 0 1 0 2 1 0 1 0   , E = ( 3 1 1 2 0 4 0 1 ) , F =  1 0 0 1 0 1 2 0 0 0 0 0   . We have the following: ‹ A is in REF (and hence in echelon form). ‹ B is not in echelon form because the pivot in row 2 is to left of the pivot in row 1. This can be put in echelon form by swapping the ﬁrst two rows, and then into REF by multiplying the second row by 5. ‹ C is not in echelon form because the pivot in row 2 is directly below the pivot in row 1 (it must be strictly to the right). Making this entry 0, by subtracting the ﬁrst row, would put it in REF. ‹ D is not in echelon form because the pivot in row 3 is directly below the pivot in row 2. Subtracting 1/2 times row 2 from row 3 will put it in echelon form. To put it in REF we would then have to multiply each row by a constant to make the pivots equal to 1 and then make the entries above the pivots zero. 4.2. ELIMINATION 85 ‹ E is in echelon form, but not in REF because the pivots are not equal to 1 and the entry above the pivot in row 2 is not equal to zero. ‹ F is in REF. Remark 4.2.4. Every matrix can be put into REF by a sequence of elementary opera- tions and the reduced echelon form of a matrix is unique. We can easily determine the solutions from an augmented matrix in REF. For example, suppose the REF of the augmented matrix is   1 2 0 −1 0 2 0 0 1 −3 0 1 0 0 0 0 1 4   , where we have boxed the pivots. The variables corresponding to columns without pivots are called free variables, so in this example the free variables are x2 and x4. They are called free because the other variables can be written in terms of them, therefore they can be deﬁned freely and then the remaining variables are ﬁxed. The aim is to move the free variables to the right side. So the solution of this system is x1 = 2 − 2x2 + x4, x2 is free, x3 = 1 + 3x4, x4 is free, x5 = 4. Since x2 and x4 are free, let us set x2 = t and x4 = s (this is only notational, not mathematical). Then writing the solution in vector form we get that any solution x is of the form x =       2 − 2t + s t 1 + 3s s 4  | | | |  =       2 0 1 0 4  | | | |  + t       −2 1 0 0 0  | | | |  + s       1 0 3 1 0  | | | |  . Hence the set of solutions is a plane in R 5 (2-parameter family of solutions). We can put a matrix into echelelon (or reduced echelon) form using the following algo- rithm: 86 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS Gauss-Jordan elimination algorithm: 1. If the matrix consists entirely of zeros, stop, it is in echelon form. 2. Find the ﬁrst column from the left containing a non-zero entry and use a type I row operation to put that entry into the top row - this is the pivot entry for the row. 3. Add an appropriate multiple of the ﬁrst row to each row below (type III operations) to make all entries below the pivot zero. 4. Apply Steps 1-3 to the rows below the current row. Repeat this until no rows remain or the remaining rows are zero. The above algorithm will put the matrix into echelon form. In order to put it in REF we also carry out the following steps: 5. Multiply each non-zero row by an appropriate constant (a type II operation) to make the pivot entry 1. 6. Working from right to left, use type III operations to make all entries directly above each pivot zero. Example 4.2.5. Put the following matrix into REF:  0 0 3 1 2 0 −4 0 4 1 2 1   . Solution. Applying the Gauss-Jordan elimination algorithm we get the following:  0 0 3 1 2 0 −4 0 4 1 2 1   R1↔R2 −−−−→   2 0 −4 0 0 0 3 1 4 1 2 1   R3→R3−2R1 −−−−−−−→  2 0 −4 0 0 0 3 1 0 1 10 1   R2↔R3 −−−−→  2 0 −4 0 0 1 10 1 0 0 3 1   R3→ 1 3 R3 −−−−−→ R1→ 1 2 R1  1 0 −2 0 0 1 10 1 0 0 1 1/3   R2→R2−10R3 −−−−−−−−→ R1→R1+2R3  1 0 0 2/3 0 1 0 −7/3 0 0 1 1/3   . This is now in REF. 4.2. ELIMINATION 87 4.2.4 Elementary matrices Deﬁnition 4.2.6. A square matrix is elementary if it diﬀers from the identity matrix by one elementary row operation. Every elementary row operation is equivalent to multiplication on the left by an elemen- tary matrix. Suppose we want to apply a row operation R to a matrix A. Let E be the matrix obtained by applying R to the identity matrix. Then the matrix obtained by applying R to A is EA. For example, if A has 4 rows we have the following: (1) Interchanging row 2 and row 4 is equivalent to left multiplication by     1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0  | |  . (2) Multiplying row 3 by α is equivalent to left multiplication by     1 0 0 0 0 1 0 0 0 0 α 0 0 0 0 1  | |  . (3) Adding α times row 1 to row 2 is equivalent to left multiplication by     1 0 0 0 α 1 0 0 0 0 1 0 0 0 0 1  | |  . Remark 4.2.7. Since row operations are reversible, elementary matrices are invertible. Let A be a matrix. Suppose it can be put into echelon form by a sequence of row operations R1, R2, . . . , Rk. Then each row operation Rj corresponds to left multiplication by an elementary matrix Ej. Set E = Ek . . . E2E1. Then the matrix EA = Ek . . . E2E1A is in echelon form. Moreover, since each Ej is invertible, E is also invertibe. lWe conclude the following: 88 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS Given a matrix A, we can ﬁnd an invertible matrix E such that EA is in echelon form (or reduced echelon form). Exercise. Let A be the matrix in Example 4.2.5. Find an invertible matrix E such that EA is in echelon form. Find a matrix F such that F A is in REF. 4.2.5 Analysing the pivots All questions regarding existence and uniqueness of solutions of a linear system can be answered by analysing the pivots in the echelon (or reduced echelon) form of the augmented matrix. First we consider when a linear system has no solutions. We call a system with no sultions inconsistent, otherwise we say it is consistent. Proposition 4.2.8. A linear system is inconsistent (i.e. has no solutions) if and only if the echelon form of the augmented matrix has a pivot in the last column. Proof. The echelon form of the augmented matrix has a pivot in the last column if and only if it has a row of the form (0 0 . . . 0 b) , for some b ̸= 0. This row corresponds to the equation 0x1 + 0x2 + · · · + 0xn = b ̸= 0, which clearly has no solutions. If we do not have such a row we can put the matrix into REF and read oﬀ a solution. We can also determine uniqueness of solutions by analysing pivots; here one only needs to consider the coeﬃcient matrix. Proposition 4.2.9. Let A : Fn → Fm be a linear map (matrix). (i) The linear system Ax = b is consistent for all right sides b ∈ Fm if and only if the echelon form of the coeﬃcient matrix has a pivot in every row. (ii) The linear system Ax = b has a unique solution for every right side b ∈ F m if and only if the echelon form of the coeﬃcient matrix has a pivot in every column and every row. 4.2. ELIMINATION 89 Proof. (i) Let E be an invertible matrix such that EA is in echelon form. If EA has a pivot in every row, we cannot have a pivot in the last column of the augmented matrix( EA Eb) . Hence the system is always consistent, regardless of b. Suppose that EA does not have a pivot in the last row, i.e. it has a zero row. Then we can choose b = E−1em, where em = (0, . . . , 0, 1), so that (EA Eb) = ( EA EE−1em) = ( EA em) , which is inconsistent. (ii) It’s clear that a solution (if it exists) is unique if and only if there are no free variables (these are the source of the non-uniqueness). This happens precisely when the echelon form of the coeﬃcient matrix has a pivot in every column. Combining this observation with (i) proves (ii). Proposition 4.2.9 has the following immediate consequences. Corollary 4.2.10. A matrix A is invertible if and only if its echelon form has a pivot in every column and every row. Proof. By Question 6 on Assignment 7, A is invertible if and only if the equation Ax = b has a unique solution for every right side b. The claim now follows from Proposition 4.2.9(ii). Remark 4.2.11. Any row or column of a matrix in echelon form can have at most one pivot in it. Therefore if the echelon form of a matrix has a pivot in every row and every column, it must have the same number of rows and columns. In particular, this shows us that an invertible matrix must be square. Questions about whether a collection of vectors in Fn is linearly independent or spanning can also be answered with row reduction. Proposition 4.2.12. Let v1, v2, . . . , vm be vectors in Fn. Let A be the n × m matrix with columns v1, v2, . . . , vm, i.e. A =   ↑ ↑ . . . ↑ v1 v2 . . . vm ↓ ↓ . . . ↓   . Then the following hold: 90 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS (i) the vectors v1, v2, . . . , vm are linearly independent if and only if the echelon form of A has a pivot in every column; (ii) the vector v1, v2, . . . , vm span Fn if and only if the echelon form of A has a pivot in every row; (iii) the vector v1, v2, . . . , vm form a basis of F n if and only if the echelon form of A has a pivot in every column and every row. Proof. (i) The vectors v1, v2, . . . , vm are linearly independent if and only if the equation x1v1 + x2v2 + · · · + xmvm = 0 has the unique solution x1 = x2 = · · · = xm = 0. Equivalently, the equation Ax = 0 has unique solution x = 0. This happens precisely when there are no free variables, i.e. when the echelon form of A has a pivot in every column. (ii) The vectors v1, v2, . . . , vm span Fn if and only if the equation x1v1 + x2v2 + · · · + xmvm = b has a solution x = (x1, x2, . . . , xm) for any right side b. By Proposition 4.2.9(i), this happens precisely when the echelon form of A has a pivot in every row. (iii) Combine (i) and (ii). 4.2.6 Consequences for square matrices Recall that it is possible for a matrix A to have have a left inverse, i.e. a matrix B such that BA = I, but not a right inverse so that AB ̸= I. Similarly, a matrix may have a right inverse but not a left inverse. This is not possible when A is square. Proposition 4.2.13. Let A be a (square) n × n matrix. Then A has a left inverse if and only if it has a right inverse. In general, if you have a linear map A and you want to check that another linear map B is the inverse of A, you have to check that both AB = I and BA = I hold. This proposition shows that if you know that the matrix of A is square (i.e. that the domain and range have the same dimension), then it is suﬃcient to check just one of these and you get the other one automatically. 4.2. ELIMINATION 91 Proof. Suppose that B is a left inverse of A so that BA = I. Then if the equation Ax = b has a solution x, we must have that x = BAx = Bb and so x = Bb is the unique solution. Since every equation Ax = b that is consistent has a unique solution, by Proposition 4.2.9(ii) the echelon form of A must have a pivot in every column. Since A is square this means it also has a pivot in every column. Then by Corollary 4.2.10 A must be invertible. Conversely, suppose that C is a right inverse of A so that AC = I. Then for any b ∈ F n we have b = Ib = ACb, so that Cb is a solution of the equation Ax = b. Since the equation Ax = b is consistent for every b ∈ Fn, by Proposition 4.2.9(i) the echelon form of A must have a pivot in every row. As before, this means it also has a pivot in every column and so it is invertible by Corollary 4.2.10. The key point in the proof was that for square matrices, having a pivot in every row is equivalent to having a pivot in every column. We can use this to show that all properties of a matrix that are determined by whether or not the its echelon form has pivot in every row or every column are equivalent for square matrices. For example, we have the following: Proposition 4.2.14. Let A be a (square) n × n matrix. Then A is invertible if and only if the equation Ax = 0 has a unique solution. Proof. We have already seen that if A is invertible then the equation Ax = b has a unique solution for all b ∈ Fn, so one direction is immediate. Suppose that the system Ax = 0 has a unique solution. This means that there are no free variables and hence that the echelon form of A has a pivot in every column. Since A is square it must also have a pivot in every row, so by Corollary 4.2.10 it is invertible. 4.2.7 Computing the inverse by elimination Let A be an invertible matrix. As discussed above, A must be square and its echelon form must have a pivot in every row and column. Therefore its reduced echelon form must be the identity matrix I. From the discussion in Section 4.2.4, there exists a matrix E such that EA = I. Hence E must be the inverse of A, i.e. E = A −1. Recall that E is the product of the elementary matrices corresponding to the row operations required to put A into REF. Thus E can be computed by applying the same row operations to the identity matrix. This gives us the following algorithm for computing A−1 for an n × n matrix A: 92 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS 1. Form and augmented n × 2n matrix ( A I). 2. Perform row operations on the augmented matrix to transform A to the identity matrix I. 3. The matrix I that was added will be transformed to A −1. Remark 4.2.15. If it is not possible to transform A to the identity matrix by row operations then A is not invertible. After applying the algorithm above, the augmented matrix ( A I) will be transformed to the matrix (EA EI) = ( I E) . Example 4.2.16. Compute the inverse of the matrix A =  1 −1 0 1 0 −2 1 −1 1   . Solution. First we form the augmented matrix ( A I) =   1 −1 0 1 0 0 1 0 −2 0 1 0 1 −1 1 0 0 1   . Then we apply row reduction to transform A into I:  1 −1 0 1 0 0 1 0 −2 0 1 0 1 −1 1 0 0 1   R3→R3−R1 −−−−−−−→ R2→R2−R1  1 −1 0 1 0 0 0 1 −2 −1 1 0 0 0 1 −1 0 1   R2→R2+2R3 −−−−−−−→  1 −1 0 1 0 0 0 1 0 −3 1 2 0 0 1 −1 0 1   R1→R1+R2 −−−−−−−→  1 0 0 −2 1 2 0 1 0 −3 1 2 0 0 1 −1 0 1   . We conclude that A is invertible and A −1 =   −2 1 2 −3 1 2 −1 0 1   . We can check that   −2 1 2 −3 1 2 −1 0 1    1 −1 0 1 0 −2 1 −1 1   =  1 −1 0 1 0 −2 1 −1 1    −2 1 2 −3 1 2 −1 0 1   =  1 0 0 0 1 0 0 0 0   . 4.3. PRINCIPLE OF LINEARITY 93 4.3 Principle of linearity Deﬁnition 4.3.1. A linear equation (linear system) Ax = b is homogeneous if b = 0 - that is, a homogeneous linear equation is an equation of the form Ax = 0. Otherwise we say it is inhomogeneous. Given a linear system Ax = b, we call the system Ax = 0 the associated homogeneous system. For example the associated homogeneous system of the linear system ( 1 −5 2 3 ) ( x1 x2 ) = (2 7 ) is the system ( 1 −5 2 3 ) ( x1 x2 ) = (0 0 ) . Consider a linear system Ax = b and its associated homogeneous system Ax = 0. Suppose x0 is solution of the original system, i.,e Ax0 = b, and suppose that xh is a solution of the homogeneous system, i.e. Axh = 0. Then A(x0 + xh) = Ax0 + Axh = b + 0 = b. Hence x0 + xh is also a solution of the original system. Now suppose that x1 is another solution of the original system. Then A(x1 − x0) = Ax1 − Ax0 = b − b = 0. Hence x1 − x0 is a solution of the homogeneous system. Moreover, we have x1 = x0 + (x1 − x0), and thus every solution of the original system is the sum of x0 and a solution of the homogeneous system. We have proved the following: 94 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS Theorem 4.3.2 (Principle of linearity). Let A : V → W be a linear map and let b ∈ W . Let x0 ∈ V satisfy the equation Ax0 = b and let H denote the set of solutions of the associated homogeneous equation Ax = 0. Then the set {x0 + xh : xh ∈ H} is the set of all solutions of the equation Ax = b. This can be restated as General solution of Ax = b = A particular solution of Ax = b + General solution of Ax = 0 Any particular solution will do in the above statement. Remark 4.3.3. Observe that Theorem 5.2 is valid for all linear maps. In particular, it is valid for linear maps between inﬁnite dimensional spaces where one cannot simply use elimination to determine the general solution. Exercise. Let A : V → W be a linear map. Which, if any, of the following statements are true? (a) The equations Ax = b and Ax = 0 have the same solutions for all b ∈ W . (b) The equations Ax = b and Ax = 0 have the same solutions for some b ∈ W . (c) The equations Ax = b and Ax = 0 have the same number of solutions for all b ∈ W . (d) The equations Ax = b and Ax = 0 have the same number of solutions for some b ∈ W . Example 4.3.4. Consider the linear system     1 1 −1 2 3 0 1 −3 7 1 1 −4 0 3 −4 9  | |  x =     2 −1 0 7  | |  . Show that the general solution of this sytem is x =     0 1 −1 0  | |  + t     −1 5 6 1  | |  + s     2 −7 −3 1  | |  , t, s ∈ F. (4.3.1) 4.3. PRINCIPLE OF LINEARITY 95 Solution. One way to proceed is to perform elimination. However, this is generally time consuming. Moreover, elimination produces the general solution x =     −1/3 7/3 0 0  | |  + t     −1/3 4/3 1 0  | |  + s     1 −3 0 1  | |  , t, s ∈ F, which looks diﬀerent, and so we would still be left with the task of verifying that the two sets of solutions are indeed the same. An easier approach is to use the principle of linearity. We ﬁrst check that     1 1 −1 2 3 0 1 −3 7 1 1 −4 0 3 −4 9  | |      0 1 −1 0  | |  =     2 −1 0 7  | |  , and so (0, 1, −1, 0) is a particular solution. We then check that     1 1 −1 2 3 0 1 −3 7 1 1 −4 0 3 −4 9  | |      −1 5 6 1  | |  =     1 1 −1 2 3 0 1 −3 7 1 1 −4 0 3 −4 9  | |      2 −7 −3 1  | |  = 0, and so t     −1 5 6 1  | |  + s     2 −7 −3 1  | |  is a solution of the associated homogeneous system for every t, s ∈ F. It follows that (4.3.1) is a solution of the original system for any t, s ∈ F. This does not tell us that we have all of the solutions (the general solution). To see this we note that the ﬁrst two columns of the coeﬃcient matrix are linearly independent and so there cannot be more than two free variables in the general solution. This shows that (4.3.1) is indeed the general solution. 4.3.1 Application to diﬀerential equations Fix λ ∈ R and consider the ﬁrst order linear ODE du dx = λu(x). 96 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS The general solution to this equation is u(x) = Ce λx, where C is an arbitrary constant. This can also be phrased in terms of linear maps. Let V be the (real) vector space of all smooth (inﬁnitely diﬀerentiable) functions u : R → R. Let D : V → V be the linear map deﬁned by Du(x) = du dx , u ∈ V. Then the statement above can be rephrased as saying that the general solution of the homogeneous linear equation (D − λI)u = 0 is u(x) = Ce λx, C ∈ R. Next, consider the second order equation a d 2u dx2 + bdu dx + cu(x) = 0, (4.3.2) where a, b, c ∈ R are constants. If we let T : V → V be the linear map T = aD2 +bD +cI (recall that D2 = D ◦ D) is the second derivative) then (4.3.2) can be written as T u = 0. It is still reasonable to look for a solution of the form u(x) = eλx. Substituting this into (4.3.2) we get (aλ2 + bλ + c)e λx = 0. Since e λx > 0 for all x ∈ R we must have aλ2 + bλ + c = 0. This equation has two roots, λ1 and λ2 say. Therefore (4.3.2) is satisﬁed whenever u(x) = eλ1x or u(x) = e λ2x. Moreover, if λ1 ̸= λ2 then the set of linear combinations of the form u(x) = C1eλ1x + C2eλ2x, C1, C2 ∈ R, give the general solution of (4.3.2) (a proof this last claim is beyond the scope of this course). Example 4.3.5. Determine the general solution of the linear ODE d 2u dx2 − du dx − 6u(x) = 6 cos 3x. (4.3.3) Before proceeding to the solution let us observe that this is a linear equation in the vector space V os smooth functions. Indeed setting T = D2 − D − 6I and b(x) = 6 cos 3x, we see that we are looking for the general solution in V of the linear equation T u = b. 4.3. PRINCIPLE OF LINEARITY 97 Since V is inﬁnite dimensional we cannot use elimination. However, we can use the principle of linearity. Solution. First we try to “guess” a particular solution. Since the right hand side is 6 cos 3x, we look for a solution of the form u(x) = α cos 3x + β sin 3x. In this case, we will have du dx = −α sin 3x + β cos 3x and d2u dx2 = −α cos 3x + −β sin 3x Substituting these into (4.3.3) we get −9α cos 3x − 9β sin 3x + 3α sin 3x − 3β cos 3x − 6α cos 3x − 6β sin 3x = 6 cos 3x and so (−15α − 3β) cos 3x + (3α − 15β) sin 3x = 6 cos 3x. Equating coeﬃcients of cos 3x and sin 3x gives −15α − 3β = 6, 3α − 15β = 0. This has solution α = −5, β = −1. Therefore u(x) = −5 cos 3x − sin 3x is a particular solution of (4.3.3). Next we need to ﬁnd the general solution of the associated homogeneous equation d2u dx2 − du dx − 6u(x) = 0. (4.3.4) Substituting u(x) = e λx into (4.3.4) we get (λ 2 − λ − 6)e λx = 0. Dividing by eλx we see that λ 2 − λ − 6 = (λ − 3)(λ + 2) = 0. Thus the general solution of (4.3.4) is u(x) = C1e 3x + C2e −2x, C1, C2 ∈ R, Combining these we ﬁnd that the general solution of (4.3.3) is u(x) = −5 cos 3x − sin 3x + C1e 3x + C2e −2x, C1, C2 ∈ R. 98 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS 4.4 Image and kernel of a linear map We have seen that many properties of linear maps are related to the existence and uniqueness of solutions of the form Ax = b. Thus we are naturally lead to consider the following subspaces. Deﬁnition 4.4.1. Let A : V → W be a linear map. The image (or column space) of A, denoted Im A, is the set of all vectors w ∈ W such that w = Av for some v ∈ V , i.e Im A := {Av ∈ W : v ∈ V }. The kernel (or null space) of A, denoted Ker A is the set of all vectors v ∈ V such that Av = 0, i.e. Ker A = {v ∈ V : Av = 0}. Equivalently, Im A is the set of right sides b such that the equation Ax = b has solutions and ker A is the set of solutions of the homogeneous equation Ax = 0. Let A : Fn → Fm be given by an m × n matrix with columns a1, a2, . . . , an ∈ F m, i.e A =   ↑ ↑ . . . ↑ a1 a2 . . . an ↓ ↓ . . . ↓   . Then for x = (x1, x2, . . . , xn) we have that Ax = x1a1 + x2a2 + . . . x2an. Therefore Im A = span{a1, a2, . . . , an}. This is the reason for the term “column space”. Remark 4.4.2. If A is an m × n matrix, we can also consider the range and kernel of A T . Im A T and Ker A T sometimes called the row space and left null space respectively. The four spaces Im A, Ker A, Im A T and Ker A T are called the fundamental subspaces of A. Example 4.4.3. Recall that Pn is the set of all polynomials in one variable t of degree at most n. Let D : Pn → Pn be the diﬀerentiation map Dp(t) = dp dt . Since the derivative of every polynomial in Pn is a polynomial in Pn−1 and every poly- nomial in Pn−1 is the derivative of some polynomial in Pn, we have that Im D = Pn−1. We also have that the derivative of a function p(t) is zero if and only if p(t) is constant. Hence Ker D = {p ∈ Pn : p(t) ≡ c for some constant c ∈ F}. 4.4. IMAGE AND KERNEL OF A LINEAR MAP 99 Proposition 4.4.4. Let A : V → W be a linear map. Then Im A is a subspace of of W and Ker A is a subsapce of V . Proof. This is left as an assignment question. Remark 4.4.5. Observe that a linear map A : V → W is surjective if and only if Im A = W (this is just the deﬁnition of being surjective). Proposition 4.4.6. Let A : V → W be a linear map. A is injective if and only if Ker A = {0}. Proof. First, let us assume that Ker A = {0}. Take v1, v2 ∈ V with Av1 = Av2. We need to show that v1 = v2. We have that A(v1 − v2) = Av1 − Av2 = 0, and so v1 − v2 ∈ Ker A. Since Ker A = {0} we have that v1 − v2 = 0, i.e v1 = v2. Now let us assume that there exists a non-zero vector v ∈ Ker A. Then A0 = 0 and Av = 0 with v ̸= 0. Hence A is not injective. Exercise. Let A : V → V be a linear map. Which, if any, of the following statements are true? (a) If A is injective then A2 is injective. (b) If A is sujective then A 2 is surjective. (c) If A2 is injective then A is injective. (d) If A2 is sujective then A is surjective. The dimensions of the kernel and image of a linear map contain important information about it, and are related to each other. Deﬁnition 4.4.7. Let A : V → W be a linear map. The rank of A, denoted r(A), is the dimension of the image of A, i.e. r(A) := dim Im A. The nullity of A, denoted n(T ), is the dimension of the kernel of A, i.e. n(A) = dim Ker A. Let A be an m × n matrix with columns a1, a2, . . . , an and let E be an invertible matrix such that EA is in echelon form. Then the columns of EA are Ea1, Ea2, . . . Ean. Let us suppose that the pivot columns of EA are Ea1, . . . Eak, k ≤ n (we can relabel the columns if necessary). Then Ea1, . . . Eak form a basis for Im EA. Multiplying each of these by the invertible matrix E−1 we see that a1, . . . ak are linearly independent. We 100 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS claim that they also span Im A. Indeed for any v ∈ Im A, Ev ∈ Im EA and so there are scalars β1, . . . βk such that Ev = β1Ea1 + β2Ea2 + · · · + βkEak. Then multiplying both sides by E−1 gives v = β1a1 + β2a2 + · · · + βkak. We conclude that a1, . . . ak are a basis for Im A. To compute a basis for the kernel of A, we have to solve the homogeneous equation Ax = 0. The dimension of Ker A will be number of parameters in the general solution, i.e. the number of free variables. To summarise, we have shown the following: 1. The pivot columns of A, i.e.columns of A (not its echelon form) corresponding to pivot variables, form a basis for Im A. 2. The rank of A is the number of pivots in the reduced echelon form of A. 3. The nullity of A is the number free columns (i.e columns without pivots) in the echelon form of A The following theorem now follows trivially, but it extremely important in linear algebra. Theorem 4.4.8 (Rank-nullity theorem). Let A : V → W be a linear map, where V is ﬁnite dimensional. Then r(A) + n(A) = dim V. Example 4.4.9. Consider the matrix A =     1 1 1 3 −2 3 3 1 2 −1 2 2 0 −1 1 −1 −1 1 4 −3  | |  . (a) Determine the rank of A and ﬁnd a basis for Im A. (b) Determine the nullity of A and ﬁnds a basis for Ker A. 4.4. IMAGE AND KERNEL OF A LINEAR MAP 101 Solution. After row reducing A we ﬁnd that its echelon form is     1 1 1 3 −2 0 0 -2 −7 5 0 0 0 0 0 0 0 0 0 0  | |  . (a) This matrix has two pivots and so r(A) = 2. Moreover, the pivots are in the ﬁrst and third columns and so the ﬁrst ﬁrst and third columns of A,     1 3 2 −1  | |  and     1 1 0 1  | |  , form a basis for Im A. (b) This matrix has 3 columns corresponding to free variables and so n(A) = 3. To ﬁnd a basis for ker A we solve Ax = 0. We see from the echelon form that x5 is free, x4 is free, x3 = −7 2 x4 + 5 2 x5 x2 is free, x1 = −x2 + 1 2 x4 + 1 2x5. Thus the general solution of Ax = 0 is x = x2       −1 1 0 0 0  | | | |  + x4       1/2 0 −7/2 1 0  | | | |  + x5       1/2 0 5/2 0 1  | | | |  . We conclude that the vectors       −1 1 0 0 0  | | | |  ,       1/2 0 −7/2 1 0  | | | |  ,       1/2 0 5/2 0 1  | | | |  form a basis for Ker A. 102 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS Exercise. Let D : Pn → Pn be the derivative map given by Df (t) = f ′(t). What is the rank and nullity of D? Corollary 4.4.10. Let V and W be vector spaces of dimension n and let A : V → W be a linear map. Then the following properties of V are equivalent: (i) A is surjective. (ii) r(A) = n. (iii) n(A) = 0. (iv) A is injective. Proof. We will show that (i) ⇔ (ii) ⇔ (iii) ⇔ (iv). This will show that they are all equivalent. (i) ⇔ (ii). A is surjective precisely means that Im A = W . Since Im A is a subspace of W , this holds if and only if r(A) = dim Im A = dim W = n. (ii) ⇔ (iii). By the rank-nullity theorem, r(A) + n(A) = n. Hence r(A) = n if and only if n(A) = 0. (iii) ⇔ (iv). We have that n(A) = 0 and only if Ker A = {0}. By Proposition 4.4.6, this holds if and only if A is injective. Remark. Observe that if a linear map A satisﬁes the conditions of Corollary 4.4.10, and any (and hence all) of the conclusions (i)–(iv) in the Corollary hold, then A must be invertible. Proposition 4.4.11. For any matrix A we have that r(A) = r(A T ). Proof. Let E be an invertible matrix such that EA is in echelon form. Then (EA) T = A T ET . If v = A T x so that v ∈ Im AT , then v = A T ET (E−1)T x so v ∈ Im (EA)T . Hence applying row operations to A does not change row space. It follows that the pivot columns of EA form a basis for Im AT and so r(A T ) is the number of pivots in EA, which is also the rank of A. Remark 4.4.12. The above proof also gives an easy algorithm to compute a basis for Im A T from the echelon form of A. As a consequence of Proposition 4.4.11 we have that for A : V → W r(A) + n(AT ) = dim W. 4.5. CHANGE OF BASES. SIMILARITY 103 Exercise. Let A : F n → Fm be a linear map. Which, if any, of the following statements are true? (a) If A is injective then AT is injective. (b) If A is surjective then A T is surjective. (c) If A is injective then AT is surjective. (d) If A is surjective then A T is injective. 4.5 Change of bases. Similarity Let V be a vector space and let E = {e1, e2, . . . , en} be a basis of V . Recall that for v ∈ V , [v]E =      α1 α2 ... αn  | | |  ⇔ v = α1e1 + α2e2 + · · · + αnen. The map v 7→ [v]E is an invertible linear map between V and Fn. Let F = {f1, f2, . . . , fm} be a basis of a vector space W and let T : V → W be a linear map. Then [T v]F = [T ]E F [v]E, v ∈ V, (4.5.1) where [T ] E F is the matrix of T with respect to the bases E and F given by [T ] E F =   ↑ ↑ . . . ↑ [T (e1)]F [T (e2)]F . . . [T (en)]F ↓ ↓ . . . ↓   . Suppose V = W , so that E and F are both bases of V . Let I : V → V be the identity map given by Iv = v, v ∈ V . Then by (4.5.1) we have that for each v ∈ V , [v]F = [I] E F [v]E. Multiplying by [I]E F transforms coordinates in the E basis to coordinates in the F basis. 104 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS The matrix [I] E F is known as the transition matrix (or change of coordinate matrix) from E to F. Observe that we necessarily have [I]F E = ( [I] E F )−1 . Example 4.5.1. Let E and F be the bases of R 2 given by E = { e1 = ( 1 0 ) , e2 = (0 1 )} , F = {f1 = ( 1 1 ) , f2 = ( −1 1 )} . We can relate the two bases as follows: f1 = e1 + e2, f2 = −e1 + e2, and e1 = 1 2 f1 − 1 2f2, e2 = 1 2 f1 + 1 2f2. Therefore [I]E F = ( 1/2 1/2 −1/2 1/2 ) and [I]F E = (1 −1 1 1 ) . We can verify that [I]E F [I] F E = ( 1/2 1/2 −1/2 1/2 ) ( 1 −1 1 1 ) = ( 1 0 0 1 ) . Take, for example, u = (3, 1). Then we can determine the coordinates of u with respect to the basis F as follows: [u]F = [I]E F [u]E = ( 1/2 1/2 −1/2 1/2 ) ( 3 1 ) = ( 2 −1 ) . We can verify that the equality u = 2f1 − f2 holds, i.e that ( 3 1 ) = 2 (1 1 ) − ( −1 1 ) . holds. 4.5. CHANGE OF BASES. SIMILARITY 105 Exercise. Let E and F be bases of V . Suppose F ′ contains the same vectors as F but in a diﬀerent order. How are [I]E F ′ and [I]E F related? What about [I] F ′ E and [I]F E ? Let G be another basis of V . Then [I] E G = [I]F G [I] E F . This identity is useful because when determining the transition matrix from one basis to another, it is often easier to go through an intermediate basis in which it is quicker to do computations. Example 4.5.2. Let E = {1, 1 + t} and F = {1 − t, 2t + 1} be bases of P1. Determine the transition matrix from E to F. Solution. We will do this using two methods: Method 1. Let e1(t) = 1, e2(t) = 1 + t, f1(t) = 1 − t and f2(t) = 2t + 1 so that E = {e1, e2} and F = {f1, f2}. Let us write e1 = αf1 + βf2 so that 1 = α(1 − t) + β(2t + 1). Equating constants and coeﬃcients of t we get 1 = α + β, 0 = −α + 2β. We ﬁnd that β = 1/3 and α = 2/3, and so e1 = 2 3f1 + 1 3f2. (4.5.2) We repeat this for e2. Write e2 = αf1 + βf2 so that 1 + t = α(1 − t) + β(2t + 1). Equating constants and coeﬃcients of t we get 1 = α + β, 1 = −α + 2β. We ﬁnd that β = 2/3 and α = 1/3, and so e2 = 1 3f1 + 2 3f2. (4.5.3) 106 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS It follows from (4.5.2) and (4.5.3) that [I] E F = (2/3 1/3 1/3 2/3 ) . Method 2. Let M = {1, t} be the monomial basis of P1. We will ﬁrst go from E to M, then from M to F. It is immediate that [I]E M = ( 1 1 0 1 ) and [I] F M = ( 1 1 −1 2 ) . We can invert [I] F M to ﬁnd that [I]M F = ( [I] F M)−1 = (2/3 −1/3 1/3 1/3 ) . Finally, we compute that [I]E F = [I]M F [I] E M = ( 2/3 −1/3 1/3 1/3 ) ( 1 1 0 1 ) = ( 2/3 1/3 1/3 2/3 ) . Let E and F be bases of V and let T : V → V be a linear map. Let us consider how to relate the matrix [T ]E E to the matrix [T ]F F . We know that for each v ∈ V [T v]F = [T ]F F [v]F . However, we also have that [T v]F = [I]E F [T v]E = [I]E F [T ]E E[v]E = [I] E F [T ]E E[I]F E [v]F . Hence we have the identity [T ]F F = [I]E F [T ]E E[I]F E . Combining this with the fact that [I]E F = ( [I] F E )−1 we get the following: If P is the transition matrix from F to E then [T ] F F = P −1[T ] E EP. 4.5. CHANGE OF BASES. SIMILARITY 107 Example 4.5.3. Let T : R 2 → R 2 be the linear map given by T ( x y ) = 1 2 ( x + 3y 3x + y ) . Let E be the standard basis in R 2 and let F be the basis F = { f1 = (1 1 ) , f2 = ( −1 1 )} . Determine the matrix of T with respect to the basis F for both the domain and codomain. Solution. First Let us observe that [T ] E E = (1/2 3/2 3/2 1/2 ) . In Example 4.5.1 we showed that [I]E F = ( 1/2 1/2 −1/2 1/2 ) and [I]F E = (1 −1 1 1 ) . Then [T ]F F = [I]E F [T ] E E[I] F E = ( 1/2 1/2 −1/2 1/2 ) ( 1/2 3/2 3/2 1/2 ) ( 1 −1 1 1 ) = ( 2 0 0 −1 ) . From this we can see that T acts by ‘scaling’ by a factor of 2 in the f1 direction and reﬂecting in the line tf1. Example 4.5.4. Let T, E, F be as in Example 4.5.3 and let u = (3, 1). Then T u = [T u]E = [T ] E E[u]E = ( 1/2 3/2 3/2 1/2 ) ( 3 1 ) = ( 3 5 ) . We can also compute this using [T ]F F . In Example 4.5.1 we showed that [u]F = ( 2 −1 ) . Then [T u]F = [T ] F F [u]F = (2 0 0 −1 ) ( 2 −1 ) = ( 4 1 ) . 108 CHAPTER 4. SYSTEMS OF LINEAR EQUATIONS Hence T u = 4f1 − f2 = 4(e1 + e2) + (−e1 + e2) = 3e1 + 5e2, which as expected gives T u = (3 5 ) . Deﬁnition 4.5.5. Let A and B be square matrices. Then A is similar to B if there exists an invertible matrix P such that A = P −1BP. Observe that if A is similar to B then B is also similar to A. Indeed one easily checks that A = P −1BP if and only if B = P AP −1. Setting Q = P −1 we get that B = Q −1AQ. Remark 4.5.6. Similarity is an equivalence relation on the set of n × n matrices. We have seen that if two n × n matrices represent the same linear map with respect to (possibly) diﬀerent bases, then they are similar. Let V be a vector space with basis E and let T : V → V be a linear map. Suppose that B = [T ]E E and A = P −1BP with P =   ↑ ↑ . . . ↑ v1 v2 . . . vn ↓ ↓ . . . ↓   . For j = 1, 2, . . . , n let fj be the unique vector in V such that [fj]E = vj. Note that since P is invertible, {v1, v2, . . . , vn} is a basis of Fn. Therefore, since the map v 7→ [v]E is invertible and linear, Theorem 3.3.3 implies that F = {f1, f2, . . . , fn} is a basis of V . Then P =   ↑ ↑ . . . ↑ [f1]E [f2]E . . . [fn]E ↓ ↓ . . . ↓   = [I] F E . Hence A = P −1BP = [I] E F [T ] E E[I] F E = [T ] F F . We have shown the following: Two n × n matrices are similar if and only if they represent the same linear map with respect to (possibly) diﬀerent bases. Chapter 5 Determinants The determinant is a function that associates a single number to every square matrix. This number contains a surprising amount of information about the matrix. In particular, we will se that a square matrix is invertible if and only if its determinant is non-zero. The determinant of a square matrix A is usually denoted det(A) or |A|. Determinant of a 1 × 1 matrix Let A = ( a) be a 1 × 1 matrix. Then A is invertible if and only if a ̸= 0. We deﬁne det(A) := a. Determinant of a 2 × 2 matrix Let A = ( a b c d ) . If A is invertible then at least one of a or c must be non-zero (otherwise A would have a zero column). We suppose that a ̸= 0. Then we can row reduce A as follows: (a b c d ) −→ ( a b 0 d − cb a ) . 109 110 CHAPTER 5. DETERMINANTS This is invertible if and only if it has a pivot in each row. Hence A is invertible if and only if a ( d − cb a ) = ad − bc ̸= 0. We deﬁne det(A) := ad − bc. Example 5.0.1. det ( 3 1 −2 5 ) = 15 − (−2) = 17. 5.1 Geometric interpretation Let e1, e2 be the standard basis vectors in R 2 and let A be the 2 × 2 matrix A = ( a b c d ) , a, b, c, d ∈ R. Then the matrix A transforms the standard basis to the columns of A and so the square determined by e1 and e2, i.e the square with vertices (0, 0), (0, 1), (1, 0) and (1, 1), is transformed to the parallelogram determined by the columns of A, i.e the parallelogram with vertices (0, 0), (a, c), (b, d) and (a + b, c + d). Suppose that Ae1 and Ae2 have polar coordinates (r, θ) and (s, ϕ) respectively (see Figure 5.1) so that ( a c ) = ( r cos θ r sin θ ) and (b d ) = ( s cos ϕ s sin ϕ ) . Then the area of the parallelogram determined by the columns of A is |rs sin(ϕ − θ)| = |rs(sin ϕ cos θ − sin θ cos ϕ)| = |ad − bc| = | det(A)| Since any shape (or more precisely, any open set) in the plane is a disjoint1 union of squares, we conclude the following: 1Strictly speaking, the boundaries of the squares may intersect but these have zero area so we can ignore them. 5.1. GEOMETRIC INTERPRETATION 111 Ae1 = ( a c ) Ae2 = (b d ) r s θ ϕ Figure 5.1: | det(A)| is the area of the parallelogram determined by the columns of A. Applying the transformation A scales areas by a factor of | det(A)|. Observe that det(A) is negative when the acute angle measured from Ae1 to Ae2 is negative, i.e. when ϕ − θ is negative in Figure 5.1. Thus the sign of det(A) tells us whether or not the relative orientation of e1 and e2 has been preserved or reversed by A. The determinant of A is the signed area of the parallelogram determined by Ae1 and Ae2. Exercise. Let A be a real 2 × 2 matrix with positive determinant. Does it follow that the determinant of −A will be negative? Justify your answer. 112 CHAPTER 5. DETERMINANTS 5.2 Generalisation to n × n matrices Let A be a real n × n matrix with columns a1, a2, . . . , an, i.e. A =   ↑ ↑ . . . ↑ a1 a2 . . . an ↓ ↓ . . . ↓   . We want to construct the determinant of A, det(A) to be the signed n-dimensional volume of the parallelepiped determined by a1, a2, . . . , an. We will use the notation D(a1, a2, . . . , an) = det(A) when we want to emphasise the dependence on the columns. 5.2.1 Properties the determinant should have Linearity in each argument. If we multiply a column, say ak, by a positive constant β, then the volume D(a1, a2, . . . , an) should also be multiplied by β (see Figure 5.2 for the 2 × 2 case). Allowing negative volumes, this property should hold for all scalars β. Therefore, for all β ∈ R, the determinant should satisfy D(a1, . . . , βak, . . . , an) = βD(a1, . . . , ak, . . . , an) (5.2.1) a1 βa1 a2 Figure 5.2: The area of the parallelogram determined by a1 and βa2 is β times the area of the parallelogram determined by a1 and a2. Similarly, if we add two vectors, the length of the result in any particular direction is the sum of the lengths of each of the vectors in that direction and so the determinant should satisfy D(a1, . . . , ak + b | {z } k , . . . , an) = D(a1, . . . , ak|{z} k , . . . , an) + D(a1, . . . , b|{z} k , . . . , an). (5.2.2) 5.2. GENERALISATION TO N × N MATRICES 113 Combining (5.2.1) and (5.2.2) we see that if we ﬁx n − 1 columns, then D is a linear function of the remaining column. This can rephrased by saying that D is linear in each argument separately. Warning: The determinant is not a linear function from Mn(R) to R. Remark 5.2.1. A function of several variables that is linear in each variable separately is called multilinear. So we require that D is multilinear. Antisymmetry. If two of the ak are equal, the volume of the parallelepiped determined by a1, a2, . . . , an should be zero (the parallelepiped will be “ﬂat”). Hence if aj = ak with j ̸= k, we require that D(a1, a2, . . . , an) = 0. Suppose we swap two columns, a1 and a2 say. Then by the preceding observation and the multilinearity property, the resulting volume must satisfy D(a2, a1, a3 . . . , an) + D(a1, a2, a3, . . . , an) = D(a2, a1, a3 . . . , an) + D(a1, a1, a3, . . . , a ¯ n) + D(a1, a2, a3 . . . , an) + D(a2, a2, a3, . . . , an) = D(a2 + a1, a1, a3, . . . , an) + D(a2 + a1, a2, a3, . . . , an) = D(a2 + a1, a2 + a1, a3, . . . , an) = 0. Hence D(a2, a1, . . . , an) = −D(a1, a2, . . . , an). The same reasoning shows that this must hold for all pairs of columns. Therefore, when we interchange two columns, the determinant is multiplied by −1. Remark 5.2.2. A function of several variables that that is multiplied by −1 whenever two of the variables are interchanged is called antisymmetric (or alternating). So we require that D is antisymmetric. Normalization. Let e1, e2, . . . , en be the standard basis is R n. Then the parallelepiped determined by e1, e2, . . . , en is the unit (n-dimensional) cube in Rn and the volume of this should be 1, i.e D(e1, e2, . . . , en) = det(I) = 1. To summarise we have shown that the determinant must satisfy the three basic proper- ties: 114 CHAPTER 5. DETERMINANTS (1) Multilinearity: The determinant is linear as a function of each column separat- edly. (2) Antisymmetry: The determinant is multiplied by −1 whenever we interchange two columns. (3) Normalization: The derminant of the identity matrix is 1. Remark 5.2.3. Although the motivation for these properties comes from considering real matrices, we can also use them to deﬁne the determinant for complex matrices. Theorem 5.2.4. For each n ∈ N, there exists a unique function det : Mn(C) → C satisfying the basic properties above. In other words, the determinant is well-deﬁned and uniquely determined by the basic properties. We will not prove this yet. Exercise. Let A be a 3 × 3 matrix with det(A) = 2. What is det(3A)? We can deduce some additional properties from the basic ones. Proposition 5.2.5. For a square matrix A, the following statements hold: (i) If A has a zero column, the determinant of A is zero. (ii) If the columns of A are linearly dependent, the determinant of A is zero. (iii) Adding a multiple of one column to another leaves the determinant unchanged. Proof. The proofs of (i) and (ii) are left as an assignment question. Let us prove (iii). Suppose that A =   ↑ ↑ . . . ↑ a1 a2 . . . an ↓ ↓ . . . ↓   . Then adding β times column k to column 1 and using the multilinearity property we ﬁnd that the resulting determinant is D(a1 + βak, . . . , ak, . . . an) = D(a1, . . . , ak, . . . an) + βD(ak, . . . , ak, . . . an). 5.2. GENERALISATION TO N × N MATRICES 115 The second determinant on the right hand side has two equal arguments and so must be zero. Hence D(a1 + βak, . . . , ak, . . . an) = D(a1, . . . , ak, . . . an). Corollary 5.2.6. If A is not invertible, the determinant of A is zero. 5.2.2 Determinants of diagonal and triangular matrices Deﬁnition 5.2.7. A square matrix A = (ajk) is diagonal if ajk = 0 whenever j ̸= k, that is, if A is of the form A =      a1 0 . . . 0 0 a2 . . . 0 ... ... . . . ... 0 0 . . . an  | | |  . Remark 5.2.8. The entries ajk of A with j = k are called the main diagonal. So we say that a diagonal matrix is zero away from the main diagonal. If A is the diagonal matrix above, the multilinearity and normalisation properties imply that det(A) = det      a1 0 . . . 0 0 a2 . . . 0 ... ... . . . ... 0 0 . . . an  | | |  = a1a2 . . . an det(I) = a1a2 . . . an. The determinant of a diagonal matrix is the product of its diagonal entries. Deﬁnition 5.2.9. A square matrix A = (ajk) is called upper triangular if ajk = 0 whenever k < j and lower triangular if ajk = 0 whenever k > j. A matrix is triangular if it is either upper or lower triangular. Remark 5.2.10. Equivalently, a matrix is upper (resp. lower) triangular if its entries are zero below (resp. above) the main diagonal. 116 CHAPTER 5. DETERMINANTS For example,  a b c 0 d e 0 0 f   is upper triangular,  a 0 0 b c 0 d e f   is lower triangular. First observe that if A is a triangular matrix and one of its diagonal entries is zero then the columns are linearly dependent (since A does not have a pivot in each row). So in this case the determinant of A is zero. If each diagonal entry of A is non-zero, then by repeatedly adding a multiple of one column to another, A can be transformed into a diagonal matrix with the same diagonal entries as A. Since these properties leave the determinant unchanged we get the following: The determinant of a triangular matrix is the product of its diagonal entries. Exercise. Determine whether each of the following statements is true or false. (a) The inverse of an elementary matrix is elementary. (b) The transpose of an elementary matrix is elementary (c) Every product of elementary matrices is invertible. (d) Every invertible matrix is a product of elementary matrices. 5.2.3 Determinant of a transpose and product Consider the following elementary column operations: (I) Interchange column j with column k, where j ̸= k. (II) Multiply column j by a scalar α. (III) Add α times column j to column k. We know the eﬀect of each of these on the determinant of square matrix: operations of type (I) multiply the determinant by −1, type (II) multiply the determinant by α and type (III) leave the determinant unchanged (multiply by 1). Therefore, for each elementary column operation, there is a scalar β ∈ F such that applying the column operation to a square matrix, results in multiplying the determinant by β. Let E be an elementary matrix that corresponds to a particular row operation. Applying the same operation to columns of a matrix A (i.e. to the rows of A T ) produces the matrix (EA T )T = AET . 5.2. GENERALISATION TO N × N MATRICES 117 Observe that ET is also an elementary matrix of the same type as E. Hence every ele- mentary column operation can be realized as multiplication on the right by an elementary matrix. Lemma 5.2.11. Let E be an elementary matrix. Then the following statements hold: (i) det(E) = det(ET ). (ii) For every square matrix A, det(AE) = det(A) det(E). Proof. (i) Observe that ET corresponds to a column operation of the same type as E. Indeed if E corresponds to interchanging columns, then ET also corresponds to interchanging (possibly diﬀerent) columns; if E corresponds to multiplying a column by α, ET corresponds to multiplying a column by α; if E corresponds to adding α times one column to another, ET corresponds to adding α times one column to another. Hence multiplication on the right E and by ET will have the same eﬀect on the determinant of a square matrix. In particular, det(E) = det(IE) = det(IET ) = det(ET ). (ii) Since right multiplication by E is the same as applying an elementary column oper- ation, there is a non-zero scalar β ∈ F such that for every square matrix A, det(AE) = β det(A). It follows that det(E) = det(IE) = β det(I) = β, and so det(AE) = det(A) det(E). Theorem 5.2.12. Let A be a square matrix. Then det(A) = det(A T ). Proof. If A is not invertible, A T is not invertible and so det(A) = det(A T ) = 0. 118 CHAPTER 5. DETERMINANTS Let A be an invertible matrix. Then there exist elementary matrices E1, E2, . . . , Ek such that A = E1E2 . . . Ek. Then by Lemma 5.2.11, det(A) = det(E1E2 . . . Ek) = det(E1) det(E2) . . . det(Ek) = det(ET k ) . . . det(ET 2 ) det(ET 1 ) = det(ET k . . . ET 2 ET 1 ) = det(AT ). Corollary 5.2.13. The following properties hold for the determinant: (i) The determinant is linear in each row separately (i.e. it is a multilinear function of the rows). (ii) Elementary row operations have the same eﬀect on the determinant as the corre- sponding column operations. Theorem 5.2.14. Let A and B be square matrices. Then det(AB) = det(A) det(B). Proof. If either A or B is not invertible, the product AB will not be invertible and so det(AB) = det(A) det(B) = 0. Suppose that both A and B are invertible. Then there exist elementary matrices E1, E2, . . . , Ek such that B = E1E2 . . . Ek. Then by Lemma 5.2.11, det(AB) = det(AE1E2 . . . Ek) = det(A) det(E1) det(E2) . . . det(Ek) = det(A) det(E1E2 . . . Ek) = det(A) det(B). Warning: In general, det(A + B) ̸= det(A) + det(B). Corollary 5.2.15. Let A be a square matrix. Then A is invertible if and only if det(A) ̸= 0. 5.2. GENERALISATION TO N × N MATRICES 119 Proof. We know that if A is not invertible then the columns of A are linearly dependent, and so det(A) = 0. Conversely, if A is invertible, then A = E1E2 . . . Ek for some elementary matrices E1, E2, . . . , Ek. Since the determinant of an elementary matrix is non-zero, we have that det(A) = det(E1) det(E2) . . . det(Ek) ̸= 0. Determine which of the following statements are true for all 2 × 2 matrices A. Exercise. Determine which of the following statements are true for all invertible 2 × 2 matrices A. (a) det(A + A T ) = 2 det(A) (b) det(A − A T ) = 0. (c) det(A T A−1) = 1. What about for 3 × 3 matrices? 5.2.4 Formal deﬁnition. Cofactor formula Deﬁnition 5.2.16. Let A be an n × n matrix. For 1 ≤ i, j ≤ n, the minor Mij(A) is the determinant of the (n − 1) × (n − 1) matrix obtained by deleting the row i and column j from A. The cofactor Cij(A) is (−1) i+jMij(A). Example 5.2.17. Let A =  3 1 5 1 −1 0 2 1 2   . Then M12(A) = det ( 1 0 2 2 ) = 2, C12(A) = (−1)3 × 2 = −2, M33(A) = det ( 3 1 1 −1 ) = −4, C33(A) = (−1)6 × (−4) = −4. 120 CHAPTER 5. DETERMINANTS Let A = (aij) be an n × n matrix. We consider several cases in order to deﬁne the determinant for general matrices. Case 1: a11 is the only non-zero entry in the ﬁrst row. We perform column operations on columns 2 to n to put A into lower triangular form. Let E be an invertible matrix such that AE is in lower triangular form. Then det(A) = a11(AE)22(AE)33 . . . (AE)nn × 1 det(E). Observe that the product (AE)22(AE)33 . . . (AE)nn × 1 det(E), i.e. the term on the right without a11, is precisely the determinant M11(A). Hence det(A) = a11M11(A) = a11C11(A). Case 2: a1k is the only non-zero entry in the ﬁrst row, 1 ≤ k ≤ n. We interchange column k with column k − 1, then column k − 1 with column k − 2, and so on until column k is the left most column. This requires k − 1 column exchanges and so det(A) = (−1) k−1a1kM1k = a1kC1k(A). Case 3: General n × n matrix. We use linearity in the ﬁrst row. Let Ak be the matrix obtained from A by making all entries in the ﬁrst row except a1k zero, and leaving all other entries unchanged. Then since rows 2 to n are the same for A1, A2, . . . , An we have that det(A) = det(A1) + det(A2) + . . . det(An). Hence det(A) = n∑ j=1 a1jC1j(A) = a11C11(A) + a12C12(A) + · · · + a1nC1n(A). As a result of this we introduce the following deﬁnition of the determinant. 5.2. GENERALISATION TO N × N MATRICES 121 Deﬁnition 5.2.18. Let A = (aij) be an n × n matrix. Then the determinant of A, det(A), is deﬁned inductively as follows: ‹ if n = 1, det(A) = a; ‹ if n ≥ 2, det(A) = n∑ j=1 a1jC1j(A) = a11C11(A) + a12C12(A) + · · · + a1nC1n(A). Proposition 5.2.19. For each n ∈ N, the determinant det : Mn(C) → C satisﬁes the basic properties given in Section 5.2.1 (linearity in each column, antisymmetry and normalization). Proof. We prove each property in turn. Linearity in each column. We prove this by induction on n. It is clear that a 1 × 1 determinant is linear. Assume that the determinant of (n − 1) × (n − 1) matrices is linear in each argument. Let A = (aij) be an n × n matrix with columns a1, a2, . . . , an. Let B = (bij) be the n × n matrix obtained from A by replacing column k with b and let ̃A = (̃aij) be the n × n matrix obtained from A by replacing column k with αak + βb. We need to show that det( ̃A) = α det(A) + β det(B). We compute that det( ̃A) = n∑ j=1 ̃a1jC1j( ̃A) = ∑ j̸=k ̃a1jC1j( ̃A) + (αa1k + βb1k)C1k(A). Since (n − 1) × (n − 1) determinants are linear in each argument, we have that for j ̸= k, C1j( ̃A) = αC1j(A) + βC1j(B). Hence det( ̃A) = ∑ j̸=k ̃a1j(αC1j(A) + βC1j(B)) + (αa1k + βb1k)C1k(A) = α n∑ j=1 a1jC1j(A) + β ( ∑ j̸=k a1jC1j(B) + b1kC1k(A) ) . 122 CHAPTER 5. DETERMINANTS Observe that C1k(A) = C1k(B) and a1j = b1j for j ̸= k. Therefore det( ̃A) = α n∑ j=1 a1jC1j(A) + β n∑ j=1 b1jC1j(B) = α det(A) + β det(B). We have shown that n × n determinants are linear in each argument. It now follows by induction that this holds for all n ∈ N. Antisymmetry. We again prove this by induction on n. Suppose that the (n−1)×(n−1) determinant is antisymmetric. Let A = (aij) be an n × n matrix. Suppose we interchange two adjacent columns of A, column k and column k + 1 say, to produce a matrix ̃A = (̃aij). Observe that M1k(A) = M1(k+1)( ̃A), and so C1k(A) = −C1(k+1)( ̃A). Similarly, C1(k+1)(A) = −C1k( ̃A). For each j ̸= k, k + 1, M1j( ̃A) and M1j(A) are determinants of (n − 1) × (n − 1) matrices that diﬀer by one column exchange. Hence M1j( ̃A) = −M1j(A) by assumption. We conclude that det( ̃A) = − n∑ j=1 a1jC1j(A) = − det(A). Now suppose that 1 ≤ j < k ≤ n and let A have columns a1, a2, . . . , an. Then exchanging column j and column k can be carried out as follows: exchange column j with column j + 1, then column j + 1 with column j + 2, and so on until aj is in column k. At this point, ak will be in column k − 1 and so we exchange column k − 1 with column k − 2 and so on until ak is in column j. One easily checks that this requires 2(k − j) − 1 exchanges of adjacent columns. Hence the determinant is multiplied by (−1) 2(k−j)−1 = −1. Since a 1 × 1 determinant is trivially antisymmetric, it follows by induction that n × n determinants are antisymmetric for all n ∈ N. Normalization. Let In denote the n × n identity matrix. One easily checks that det(In) = det(In−1). Since det(I1) = 1, it follows that det(In) = 1 for all n ∈ N. Remark 5.2.20. Proposition 5.2.19 combined with the discussion preceding it provide a proof of Theorem 5.2.4. By exchanging rows, we see that we can expand in any row (rather than just the ﬁrst 5.3. COFACTOR FORMULA FOR A−1 123 row). Moreover, since det(A) = det(A T ) we can expand in any column. det(A) = n∑ j=1 a1jC1j(A) (5.2.3) = n∑ j=1 akjCkj(A) (expand in row k) (5.2.4) = n∑ j=1 ajkCjk(A) (expand in column k) (5.2.5) (5.2.6) Example 5.2.21. Let A =     5 1 2 0 3 0 0 0 1 6 0 −1 −6 3 1 0  | |  . Then expanding in the second row we get that det(A) = −3 det  1 2 0 6 0 −1 3 1 0   . We compute that det   1 2 0 6 0 −1 3 1 0   = det ( 0 −1 1 0 ) − 2 det ( 6 −1 3 0 ) = 1 − 6 = −5. Therefore det(A) = 15. 5.3 Cofactor formula for A−1 For a 2 × 2 matrix A = ( a b c d ) one easily checks that if det(A) = ad − bc ̸= 0 then A−1 = 1 ad − bc ( d −b −c a ) . 124 CHAPTER 5. DETERMINANTS There is a similar formula for n × n matrices. Proposition 5.3.1. Let A be an n × n matrix with det(A) ̸= 0. Let C be the n × n matrix whose i, j-entry is the cofactor Cij(A), i.e C =      C11(A) C12(A) . . . C1n(A) C21(A) C22(A) . . . C2n(A) ... ... . . . ... Cn1(A) Cn2(A) . . . Cnn(A)  | | |  . Then A −1 = 1 det(A)C T . Proof. Suppose A = (aij). We will show that AC T det(A)I. Then since A is square, it will follow that A is invertible and A −1 = 1 det(A)C T . The i, j-entry of AC T is the ‘dot product’ of row i of A and column j of C T (the latter is just row j of C), i.e. (AC T )ij = n∑ j=1 aikCjk(A). If i = j, this is just the cofactor expansion of A along row j of A, so this equals det(A). If i ̸= j, this is the cofactor expansion along row j of the matrix obtained from A by replacing row j with row i (and leaving all other rows unchanged). Since this matrix has two equal rows, it will have determinant equal to zero. Therefore AC T = det(A)I. Chapter 6 Introduction to eigenvalues and canonical forms This chapter is intended to serve as a brief introduction to some of the concepts that will be covered in more detail in Linear Algebra and Geometry II. The techniques introduced here will also be useful in other courses next semester; in particular, knowledge of the content in this chapter will be assumed in 4CCM131A Introduction to Dynamical Systems. 6.1 Eigenvalues and eigenvectors Deﬁnition 6.1.1. Let A : Fn → Fn be a linear map. Then λ ∈ C is an eigenvalue of A if there exists a non-zero vector x ∈ Fn such that Ax = λx. (6.1.1) A non-zero vector x is an eigenvector of A corresponding to the eigenvalue λ if it satisﬁes (6.1.1). Observe that (6.1.1) is satisﬁed if and only if (A − λI)x = 0, so that x ∈ Ker (A − λI). Thus λ is an eigenvalue of A if and only Ker (A − λI) ̸= {0} and the eigenvalues corresponding to λ are the non-zero elements of Ker (A − λI). Recall that if the kernel of an n × n matrix contains a non-zero vector, it is not invertible and so its determinant is zero. This gives us the following characterisation of eigenvalues: 125 126CHAPTER 6. INTRODUCTION TO EIGENVALUES AND CANONICAL FORMS λ ∈ C is an eigenvalue of A if and only if det(A − λI) = 0. When A is an n × n matrix, the function p(λ) = det(A − λI) is a polynomial in λ of degree n. By the Fundamental Theorem of Algebra, it has precisely n complex roots, and so A has precisely n (possibly repeated) eigenvalues. Remark 6.1.2. The polynomial p(λ) = det(A − λI) is called the characteristic polyno- mial of A. Example 6.1.3. Determine all eigenvalues and eigenvectors of the following matrices: (a) A = ( 1 −10 −5 −4 ) (b) B =  3 2 2 0 1 −1 0 0 2   Solution. (a) We ﬁrst ﬁnd the eigenvalues by solving the equation det(A = λI) = 0. We have that A − λI = ( 1 −10 −5 −4 ) − (λ 0 0 λ ) = ( 1 − λ −10 −5 −4 − λ ) , and so det(A − λI) = (1 − λ)(−4 − λ) − 50 = λ 2 + 3λ − 54 = (λ − 6)(λ + 9). Thus the eigenvalues of A are λ1 = 6 and λ2 = −9. To ﬁnd the eigenvectors of A, we have to solve the system (A − λI)x = 0 for λ = 6, −9. Solving the system (A − 6I)x = ( −5 −10 −5 −10 ) ( x1 x2 ) = ( 0 0 ) we get x1 = −2x2. It follows that every eigenvector of A with eigenvalue 6 is a non-zero multiple of ( −2 1 ) . 6.1. EIGENVALUES AND EIGENVECTORS 127 Similarly, solving the system (A − 9I)x = ( 10 −10 −5 5 ) ( x1 x2 ) = ( 0 0 ) we get x1 = x2. It follows that every eigenvector of A with eigenvalue −9 is a non-zero multiple of ( 1 1 ) . (b) As before, we ﬁrst ﬁnd the eigenvalues. B − λI =  3 − λ 2 2 0 1 − λ −1 0 0 2 − λ   , so det(B − λI) = (3 − λ)(1 − λ)(2 − λ). Thus the eigenvalues of B are λ1 = 1, λ2 = 2 and λ3 = 3. We now ﬁnd the eigenvectors corresponding to each of the eigenvalues in turn. λ = 1: Solving the equation (B − I)x =  2 2 2 0 0 −1 0 0 1   =  0 0 0   we get x3 = 0 and x1 = −x2, and so every eigenvector of B with eigenvalue 1 is a non-zero multiple of   1 −1 0   . λ = 2: Solving the equation (B − I)x =  1 2 2 0 −1 −1 0 0 0   =  0 0 0   we get x1 = 0 and x2 = −x3, and so every eigenvector of B with eigenvalue 2 is a non-zero multiple of   0 1 −1   . 128CHAPTER 6. INTRODUCTION TO EIGENVALUES AND CANONICAL FORMS λ = 3: Solving the equation (B − I)x =  0 2 2 0 −2 −1 0 0 −1   =  0 0 0   we get x2 = x3 = 0, and so every eigenvector of B with eigenvalue 3 is a non-zero multiple of   1 0 0   . Exercise. Let A and B be n × n matrices and let λ be an eigenvalue of A and µ be an eigenvalue fo B. Which of the following statements must be true? (a) 2λ is an eigenvalue of 2A. (b) λ2 is an eigenvalue of A 2. (c) λ + µ is an eigenvalue of A + B. (d) λµ is an eigenvalue of AB. 6.2 Canonical forms of 2 × 2 matrices Let A and B be n × n matrices. Recall that A is similar to B if there exists an invertible matrix P such that A = P −1BP. We say that a matrix is diagonalizable if it is similar to a diagonal matrix. More precisely, we say that it is diagonalizable over F if it is similar to a diagonal matrix with entries in F. Note that it is possible that a real matrix is diagonalizable over C but not over R. Theorem 6.2.1. Let A be a 2 × 2 matrix. Then exactly one of the following hold: (i) A is diagonalisable over C so that there exists an invertible matrix P and λ1, λ2 ∈ C such that P −1AP = ( λ1 0 0 λ2 ) . 6.2. CANONICAL FORMS OF 2 × 2 MATRICES 129 (ii) A is not diagonalisable over C but there exists an invertible matrix P and λ0 ∈ C such that P −1AP = ( λ0 1 0 λ0 ) . To determine which class a particular matrix A falls into, we need to determine its eigenvalues. Let A be a 2 × 2 matrix. Since the function det(A − λ) is a quadratic polynomial in λ we have one of two possibilities for its roots: (1) det(A − λI) has two distinct roots λ1, λ2 ∈ C (A has two distinct eigenvalues). In this case we have that P −1AP = ( λ1 0 0 λ2 ) for some invertible matrix P . Moreover, we can construct P by taking its columns to be the eigenvectors of A, i.e the columns of P will be non-zero solutions of the homogeneous linear equations (A − λ1I)x = 0, (6.2.1) (A − λ2I)x = 0. (6.2.2) In particular, we take the ﬁrst column of P to be any solution of (6.2.1) and the second column of P to be any solution of (6.2.2). (2) det(A − λI) has a repeated root λ0 ∈ C (A has a repeated eigenvalue). In this case we have to we have to compute the eigenvectors of A, i.e we have to determine Ker (A − λ0I). Here we again have two separate cases to consider: (a) If Ker (A − λ0I) is two dimensional then P −1AP = (λ0 0 0 λ0 ) , where we can take the columns of P to be any basis of Ker (A−λ0I). (Observe that this is case (i) above with λ1 = λ2 = λ0.) (b) If Ker (A − λ0I) is one dimensional then P −1AP = (λ0 1 0 λ0 ) , 130CHAPTER 6. INTRODUCTION TO EIGENVALUES AND CANONICAL FORMS for some invertible matrix P . Let x be an eigenvector of A (all others are multiples of x) and let y be any vector which is linearly independent to x. Then there is a non-zero scalar α ∈ C such that Av = λ0x + αy. We can take the ﬁrst column of P to be x and the second column to be α−1y. If A is a real matrix then we can also say that following. Theorem 6.2.2. Let A be a real 2 × 2 matrix. Then exactly one of the following hold: (i) A is diagonalisable over R so that there exists an invertible matrix P and λ1, λ2 ∈ R such that P −1AP = ( λ1 0 0 λ2 ) . (ii) A is not diagonalisable over R but there exists an invertible matrix P and λ ∈ R such that P −1AP = ( λ 1 0 λ ) . (iii) A is not diagonalizable over R but there exists an invertible matrix P and α, β ∈ R, β ̸= 0, such that P −1AP = ( α β −β α ) . As before, we determine which of these cases applies to a particular matrix A by consid- ering its eigenvalues, i.e. the roots of the quadratic function det(A − λI). Since we are considering real roots we now have three possibilities: (1) det(A − λI) has two real roots λ1, λ2 ∈ R. In this case the previous discussion applies and we see that A is similar to (λ1 0 0 λ2 ) . (2) det(A−λI) has one real root λ0 ∈ R. As before we have that if dim Ker (A−λI) = 2 then A is similar to (λ0 0 0 λ0 ) , otherwise A is similar to (λ0 1 0 λ0 ) . 6.2. CANONICAL FORMS OF 2 × 2 MATRICES 131 (3) det(A − λI) has two complex roots α ± iβ, β ̸= 0. In this case A i similar to ( α β −β α ) . Observe that A is also similar to ( α + iβ 0 0 α − iβ ) . Exercise. Let A be an n × n matrix. Suppose A is diagonalizable. Is A T necessarily diagonalizable? Example 6.2.3. Consider the matrix A = ( 7 −5 10 −8 ) . To determine the canonical form of A (given by Theorem 6.2.2), we need to ﬁnd the eigenvalues of A, i.e. we have to ﬁnd the solutions λ ∈ C of det(A − λI) = 0. det(A − λI) = det ( 7 − λ −5 10 −8 − λ ) = (7 − λ)(−8 − λ) + 50 = λ 2 + λ − 6 = (λ + 3)(λ − 2). So det(A − λI) = 0 if and only if λ = −3 or λ = 2. Hence there exists an invertible matrix P such that P −1AP = ( 2 0 0 −3 ) . Next, we solve (A − λI)x = 0 for λ = 2, −3. If λ = 2, (A − λI)x = ( 5 −5 10 −10 ) ( x1 x2 ) = (0 0 ) , so x1 = x2. Hence all of the solutions (eigenvectors with eigenvalue 2) are of the form t ( 1 1 ) , t ∈ R. 132CHAPTER 6. INTRODUCTION TO EIGENVALUES AND CANONICAL FORMS If λ = −3, (A − λI)x = ( 10 −5 10 −5 ) ( x1 x2 ) = ( 0 0 ) . so 10x1 = 5x2 which gives 2x1 = x2. Hence all of the solutions (eigenvectors with eigenvalue −3) are of the form t ( 1 2 ) , t ∈ R. Therefore we can take P = (1 1 1 2 ) . Then P −1 = ( 2 −1 −1 1 ) , and we check that P −1AP = ( 2 −1 −1 1 ) ( 7 −5 10 −8 ) ( 1 1 1 2 ) = ( 2 0 0 −3 ) . Example 6.2.4. Let B = ( 3 −1 1 2 ) . Then det(B − λI) = det ( 3 − λ −1 1 2 − λ ) = (3 − λ)(2 − λ) + 1 = λ2 − 5λ + 7 = (λ − 5/2) 2 + 3/4. So det(B − λI) = 0 if and only if λ = 5/2 ± i √3/2. Hence there is an invertible matrix P such that P −1BP = ( 5/2 + i √3/2 0 0 5/2 − i√3/2 ) . There is also a real invertible matrix Q such that Q −1BQ = ( 5/2 √3/2 −√ 3/2 5/2 ) . 6.3. APPLICATION TO SYSTEMS OF ODES 133 Example 6.2.5. Let C = ( 4 9 −1 −2 ) . Then det(C − λI) = det ( 4 − λ 9 −1 −2 − λ ) = (4 − λ)(−2 − λ) + 9 = λ 2 − 2λ + 1 = (λ − 1)2. So the only eigenvalue of C is λ = 1. Let us solve (C − λI)x = 0 for λ = 1: (C − I)x = ( 3 9 −1 −3 ) ( x1 x2 ) = (0 0 ) , so x1 = −3x2. Hence each eigenvector is a multiple of (−3 1 ) . Since there is only one linearly independent eigenvector, there exists an invertible matrix P such that P −1CP = ( 1 1 0 1 ) . Exercise. Let λ1 and λ2 be distinct real numbers and let A be the matrix ( λ1 1 0 λ2 ) . What is the canonical form of A given by Theorem 6.2.2? 6.3 Application to systems of ODEs For simplicity, we will only consider ﬁrst order homogeneous constant coeﬃcient equa- tions. 134CHAPTER 6. INTRODUCTION TO EIGENVALUES AND CANONICAL FORMS One equation. For λ ∈ F, consider the ODE du dt = λu. This has solutions u(t) = Ce λt, C ∈ F. Note that C = u(0). n equations. Now we have n unknown functions u1(t), . . . , un(t) and a system of ODEs: du1 dt = a11u1 + a12u2 + · · · + a1nun du2 dt = a21u1 + a22u2 + · · · + a2nun ... ... dun dt = am1u1 + am2u2 + · · · + amnun. In general, these equations are coupled, i.e each equation cannot be solved on its own. As with ordinary linear equations, we can write this as a single vector ODE: du dt = Au, (6.3.1) where A =      a11 a12 . . . a1n a21 a22 . . . a2n ... ... . . . ... am1 am2 . . . amn  | | |  , u(t) =      u1(t) u2(t) ... un(t)  | | |  , du dt =      du1/dt du2/dt ... dun/dt  | | |  . The equation (6.3.1) is linear and homogeneous, i.e if u(t) and v(t) are solutions, so is αu(t) + βv(t) for every α, β ∈ F. Suppose λ is an eigenvalue of A with eigenvector x, so that Ax = λx. Then if u(t) = e λtx =      x1eλt x2eλt ... x2eλt  | | |  , 6.3. APPLICATION TO SYSTEMS OF ODES 135 we have that du dt = λe λtx and Au = A(e λtx) = e λtAx = λe λtx. So u(t) = eλtx is a solution. Hence if λ1, λ2, . . . , λn are eigenvalues of A with respective eigenvectors x1, x2, . . . , xn, then u(t) = C1eλ1tx1 + C2e λ2tx2 + · · · + Cne λntxn is a solution. In fact, we have the following: If A is diagonalizable and x1, . . . , xn are linearly independent eigenvectors corre- sponding to λ1, . . . , λn respectively, then the general solution of (6.3.1) is u(t) = C1e λ1tx1 + C2eλ2tx2 + · · · + Cneλntxn, C1, . . . , Cn ∈ F. Remark 6.3.1. If A is not diagonalizable, there are other solutions. In particular, if λ is an eigenvalue which is repeated k times but has only one linearly independent eigenvector x, then e λtx, te λtx, . . . , tk−1eλtx will be linearly independent solutions. Example 6.3.2. Find the general solution of the following system of ODEs: du1 dt = 7u1 − 5u2, du2 dt = 10u1 − 8u2. Solution. This can be written in vector form as du dt = ( 7 −5 10 −8 ) u. We know from Example 6.2.3 that A is diagonalizable and that it has eigenvalues λ1 = 2, λ2 = −3 with corresponding eigenvectors x1 = (1 1 ) , x2 = ( 1 2 ) . 136CHAPTER 6. INTRODUCTION TO EIGENVALUES AND CANONICAL FORMS Therefore the general solution is u(t) = C1e 2t (1 1 ) + C2e−3t ( 1 2 ) , C1, C2 ∈ F. or in scalar form, u1(t) = C1e 2t + C2e −3t, u2(t) = C1e 2t + 2C2e −3t. Example 6.3.3. Find the particular solution of the system of ODEs du dt =  3 2 2 0 1 −1 0 0 2   u satisfying u(0) = (1, 1, 1). (You may use the fact that A is diagonalisable.) Solution. We know from Example 6.1.3(b) that the matrix  3 2 2 0 1 −1 0 0 2   has eigenvalues 1, 2, 3 with respective eigenvectors x1 =   1 −1 0   , x2 =   0 1 −1   , x3 =  1 0 0   . Therefore the general solution of the ODE is u(t) = C1et   1 −1 0   + C2e 2t   0 1 −1   + C3e3t   1 0 0   . If u(0) = (1, 1, 1) we must have C1   1 −1 0   + C2   0 1 −1   + C3   1 0 0   =  1 1 1   . 6.3. APPLICATION TO SYSTEMS OF ODES 137 This gives C1 = −2, C2 = 0 and C3 = 3. Thus the solution we seek is u(t) = −2e t   1 −1 0   + 3e 3t  1 0 0   .","libVersion":"0.2.2","langs":""}