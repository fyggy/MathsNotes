{"path":".obsidian/plugins/text-extractor/cache/d49474d4a504b88d02df52189172228f.json","text":"Discrete Mathematics 251 Semester 2021/2022 King’s College London Edited by Lassina Dembélé and Aled Walker lassina.dembele@kcl.ac.uk and aled.walker@kcl.ac.uk Version: April 27, 2022 1 CONTENTS 2 Contents 1 Arithmetic 5 1.1 Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Divisibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.3 Modular arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.4 Binary expansions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2 Recurrence relations 11 2.1 Recursive functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2 Fibonacci numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.3 Constant coeﬃcients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.4 Particular solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.5 Derangements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.6 Other counting applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3 Arithmetical algorithms 22 3.1 First concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3.2 Powers by squaring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.3 Euclid’s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.4 Consolidation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4 Basic graph theory 30 4.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4.2 Connectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.3 Eulerian graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4.4 Hamiltonian cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 5 Vertex colouring 42 5.1 Chromatic number . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5.2 Colouring results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 5.3 Brooks’ algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 6 Planarity 47 6.1 The Platonic graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.2 Detecting non-planarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 6.3 Further results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 CONTENTS 3 6.4 The four-colour theorem* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 7 Navigation in graphs 55 7.1 Adjacency data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 7.2 Search trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 7.3 Shortest paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 8 Optimality 68 8.1 Shortest paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 8.2 Kruskal’s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 8.3 Back to matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 8.4 The graph Laplacian* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 9 Networks and ﬂows 75 9.1 Network ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 9.2 The max ﬂow, min cut theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 9.3 Labelling Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 9.4 Menger’s theorem* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 9.5 Dynamic programming* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 10 Cryptography 103 10.1 Euler’s totient function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 10.2 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 10.3 Basic setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 10.4 Vigenère cipher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 10.5 The RSA algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 10.6 Security of RSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 10.7 The eﬃciency of RSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 10.8 Miller’s test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 11 Codes* 113 11.1 Check digits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 11.2 Binary codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 11.3 Binary linear codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 11.4 Correcting one error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 CONTENTS 4 Preface These are the 2021/22 lecture notes for the module 5ccm251a. This document is an edited and moderately extended version of the notes written by Simon Salamon, the previous lecturer of this course, to whom we are greatly indebted. It is likely that these notes will be updated week by week as the module is taught in the Winter/Spring 2022. For this reason, it is not recommended that they be printed. We take this opportunity to also thank Naz Mihesi for his ongoing support in running the course. The module was successfully taught for many years by Simon Fairthorne. The current material is based on the module as he designed it. These lecture notes are dedicated to his memory. Lassina Dembélé, Aled Walker 13 January 2022 1 ARITHMETIC 5 1 Arithmetic Notation. We will use the sets N = {0, 1, 2, 3, . . .}, Z = {0, 1, −1, 2, −2, . . .}. There are competing conventions here (most number-theory sources start the natural numbers N at 1 rather than at 0). The above will be our conventions in this course. 1.1 Induction First principle. Let P(n) be some statement that makes sense for all n E n0 . (Typically, n0 = 0, 1 or 2.) Suppose that (1) P(n0) is true, and (2) for all n E n0, P(n) is true ⇒ P(n + 1) is true. Then P(n) is true for all n. Example 1.1. P(n) is the assertion that 1 + 3 + 5 + \u0005 + (2n − 1) = n2. Take n0 = 1. (1) P(1) is true because both sides equal 1. (2) Now suppose that P(n) is true, and add 2n + 1 to both sides above to give 1 + 3 + 5 + \u0005 + (2n − 1) + (2n + 1) = n2 + (2n + 1). The right-hand side simpliﬁes to (n + 1)2, so this is assertion P(n + 1). Therefore P(n) must be true for all n E 1. Note. Curly P emphasizes that P is a statement, not an arithmetical function. Second principle. Same start as above. Suppose that (1) P(n0) is true, and (2’) for all n E n0, P(k) is true for all n0 D k < n ⇒ P(n) is true. Then P(n) is true for all n. Example 1.2. Take n0 = 2. P(n) is the assertion “ n can be written as a product of (one or more) prime numbers”. (1) 2 is a prime number, so obviously P(2) is true. (2’) (i) If n is prime, then P(n) is already true. (ii) If not, then n has a divisor other than 1 and n, so we can write n = ab with 1 < a < n and 1 < b < n. If P(k) is true for all k < n then P(a) and P(b) are both true, which means that a is a prime or a product of primes, and b similarly. The same must be true of ab, and P(n) is true. Therefore any integer n E 2 is a product of primes. 1 ARITHMETIC 6 Summary. Use the ﬁrst principle when P(n + 1) appears to depend only on P(n). The second is needed when P(n) or P(n + 1) depends on more than one predecessor. But sometimes it becomes necessary to check more than one initial value. Example 1.3. Prove that an = 2n + (−3)n is a solution of œ an = 6an−2 − an−1, n E 2 a0 = 2, a1 = −1. We use the second principle with P(n) the assertion “ an = 2 n + (−3)n ” for n E 0. Then P(0) is true, since 2 0 + (−3)0 = 2. Now assume that P(k) is true for all k D n. Then an = 6an−2 = an−1 = 6[2n−2 + (−3)n−2] − [2n−1 + (−3)n−1] = 6[2n−2 + (−3)n−2] − [2 ∗ 2 n−2 − 3 ∗ (−3)n−2] = 4 ∗ 2n−2 + 9n−2 = 2n + (−3)n, provided n E 2 (for the second line). The punch line is that we need to check P(1) separately, which is easily done: 21 + (−3)1 = 2 − 3 = −1. Thus, P(n) is true for all n. Notation. To avoid confusion, we shall often indicate multiplication between actual numbers by ∗ as in common software. 1.2 Divisibility Notation. Let m, n ∈ Z. One says that m divides n, abbreviated to m S n if there exists an integer q such that mq = n. For example, 13 S 0, but 0 Ñ 13. Here is a formal Deﬁnition 1.4. A positive integer p E 2 is a prime number if a ∈ N, a S p ⇒ a = 1 or a = p. Let a be any integer, and b a positive integer. There exist integers q, r such that a = qb + r, 0 D r < b. We shall take the validity of this statement for granted. It is sometimes called the Division Algorithm, and one can imagine a mechanical way of ﬁnding the quotient q and the remainder r . Note that b S a if and only if r = 0. Example 1.5. 23 = 4 ∗ 5 + 3 −17 = (−4) ∗ 5 + 3 510510 = 510 ∗ 1001 + 0 104729 = 104 ∗ 999 + 833. 1 ARITHMETIC 7 One uses notation like a = r mod b, or a ≡ r (b), or a mixture. We shall adopt the former, so for example 104729 = 833 mod 999, also 104729 = 1 mod 104. Deﬁnition 1.6. Let a, b be integers. Then, the greatest common divisor of a and b, denoted by gcd(a, b), is the largest positive integer that divides both a and b. It is undeﬁned when a = b = 0. It is the same as hcf(a, b), the highest common factor of a and b, and is sometimes abbreviated to (a, b). If gcd(a, b) = 1 then a and b are called coprime. Example 1.7. gcd(24, 15) = 3 gcd(6, 0) = 6 gcd(−12, −24) = 12 gcd(510510, 44) = 22 gcd(104729, 10 9) = 1. Proposition 1.8. There exist integers x, y such that gcd(a, b) = xa + by . Later, we shall recall Euclid’s algorithm that determines x and y . The proposition has the following consequences: Corollary 1.9. If m is any divisor of a and b and n = gcd(a, b) then m divides n. Proof. This follows immediately from the formula n = xa + yb, since m must divide the right-hand side. Corollary 1.10. Let p be a prime number. Then p S mn Ô⇒ p S m or p S n. Proof. Suppose that p Ñ m. Then (p, m) = 1, since the only divisors of p are 1 and p, but the latter does not divide m. So we can write 1 = xp + ym. Thus n = xpn + ymn, and (since p divides both terms on the right-hand side) p S n. 1.3 Modular arithmetic Deﬁnition 1.11. We say that a1 and a2 are congruent (or equal) modulo n if n divides a1 − a2 . In symbols, a1 = a2 mod n ⇔ n S (a1 − a2). We’ll sometimes write a1 ≡ a2 if n has been ﬁxed in advance. 1 ARITHMETIC 8 Because of the division algorithm (with b = n) we know that any integer is equal modulo n to some remainder r in R = {0, 1, 2, . . . , n − 1}. We can deﬁne addition and multiplication on this set by taking remainders modulo n, like on a clockface. Example 1.12. With n = 7 3 + 5 = 1 mod 7 3 ∗ 5 = 1 mod 7 6 ∗ 6 = 1 mod 7 6 = −1 mod 7 When we are working modulo n, an element r ∈ R really represents all integers obtained from r by adding or subtracting multiples of n, i.e. it represents the set {r + kn ∶ k ∈ Z} = r + nZ. In the laguange of abstract algebra, Z is a ring, nZ is an ideal, and R = Z~nZ is the quotient ring each of whose elements is a coset r + nZ. Since R is a ring, almost all the usual laws of arithmetic apply: if a = b mod n then a + c = b + c, ac = bc, a2 = b 2, . . . mod n. Beware though that one can have divisors of zero: the statement ab = 0 Ô⇒ a = 0 or b = 0 mod n is false in general. For example, 2 ∗ 3 = 0 mod 6. But it is true if n is a prime number: Proposition 1.13. Suppose that n = p is a prime number, and that p does not divide a. Then a has an inverse modulo p. Proof. By assumption, gcd(a, p) = 1 since the only factors of p are 1 and p, and p Ñ a. By §1.2, we know that xa + yp = 1 for some x, y ∈ Z. It follows that xa = 1 mod n, and we can suppose that 0 < x < n. Example 1.14. To perform a sequence of operations, take remainders at each stage. Compute 15 8 mod 16. Note that 15 = −1 mod 16, so 15 8 = (−1)8 = 1 mod 16. Example 1.15. Solve 2x = 2 mod 16. This means 2x = 2 + 16k, so x = 1 + 8k . There are two solutions modulo 16, namely 1 and 9 ≡ −7. Let p E 2 be a prime number. Then R∗ = R \u0013 {0} = {1, 2, . . . p − 1} is a group under multiplication modulo p, and R itself is a ﬁeld (a ring in which multiplication is commutative and has inverses). Let a be an integer that is not a muliple of p. Its remainder modulo p is an element of R∗, whose order (by Cauchy’s theorem) divides p − 1. This implies 1 ARITHMETIC 9 Theorem 1.16 (Fermat’s little theorem). If p is prime and p Ñ a, then a p−1 ≡ 1 mod p. We can include the possibility that p S a by simply multiplying both sides by a: ap ≡ a mod p, ∀a ∈ Z. Example 1.17. Taking p = 11 and a = 2 gives 2 10 = 1 mod 11, which is easy to check immediately as 210 = 1024. Note that 88 ≡ 1 mod 9, because 88 ≡ (−1)8 mod 9, so taking a = 8 and p = 9 satisﬁes Fermat’s little equation, even though p is not prime. Even better: Let n = 561, which is certainly not prime. Then it is known that a561 = a mod n, for all a ∈ Z, which makes 561 a Carmichael number (it is the ﬁrst). Proposition 1.18. If p is prime, the only solutions of x2 = 1 mod p are x ≡ 1 and x ≡ −1. Proof. x2 = 1 mod p means p S (x2 − 1), so p S (x − 1)(x + 1). By an earlier corollary, p must divide at least one of these factors. If p S (x − 1) then x ≡ 1, whereas p S (x + 1) implies x ≡ −1. For example, modulo 7, we know that a6 ≡ 1. A solution of x2 = a6 is x = a 3 and we observe that 1 3 ≡ 1, 23 ≡ 1, 33 ≡ −1, 4 3 ≡ 1, 53 ≡ −1, 6 3 ≡ −1. 1.4 Binary expansions To ﬁnd the decimal expansion of an integer, we repeatedly divide by 10, and read the remainders from bottom to top. For example, 327 = 32 ∗ 10 + 7 32 = 3 ∗ 10 + 2 3 = 0 ∗ 10 + 3 1 ARITHMETIC 10 The same process works in base 2 (binary) 39 = 19 ∗ 2 + 1 19 = 9 ∗ 2 + 1 9 = 4 ∗ 2 + 1 4 = 2 ∗ 2 + 0 2 = 1 ∗ 2 + 0 1 = 0 ∗ 2 + 1 . Therefore 39 = 1001112, which is correct since 39 = 25 + 7 = 1000002 + 1112 . On a computer, 6 bits are needed to represent 39. Recall the concept of logarithm to base b. It is the inverse to exponentiation: if y = b x then x = logb y . We write ln y = loge y, lg y = log2 y, where e = lim n→∞ ‰1 + 1 n ’ n = ∞ Q n=0 1 n! = 2.7182818 . . . It is easy to show that lg y = log2 y = ln y ln 2 ≈ 1.44 ln y. In this course, we shall only use logarithms to base 2. Here are the key properties: • lg(2 x) = x • 2lg y = y • lg is strictly increasing: a < b ⇒ lg a < lg b. Suppose that n is trapped between two powers of 2: 2k D n < 2k+1, so k D lg n < k + 1. It follows that the “ﬂoor” of lg n equals k : lg n\u000f = k. Here “ﬂoor” means the largest integer less than or equal to. Observe that 2k+1 − 1 = 11\u00051 ´¹¸¹¹¶ k+1 is the largest binary number that can be represented with k + 1 bits: we need k + 1 = lg n\u000f + 1 bits to represent n. 2 RECURRENCE RELATIONS 11 Example 1.19. How many bits are needed to represent n = 8293417? We must trap n between two powers of 2. For this purpose it is useful to know that 10 3 \b 2 10. We can easily calculate 2 20 = (2 10)2 = (1024)2 = 1048576. It follows easily that 222 < n < 2 23, and 23 bits are needed. In fact, n = 111111010001100001010012. Example 1.20. On the piano, 7 octaves are equivalent to 12 perfect ﬁfths. We can write 2 7 ≈ (3~2)12, so 219 ≈ 3 12, where ≈ means ‘approximately equal’. Which side is greater? Musically, the approximation can be resolved by making every semitone correspond to an interval of 2 1~12, so that a “perfect” ﬁfth corresponds to the ratio 2 7~12 ≈ 1.498 . . . This is the equal temperament system of tuning keyboard instruments, a concept dating back to 1584 or earlier. 2 Recurrence relations 2.1 Recursive functions In this section, we shall be dealing with functions f ∶ N → N. We are used to having such functions deﬁned explicitly, such as f (n) = 3 n + (−2)n − 1 6 n − 13 36 . But one can also deﬁne functions in terms of earlier values, using a prescription like f (n) = œ 0 if n = 0 3f (n − 1) + 1 if n E 1. This gives the table n 0 1 2 3 4 f (n) 0 1 4 13 40 2f (n) 0 2 8 26 80 2f (n) + 1 1 3 9 27 81 from which we might guess that f (n) = 1 2 (3 n − 1). This can be proved by induction. But such explicit formulae are often not possible. 2 RECURRENCE RELATIONS 12 Example 2.1. Deﬁne g∶ N → N by g(n) = ¢¨¨¨ ¦ ¨¨¨¤ n if n = 0, 1 g(n~2) + 1 if n E 2 is even g(3n + 1) + 1 if n E 3 is odd This function is related to the so-called Collatz conjecture or 3n + 1 problem. In fact, g(n) − 1 is the ‘stopping time’ for the Collatz map f (n) = œ n~2 if n E 2 is even 3n + 1 if n E 3 is odd; it equals the number of applications of f needed to reach 1. Note that one can only reach 1 through the sequence 16, 8, 4, 2, 1 though one can reach 16 from both 32 and 5. It is unknown if the stopping time is ﬁnite for every integer n E 2, but it has been checked for all integers up to at least 260 . Of course, g(2 k) = k + 1 since f keeps halving. Let us compute g(7); this is done by recording a somewhat tortuous succession of equations so as to arrive at g(1), and then backtracking with the values: g(1) = 1 g(2) = g(1) + 1 ⇒ g(2) = 2 g(4) = g(2) + 1 g(4) = 3 g(8) = g(4) + 1 g(8) = 4 g(16) = g(8) + 1 g(16) = 5 g(5) = g(16) + 1 g(5) = 6 g(10) = g(5) + 1 g(10) = 7 g(20) = g(10) + 1 g(20) = 8 g(40) = g(20) + 1 g(40) = 9 g(13) = g(40) + 1 g(13) = 10 g(26) = g(13) + 1 g(26) = 11 g(52) = g(26) + 1 g(52) = 12 g(17) = g(52) + 1 g(17) = 13 g(34) = g(17) + 1 g(34) = 14 g(11) = g(34) + 1 g(11) = 15 ↑ g(22) = g(11) + 1 g(22) = 16 ↓ start g(7) = g(22) + 1 g(7) = 17 end We do not know the answer until we have got all the way to the top (and g(1)) and then back down again on the right, to ﬁnd that g(7) = 17. This set-up is called a stack, since the left-hand column resembles a stack of trays in a cafeteria (which is why we started at the bottom): g(5) went in ﬁrst, and g(1) last. Then we could retrieve g(1) ﬁrst and g(5) last. This illustrates the principle ‘Last In First Out’ or LIFO. Later on, we shall meet a diﬀerent set-up, called a queue, in which the ﬁrst item in is the ﬁrst to be processed. So ‘First In First Out’ or FIFO (like most underground lifts). 2 RECURRENCE RELATIONS 13 Here is a table of values of g(n) for n = 0, 1, 2, . . . , 104: 0, 1, 2, 8, 3, 6, 9, 17, 4, 20, 7, 15, 10, 10, 18, 18, 5, 13, 21, 21, 8, 8, 16, 16, 11, 24, 11, 112, 19, 19, 19, 107, 6, 27, 14, 14, 22, 22, 22, 35, 9, 110, 9, 30, 17, 17, 17, 105, 12, 25, 25, 25, 12, 12, 113, 113, 20, 33, 20, 33, 20, 20, 108, 108, 7, 28, 28, 28, 15, 15, 15, 103, 23, 116, 23, 15, 23, 23, 36, 36, 10, 23, 111, 111, 10, 10, 31, 31, 18, 31, 18, 93, 18, 18, 106, 106, 13, 119, 26, 26, 26, 26, 26, 88 2.2 Fibonacci numbers Leonardo di Pisa (c. 1175–1250) found his famous sequence of numbers in connection with the breeding of rabbits. One starts with a newly-born pair of rabbits, one male one female. The idealized assumption is that at one month they mature and become fertile, and at two months (and at each month thereafter) the female gives birth to another male-female pair. Let Fn = F (n) denote the total number of rabbit pairs in the middle of the nth month, so F1 = F2 = 1. Then Fn = #{immature pairs} + #{mature pairs} = Fn−2 + Fn−1 for n E 3. This is because all pairs from a month ago will be mature, and the newly-born immature rabbits are oﬀspring of parents from two months ago. To extend this relation to n = 2, we can set F0 = 0. We then have the recurrence relation Fn = Fn−1 + Fn−2, F0 = 0, F1 = 1, which can be solved recursively. The aim of this section is to show that there is a simple formula for the Fibonacci number Fn . For this purpose, deﬁne ϕ = 1 2 (1 + √5) = 1.6180 . . . , ψ = 1 2 (1 − √5) = −0.6180 . . . In §2.3, we shall show that Proposition 2.2. Fn = 1 √5 (ϕ n − ψn). Note that ϕ and ψ are the roots of x2 − x − 1 = 0 or x 1 = 1 x − 1 , and that ϕ (the positive root) is the so-called golden ratio. We leave proofs of the following statements as exercises. Corollary 2.3. The ratio Fn+1~Fn tends to ϕ as n → ∞. Corollary 2.4. Fn is the closest integer to ϕ n~√ 5 for all n. One can compute the so-called generating function for the sequence (Fn) directly from the recurrence relation. Indeed, 2 RECURRENCE RELATIONS 14 Corollary 2.5. Suppose that SxS < 1~ϕ. Then ∞ Q n=1 Fnxn = x 1 − x − x2 . (1) We can check this by setting λ = x + x 2 and using the binomial expansion (1 − λ)−1 = 1 + λ + λ 2 + λ3 + \u0005 = 1 + x + x2 + (x2 + 2x3 + x4) + (x3 + 3x4 + 3x5 + x 6) + (x4 + \u0005) + \u0005 = 1 + x + 2x2 + 3x 3 + 5x4 + \u0005 In particular, ∞ Q n=1 Fn 10n = 10 89 = 0.11235955. We shall not be concerned with questions of convergence in this course, but Corollary 1 implies that the radius of convergence of the series (1) equals 1~ϕ. A curiosity. Since 1 mile equals 1.609. . . kilometers, Fibonacci’s numbers (if you can remember them) give a suﬃciently accurate way of converting. For example, 144 km/h is almost exactly 89 mph. Example 2.6. The sum sn of the ﬁrst n odd numbers satisﬁes an obvious recurrence relation: œ sn = sn−1 + 2n − 1, s1 = 1 We already know that the solution is sn = n2, but the aim will be to solve such relations systematically without knowing the answer by other means. Deﬁnition 2.7. A recurrence relation of order k will specify an = f (n, an−1, an−2, . . . , an−k) as a function of the k preceding values and possibly n itself. One also needs to prescribe k initial values of the function n ( an . The relation is called linear if the right-hand side equals c0(n) + c1(n)an−1 + \u0005 + ck(n)an−k, for some functions ci(n) of n, as in the previous example. Such a linear relation is called homogeneous if c0(n) is absent, and it has constant coeﬃcients if c1, . . . , ck are independent of n (so constants). The usual relation described the Fibonacci numbers is therefore or order 2, linear, homogeneous with constant coeﬃcients. By contrast, an = an−1 ∗ an−2 also has order 2, but is is not linear (so the other qualiﬁcations are irrelevant). 2 RECURRENCE RELATIONS 15 2.3 Constant coeﬃcients Consider a recurrence relation with constant coeﬃcients: an = c1an−1 + \u0005 + ckan−k + c0(n), (NH) with c0(n) a non-zero function. The associated homogeneous relation is an = c1an−1 + \u0005 + ckan−k, (H) without the term at the end. We are likely to consider only k D 3. Proposition 2.8. (i) If (an) and (bn) are sequences solving (H) (with bn in place of an ) then (Aan + Bbn) will also solve (H) for any A, B ∈ R. (ii) here are k linearly independent solutions to (H) (iii) If (an) and (bb) solve (NH) then (an − bn) solves (H). The proposition is also valid for linear relations, and there is an analogy with ordinary diﬀerential equations. We omit the proofs here. For k = 2, ‘linearly independent’ simply means that one solution is not an overall multiple of the other: sequences with an = n2 and bn = n2 + 1 are independent, but an = n2 and bn = −7n2 are not. (iii) implies that the general solution of (NH) is any particular solution to it plus the general solution of (H). It is known that solutions of (H) are mostly linear combinations of λn, where λ ∈ R is constant. The next example will verify this. Example 2.9. Solve œ an = −an−1 + 6an−2, n E 2 a0 = 2, a1 = −1. Try an = λn . Substituting into the recurrence relation, λ n = 6λ n−2 − λ n−1, and (since we can assume λ ≠ 0), λ2 + λ − 6 = 0 Ô⇒ (λ − 2)(λ + 3) = 0. Taking λ = 2 and λ = 3 gives two independent solutions, and (from (i) and (ii) above) the genral solution is an = A ∗ 2n + B ∗ (−3)n. The constants A, B are determined by the initial conditions, which give 2 = A ∗ 1 + B ∗ 1, −1 = A ∗ 2 + B ∗ (−3) Ô⇒ A = B = 1. The ﬁnal answer is therefore an = 2n + (−3)n . 2 RECURRENCE RELATIONS 16 Example 2.10. For the Fibonacci sequence, the equation is λ 2 = λ + 1 or λ2 − λ − 1 = 0, which has roots ϕ = 1 2 (1 + √ 5), ψ = 1 2 (1 − √5), giving a general solution Aϕ n + Bψn . Then A, B are found by solving 0 = F0 (which implies B = −A) and 1 = F1 = Aϕ + Bψ = A(ϕ − ψ) = A√ 5. This proves the previous proposition. To summarize, here is the strategy for solving (H): Substitute an = λn Obtain a polynomial equation of degree k in λ Find its roots λ1, . . . , λk The general solution is an = A1λn 1 + \u0005 + Akλ n k Find the constants by solving the initial conditions There are two possible snags. The roots may be complex, though if the original equation is real, they will always come in complex conjugates, and the choice of constants Ai will ensure that all solutions are real. Or, there may be repeated roots, in which case (by (ii)) there must exist additional solutions. Example 2.11. Express the solution of an = −an−2 with a0 = 0 and a1 = 1 in closed form. Of course, (an) = (0, 1, 0, −1, 0, −1, 0, . . .), but we are asked for a formula. We have λ 2 + 1 = 0 so the roots are ±i where i = √−1. So the solution is Ai n + B(−i)n, with A + B = 0 and i(A − B) = 1. Then A = −B = − 1 2 i, and the closed formula is an = − 1 2 i(i n − (−i)n) = − 1 2 (in+1 + (−i)n+1). In the case of repeated roots, let us consider what happens when the roots are λ and λ + δ for δ > 0. We know from (i) that (λ + δ)n − λ n δ must be a solution. If we let δ → 0 then in the limit this becomes the derivative of λ n, namely nλn−1 . So we expect this (or equivalently nλn ) to be a second solution. In fact, a repeated root of multiplicity m will allow us to introduce solutions λn, nλn, . . . , nm−1λn. Example 2.12. Find the general solution of an = 2an−1 − an−2 . Here, λ2 − 2λ + 1 = 0 or (λ − 1)2 = 0, so we get an = A ∗ 1 n + B ∗ n ∗ 1 n = A + Bn. 2 RECURRENCE RELATIONS 17 2.4 Particular solutions To solve a non-homogeneous linear equation (NH), proceed as follows. The order is important: Find the general solution of (H) with constants Determine the form of a particular solution of (NH) Substitute to determine any constants in the particular solution Add the two solutions Apply the initial values to determine the constants relating to (H) We shall mostly see assigned functions of the form c0(n) = p(n) ∗ µn, where p(n) is a polynomial such as 7 or n2 or n3 − n + 7. Given such a function, one guesses a solution q(n) ∗ µn, where q(n) is now an arbitrary polynomial of the same degree as p(n). Here are some examples: c0(n) guess 7 α n αn + β 2n α2n n2 3n (αn 2 + βn + γ)3 n One needs to substitute into (NH) to ﬁnd the constants α, β, γ . This will work provided no term in the guess is a solution of (H). In the letter case, one needs to multiply by one or more factors of n. A simple instance follows, though future exercises will clarify this. Example 2.13. Find the general solution of an = −an−1 + 6an−2 + 2n. Had 2 not been a root, we would have tried α 2n, but (since 2n solves (H)) this would have given 0 = 2 n . So we try an = α n 2n . This gives αn2n = −α(n − 1)2 n−1 + 6α(n − 2)2 n−2 + 2 n. Dividing by an−2 , 4nα = −2α(n − 1) + 6α(n − 2) + 4; the terms involving n cancel out (as they must), and we are left with 0 = 2α − 12α + 4. Thus, α = 2~5 and we ﬁnish up with an = A 2n + B(−3)n + 2 5 n2n. Example 2.14. Solve an = −an−1 + 6an−2 + n, a0 = 2, a1 = −1. 2 RECURRENCE RELATIONS 18 The homogenous equations has general solution A ∗ 2 n + B ∗ (−3)n . For a particular solution, we substitute an = αn + β . Since the resulting equation must hold for all n, we can separate out the terms involving n and those that do not. This gives two separate equations, which imply that α = −1~4 and β = −11~16. Then we substitute an = A ∗ 2n + B ∗ (−3)n − 1 4 n − 11 16 to ﬁnd that 2 = A + B − 11 16 , −1 = 2A − 3B − 11 16 giving A = 8~5 and B = 87~80. 2.5 Derangements In this section, we will work with some linear recurrence relations that do not have constant coeﬃcients. Fix a positive integer n. Recall that a permutation of the set Ω = {1, 2, . . . , n} is a bijective mapping f ∶ Ω → Ω. There are n! such permutations. Deﬁnition 2.15. A derangement of Ω is a permutation f ∶ Ω → Ω such that no element i ∈ Ω has the property that f (i) = i. This means that no number ‘stays put’, or (in the language of analysis) f has no ﬁxed point. Let dn denote the number of derangements of {1, 2, \u0005, n}. It is obvious that d1 = 0 (because there is only the identity permutation), d2 = 1 (only swapping 1, 2 works) and d3 = 2 (because only the two 3-cycles do not have a ﬁxed point). Recall that a cycle of order k is a permutation of the form f ∶ i1 ( i2 ( \u0005 ( ik ( i1; here i1, . . . , ik are distinct positive integers. This permutation is denoted by the symbol (i1 i2 . . . ik), which we regard as a function applied (on the left) to numbers. If ij D n for all j then f is an element of order n inside the group of a permutations of {1, 2, . . . , n}. Moreover, f = σ ○ (1 2 . . . k) ○ σ−1, where σ is any permutation that maps j to ij for all j . This establishes the well-known fact that any two k -cycles are conjugate in a group of permutations. To compute d4, we appeal to the description of permutations from group theory. Any permutation can be expressed (uniquely, up to order of the factors) as a product of disjoiont cycles. type of permutation example number of them identity 1 4 cycle* (1234) 6∗ 3 cycle (123) 8 2 cycle (12) 6 pair of 2 cycles* (12)(34) 3∗ 4! = 24 2 RECURRENCE RELATIONS 19 Only the asterisked permutations are derangements, so d4 = 9. One can show that d5 = 44 with a similar table, but then it becomes complicated because of the large variety of cycle decompositions. Fortunately, we can easily ﬁnd a way of computing dn using recurrence relations. It is convenient to deﬁne d0 = 1. Theorem 2.16. (i) dn satisﬁes the second-order linear homogeneous recurrence relation dn = (n − 1)(dn−1 + dn−2), n E 2. (ii) dn satisﬁes the ﬁrst-order linear non-homogeneous recurrence relation dn = ndn−1 + (−1)n. (iii) The ratio dn~n! tends to 1~e as n → ∞. Proof. (i) The deﬁnition of d0 makes the formula work for n = 2. Now let f be a derangement of {1, 2, . . . , n} for n E 3. Suppose that f (1) = k, so k E 2. There are two subcases: (a) f (k) = 1, so the cycle decomposition contains a 2-cycle (1 k). Forgetting 1, k, what is left of f is a derangement of n − 2 objects, and there are dn−2 of these. Together with the choice of i, this gives (n − 1)dn−2 possibilities. (b) f (k) = j ≠ 1. This time, remove k and deﬁne a derangement ˜f of n − 1 objects by setting ˜f (i) = œ j if i = 1 f (i) if i ≠ 1, i ≠ k. There are (n − 1)dn−1 possibilities in this subcase. (ii) Set an = dn − ndn−1 . We want to show that an = (−1)n . Well, an + an−1 = (dn − ndn−1) + (dn−1 − (n − 1)dn−2) = dn − (n − 1)(dn−1 + dn−2) = 0, the last equality by (1). Since a1 = −1, it follows that an = (−1)n . (iii) Rewrite (2) as dn n! = dn−1 (n − 1)! + (−1)n n! = dn−2 (n − 2)! + (−1)n−1 (n − 1)! + (−1)n n! . . . . . . = d1 1! + n Q i=2 (−1)i i! . with a total of n − 1 lines. Since d1 = 0, it follows (strictly speaking, by induction) that dn n! = n Q i=0 (−1)i i! , since the ﬁrst two terms in the summation cancel (by convemtion, 0! = 1). As n → ∞, the summation tends to e−1 . 2 RECURRENCE RELATIONS 20 Here is a table of values: n 1 2 3 4 5 6 7 dn 0 1 2 9 44 265 1854 \u0005 n! 1 2 6 24 120 720 5040 \u0005 dn~n! 0 0.5 0.333 0.375 0.367 0.368 0.368 \u0005 Here the ratio is given to three signiﬁcant ﬁgures, so the probability of ‘no snap’ playing with two decks of 6 shuﬄed cards is already a good approximation to 1~e. 2.6 Other counting applications This section highlights two situations in which recurrence equations also occur naturally. Example 2.17. Consider the system of one-way roads illustrated: P1 P2 P3 P4 P5 P6 P7 Q1 Q2 Q3 Q4 Q5 Q6 Q7 S Let an denote the number of diﬀerent routes from the starting point S to Pn . (If n E 7 the diagram needs extending to the right in the obvious fashion.) To illustrate the method, we ﬁrst take n = 6. One can reach Pn in one step from three directions, shown in red. Namely, travelling north or north-west from the bottom row, or travelling east from P5 . There is only one route to any point on the bottom row, so a6 = a5 + 1 + 1, since there are a5 routes to P5 which can be followed by the one step eastwards. The same argument shows us that an = an−1 + 2. The green steps show that there are 2 routes to P1, so a1 = 2. The solution of this recurrence relation is obviously an = 2n. A similar argument can now be used to count the number bn of routes from S to Qn in the top row. Again, one should consider the immediate predecessors of Qn, which are Pn, Pn+1, Qn−1, provided n E 2. These furnish an, an+1, bn−1 routes, so bn = an + an+1 + bn−1 = bn−1 + 4n + 2, n E 2. One can arrive at Q1 from either P1 or P2, so b1 = a1 + a2 = 6. The homogeneous relation (H) has general solution bn = C = constant, so for a particular solution of (NH) we try bn = An2 + Bn, 2 RECURRENCE RELATIONS 21 giving An2 + Bn = A(n − 1)2 + B(n − 1) + 4n + 2 ⇒ 0 = −2An + A − B + 4n + 2 = 0 ⇒ A = 2, B = 4. We also have 6 = b1 = A + B + C, so C = 0. Therefore bn = 2n2 + 4n. Example 2.18. A gardener has to plant a row of n E 2 rose bushes, which come in three varieties (red, artiﬁcially blue, yellow), observing the following rules: (i) the ﬁrst bush must be red; (ii) the last (nth) bush must be red; (iii) no two colours can be adjacent. We seek the number rn of diﬀerent ways of planting the bushes. The lowest possible value of n is 3 to avoid the two reds together. For n = 3 we just need to choose the middle colour, so r3 = 2. More generally, once we know the colour of the k th bush then there are two choices of colour for bush k + 1. So without condition 2., there are 1 ∗ 2 ∗ 2 ∗ \u0005 ∗ 2 ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ n−1 = 2 n−1 choices. With condition 2., we must insist that bush n − 1 is not red, after which there is no more choice. Therefore bn = 2n−2 − bn−1. The solution is rn = 1 3 ∗ 2n−1 − 2 3 (−1)n, n E 3. When n = 7 (as in the picture), there are 22 ways of planting, illustrated below with each row now vertical. 3 ARITHMETICAL ALGORITHMS 22 3 Arithmetical algorithms 3.1 First concepts Deﬁnition 3.1. An algorithm is a ﬁnite set of unambiguous instructions that when executed terminate in a ﬁnite number of steps. Named after Muhammad ibn Musa al-Khwarizmi (c. 780–850). A more formal speciﬁcation (beyond the scope of this course) takes one into the area of recursive function theory, Turing machines and mathematical logic. Example 3.2. Consider the factorial function N → N. There are two distinct processes that can be used to compute n! Firstly, by iteration. This is exempliﬁed by the following SAGE code: def fac(n): x = 1 for i in range(2,n+1): x = x*i return x In this code, the command range(p, q) lists integers from p to q − 1 (rather than q ). The penultimate line has the eﬀect of replacing x by x times i. Note that the program correctly outputs fac(0) = fac(1). By regarding i more as a variable than a counter, we can replace the ‘for/do loop’ by a conditional: def fac(n): x = 1 i = 1 while i < n+1: x = x*i i = i+1 return x The time needed to carry out the computation is estimated by counting the number n − 1 of multiplications (the most ‘expensive’ operation). If we suppose that each multiplication takes one unit of time, then the total time T = n − 1 satisﬁes T = Θ(n). This equation is shorthand for saying that, for suﬃciently large n, there exist constants 0 < c1 < c2 such that c1n D T D c2n. Equivalently, T = O(n) and n = O(T ), so both T ~n and n~T are bounded as n → ∞. Observe that it does not matter whether a unit of time is one millisecond or one minute. Alternatively, one can use the recurrence relation an = nan−1; this gives the deﬁnition of factorials by recursion: 3 ARITHMETICAL ALGORITHMS 23 def fac(n): if n == 0: return 1 else: return n*fac(n-1) This approach starts by asserting that 0! = 1, which is true for convention, or rather convenience, as it seems to work in many formulae. Let tn denote the number of times multiplication is used to compute fac(n) using the last program. Then tn = tn−1 + 1, so tn = n. Once again, the total time equals Θ(n), but we also require memory that grows linearly with n (unlike in the ﬁrst case). Indeed, the equations stored expand and contract, like the stacking of trays. At some point of the process, we will have fac(5) = 5 ∗ (4 ∗ (3 ∗ (2 ∗ (1 ∗ fac(0))))), and we are stuck if the process is interrupted. 3.2 Powers by squaring Example 3.3. Consider computing xn, where n is a positive integer and x is a number to a given precision. This could be carried out by iteration, mimicking what we ﬁrst did for factorials: def pow(x,n): y = 1 for i in range(1,n+1): y = y*x return y This method requires n − 1 multiplications, but we can ﬁnd a much more eﬃcient way by squaring at intermediate stages. For example, the calculation x19 = (x9)2 x = ((x 4)2 x)2 x = (((x2)2)2 x)2 x uses only 6 multiplications. Here, we have eﬀectively written the exponent 19 = 100112 in binary, adding x added on the right if and only if the remainder is 1. To convert the binary expansion of the exponent n into a systematic procedure, read it from the left with initial value 1 in the ‘register’. Then • square for the privilege of processing the digit, • multiply by x for each digit ‘1’ encountered. 3 ARITHMETICAL ALGORITHMS 24 Starting with 19, the ﬁrst ‘1’ on the left allows us to write 12 ∗ x = x. This is then squared three times, though in processing the next ‘1’, we multiply by x. The ﬁnal ‘1’ causes us to square and multiply by x again, and we are ﬁnished. It would be more eﬃcient to ignore the ﬁrst squaring (which is always 1 ∗ 1) and to process x starting from the second binary digit, but the instructions are slightly neater to state using the full binary expansion. Here is the pseudocode: pow(x,n): y = 1 find the expansion n.binary() for each bit from left to right: y = y*y if bit == 1: y = y*x return y To implement this properly in SAGE one would need to deﬁne ‘bit’ as a successive element in the string consisting of the binary digits of n, but this would obscure the instructions. As it stands, the process should be suﬃciently clear by carrying it out by hand. The only values stored are x, n and each current value of y . The only operations are squaring and multiplying by x. Example 3.4. If x = 3 and n = 11 = 10112, we display each loop vertically. n 1 0 1 1 y in 1 3 9 243 y squared 1 9 81 59049 y out 3 9 243 177147 Thus 3 11 = 177147 was computed with three squarings (ignoring 1∗1) and three multiplications. Here is the same example modulo 16: y in 1 3 9 3 y squared 1 9 1 9 y out 3 9 3 11 Therefore 311 = 11 mod 16. Let’s analyse the eﬃciency. Recall from §1 that the number of bits needed to encode n in binary is lg n\u000f + 1. For each bit, we need to square (a multiplication). Ignoring the ﬁrst 1 ∗ 1 gives lg n\u000f + 1 − 1 = lg n operations. Each ‘1’ after the ﬁrst gives an additional multiplication, so there are at most lg n\u000f of these. So we have a total of 2 lg n\u000f operations, and the algorithm requires O(lg n) operations, which is much more eﬃcient than the iteration program. 3 ARITHMETICAL ALGORITHMS 25 3.3 Euclid’s algorithm Let a, b be integers with b > 0. The aim is to compute their greatest common divisor gcd(a, b). Suppose that a = qb + r, 0 D r < b, which implies that a~b = q + r~b. 0 D r~b < 1. In this situation, we know that r = a mod b, but to emphasize that r < b we can use the exact formula r = a − a~b\u000fb. This remainder is denoted a%b in C++ or SAGE. Lemma 3.5. With this notation, gcd(a, b) = gcd(b, r). Proof. Suppose that s = gcd(b, r). Then sSb and sSr . Thus sSa, and s is a common divisor of a and b. Suppose that t is another common divisor of a and b. Then tSr, so t is also a common divisor of b and r, and (since s is the greatest such) t D s. Therefore s is indeed the greatest common divisor of a and b. Eulcid’s algorithm now consists of starting from (a, b), and then repeatedly performing divison and applying the lemma. Since ri+1 < ri, we must have rn = 0 for some n: a = q0b + r1 b = q1r1 + r2 r1 = q2r2 + r3 \u0005 rn−2 = qn−1rn−1 + 0. Applying the lemma, we are led to gcd(rn−1, 0) = rn−1 . So this equals gcd(a, b). The code is therefore quite simple: def euc(a,b): if b == 0: return a else: return euc(b,a%b) If we keep track of ri as a linear combination of ri−1 and ri−2 , simpliﬁed at each stage, this we will obtain integers x, y for which gcd(a, b) = xa + yb. This is called Euclid’s Extended Algorithm. There is a quick way of carrying this out by hand using matrices. We shall illustrate the method next. We ﬁrst set up a matrix of the form „a 1 0 b 0 1‚ 3 ARITHMETICAL ALGORITHMS 26 in which the second column keeps track of coeﬃcients of a and the third column those of b: the ﬁrst row is to be interpreted as telling us that a = 1 ∗ a + 0 ∗ b and the second that b = 0 ∗ a + 1 ∗ b. One then subtracts the row with the smallest ﬁrst entry (initially the second if b < a) from the other row. One repeats this step until one row row begins with a ‘0’, in which case the entry above or below it equals gcd(a, b), and the remaining entries of that row give x and y . Example 3.6. To compute gcd(33, 93), we have „ 93 1 0 33 0 1 ‚ → „27 1 −2 33 0 1 ‚ → „ 27 1 −2 6 −1 3 ‚ ↓ „3 5 −14 0 −11 31 ‚ ← „ 3 5 −14 6 −1 3 ‚ There is no need to swap the rows each time provided one is happy to subtract the ﬁrst from the second. The ﬁnal ‘0’ tells us that gcd(93, 33) = 3 (of course, this was obvious) and 3 = gcd(33, 93) = 5 ∗ 93 − 14 ∗ 33 (the coeﬃcients x = 5 and y = −14 were less obvious). We shall not prove formally that this method does produce the correct result, but one can understand it as follows. The new numbers 27, 6, 3 in the ﬁrst columns are simply the remainders r1, r2, r3, with r4 = 0. And since we are performing elementary row operations, each row (ri xi yi) tells us that ri = xi ∗ 93 + yi ∗ 33. The penultimate one gives us the desired expression for r3 = gcd(93, 33). Example 3.7. We compute gcd(33, 93) on the left below, and using the steps on the right we express it as a linear combination of 33 and 93. 33 = 0 ∗ 93 + 33 93 = 2 ∗ 33 + 27 27 = 1 ∗ 93 − 2 ∗ 33 33 = 1 ∗ 27 + 6 6 = 1 ∗ 33 − 1 ∗ 27 = 33 − (93 − 2 ∗ 33) = −93 + 3 ∗ 33 27 = 4 ∗ 6 + 3 3 = 1 ∗ 27 − 4 ∗ 6 = (93 − 2 ∗ 33) − 4 ∗ (−93 + 3 ∗ 33) = 5 ∗ 93 − 14 ∗ 33 6 = 2 ∗ 3 + 0 The conclusion is 3 = gcd(33, 93) = 5 ∗ 93 − 14 ∗ 33. Note that one saves one line if initially SaS > b. On the right-hand side, once an equation (like 27 = \u0005) has been used twice, it can be discarded. We can avoid all the subscripts above, and express it more succinctly with the following pseudocode: EEUCLID(a,b) if b=0 then return (a,1,0) (d’,x’,y’) ← EEUCLID(b, a mod b) (d,x,y) ← (d’,y’,x’- a~b\u000fy’) return (d,x,y) 3 ARITHMETICAL ALGORITHMS 27 3.4 Consolidation The aim of this section is to draw together many of the topics we have seen so far, namely Induction (§1,1), the Fibonacci numbers (§2.2), and logarithms (§1.4 and §3.2), in order to analyse the eﬃciency of Euclid’s algorithm (§3.3). We shall show that, like our method of exponentiation by repeated squaring, it executes in ‘log time’. Example 3.8. Consider the function f (x) = 11x + 5. As it stands, it deﬁnes both a mapping R → R and a mapping Z → Z. The ﬁrst is a bijection, the second is not (because the multiplicative inverse 11 −1 does not exist in Z). We now want to work modulo 26, this being the number of charcaters in the English alphabet. Write x1 ≡ x2 to mean x1 = x2 mod 26, i.e. 26 S (x1 − x2). Then x1 ≡ x2 Ô⇒ f (x1) ≡ f (x2), and because of this it makes sense to regard f as a mapping between congruence classes modulo 26. To do this properly, we deﬁne ˜f ∶ R → R, R = {0, 1, 2, . . . , 26} by ˜f (x) = f (x) mod 26 ∈ R, explicitly f (x) − 26 f (x)~26\u000f. Then ˜f ∶ R → R is a bijection, i.e. a permutation of R. This is because 11, 26 are coprime, 1 = gcd(26, 11) = 3 ∗ 26 − 7 ∗ 11, and 11 −1 ≡ −7 exists modulo 26. Thus, y ≡ 11x + 5 Ô⇒ x ≡ 11 −1(y − 5) ≡ −7y + 9. One could use ˜f ∶ x ( y and its inverse y ( x as a simple way to encrypt/decrypt words in the alphabet {A, B, C, . . . , Z}, but it could be broken by frequency analysis of letters (of the message is large enough). A more sophisticated method based on similar principles is the so-called Vigenère cipher. Deﬁnition 3.9. Suppose that a > b > 0. Let s(a, b) denote the number of steps needed in executing Euclid’s algorithm so that the remainder becomes 0 in the ﬁnal step. It is reasonable to suppose that s(a, b) estimates the time required to compute gcd(a, b). If s(a, b) = n then we are stating that (in our previous notation) rn = 0 and rn−1 ≠ 0. Example 3.10. One has s(93, 33) = 4 s(33, 93) = 5. Computer software quickly reveals that gcd(100! + 1, 100100 − 1) = 101 s(100! + 1, 100100 − 1) = 337. 3 ARITHMETICAL ALGORITHMS 28 Fibonacci numbers are relevant to the implementation of Euclid’s algorithm. The following scheme implements the algorithm to determine the greatest common divisor of two adjacent Fibonacci numbers: Fn+2 = Fn+1 + Fn Fn+1 = Fn + Fn−1 ⋮ = ⋮ F5 = F4 + F3 F4 = F3 + F2 F3 = 2 F2 + 0 This is because each Fibonacci number like Fn+1 divides exactly once into the next higher one Fn+2 with remainder Fn (because twice Fn+1 into Fn+2 won’t go!). The conclusion is that gcd(Fn+2, Fn+1) = F2 = 1, so any two adjacent Fibonacci numbers are coprime. Here is an easy example: 13 = 1 ∗ 8 + 5 8 = 1 ∗ 5 + 3 5 = 1 ∗ 3 + 2 3 = 1 ∗ 2 + 1 2 = 2 ∗ 1 + 0. Note that s(13, 8) = s(F7, F6) = 5. More generally, one needs exactly n steps to compute gcd(Fn+2, Fn+1), because there are n remainders (including 0) in the general scheme above. The next results records this fact and generalizes it: Proposition 3.11. (i) We have s(Fn+2, Fn+1) = n for all n E 1. (ii) If a > b > 0 and s(a, b) = n then a E Fn+2 and b E Fn+1. Proof. It remains to prove the second statement, which we do by induction on n. It is obviously true for n = 1 since F3 = 2 and F2 = 1 and one only step is needed. Assume that the second statement is true when n is replaced by n − 1. Let r be the ﬁrst remainder: a = qb + r = q0b + r1. Since s(a, b) = s(b, r) + 1, we have s(b, r) = n − 1. By hypothesis, b E Fn+1 and r E Fn. But then a E b + r E Fn+1 + Fn = Fn+2. So the second statement is true for our ﬁxed value of n. Therefore it is true for all n. 3 ARITHMETICAL ALGORITHMS 29 There are other striking properties relating Fibonacci numbers to division and Euclid’s algorithm. We quote without proof the Theorem 3.12. Let m, n E 3. Then mSn if and only FmSFn . This means that the mapping n ( Fn ‘respects’ divisibility, and (as an exercise) it follows from the theorem that gcd(Fm, Fn) = Fgcd(m,n). A simple example is gcd(F8, F12) = gcd(21, 144) = 3 = F4, 4 being gcd(8, 12). Another problem a bit beyond the scope of the course is to ﬁnd the most eﬃcient way of computing the Fibonacci numbers themselves. For example, how many steps and how much memory is needed to compute F12 ? Returning to Euclid’s algorithm, we shall use the formula Fn = 1 √5 (ϕ n − ψn) from §2.2, where (recall) ϕ and ψ are the roots of x 2 − x − 1 = 0 with ψ < 0. If n is odd then ψn < 0; we deduce that Fn > 1 √5 ϕ n if n is odd. Now suppose that a > b > 0 and s(a, b) = n. By part (ii) of the last Proposition, b E Fn+1 > 1 √ 5 ϕ n. The second equality is true for all n, because if n is even we can actually replace ϕn by ϕ n+1 , whereas if n is odd we ﬁrst use the fact that Fn+1 E Fn . Therefore √5 b > ϕ n and 1 2 lg 5 + lg(b) > n lg(ϕ). Dividing by lg(b) shows and letting b → ∞ shows that there exists a constant c > 0 such that s(a, b) = n < c lg b. This can be expressed in ‘big O’ notation by the Theorem 3.13. Let a > b > 0. Then s(a, b) = O(lg b) as b → ∞. We conclude on a more elementary note by constructing a counterpart of the greatest common divisor. Let a, b ∈ N, and set g = gcd(a, b). In particular, g is a common divisor and we can write a = ga ′, b = gb ′. Consider the positive integer ℓ = ab g = ga ′b ′. 4 BASIC GRAPH THEORY 30 Proposition 3.14. ℓ is the lowest common multiple of a and b. That is, 1. a S ℓ and b S ℓ; 2. if a S m and b S m then ℓ D m. Proof. Condition 1. is immediate. For 2., write m = am1 = bm2, and recall that g = xa + yb for some x, y ∈ Z. Consider gm = (xa + yb)m = xabm2 + ybam1 = ab(ym1 + xm2). Since ym1 + xm2 ∈ Z, we have ℓ S m. In particular ℓ D m. 4 Basic graph theory 4.1 Deﬁnitions A graph consists of a ﬁnite set V of vertices and a ﬁnite family E of pairs of elements of V, the edges. (The edges are deﬁned as a family rather than a set so as to allow for multiple edges between two vertices. Moreover, an edge could consist of a loop from a vertex to itself, so the pair should be an ordered pair even though the order will not matter until we discuss digraphs.) Example 4.1. A ‘triangle’ with three vertices: V = {a, b, c} or (a, b, c), E = (ab, bc, ca). If we add an isolated vertex d, V = {a, b, c, d} but E stays the same. A ‘ﬁgure eight’ with one vertex: V = {o}, E = (oo, oo). A lower case ‘theta’ with 2 vertices and 3 edges: S = {a, b}, E = (ab, ab, ab). A graph is simple if there are no multiple edges and no loops. (In this case, E can be deﬁned as a set of unordered pairs of vertices, but it is still easier to write ab or v1v2, or even 12, than {a, b}, {1, 2} etc.) The graph is directed or a digraph if each edge has an arrow, in which case each edge really is (in the logical sense) an ordered pair like (a, b). To emphasize that the order is now important, one can denote the edge by a → b, notation that may be closer to its meaning (like a one-way ﬂow). 4 BASIC GRAPH THEORY 31 Example 4.2. Quite simple sets give rise to interesting graphs. Let S = {1, 2, 3, 4, 5} and let V be the set of all subsets of S of size 2. So V = {12, 13, 14, 15, 23, 24, 25, 34, 35, 45} (where 12 is shorthand for {1, 2} etc., as explained above) and SV S = › 5 2” = 10. We shall join two vertices (elements of V ) by an edge if and only if the two subsets are disjoint. The result is called the Petersen graph. It is an example of a regular graph: the degree if every vertex is the same: There are only 15 vertices, though it is impossible to represent the graph in 2 dimensions without edge crossings, since it is not a planar graph, a topic to be explored later. Example 4.3. We shall deﬁne a digraph with vertex set V = {2, 3, 4, 5, 6, 7, 8, 9} using modular arithmetic. Regard the elements of V as congruence (or residue) classes modulo 11 (we have excluded 0, 1 and 10 ≡ −1). The set of directed edges consists of pairs (i, j) for which j = i 2 mod 11. The vertices 3, 4, 5, 9 of the ‘square’ are the so-called quadratic residues modulo 11; they are elements admitting a square root mod 11: 2 4 3 9 5 67 8 The degree of a vertex v, written d(v), is the number of occurrences of v as an endpoint in the family of edges. Note that a loop will contribute 2 to the degree. If the graph G is simple then the degree is also the number of vertices joined to v by an edge. One often denotes the maximum degree of any vertex in G by ∆(G), and the minimum by δ(G). 4 BASIC GRAPH THEORY 32 Proposition 4.4. For any graph, the sum of the degrees of all vertices equals twice the number of edges: ∑ v∈V d(v) = 2SES. Proof. We can prove this by induction on SES. Given a graph with n edges, remove any one. Either it joined two distinct vertices, or it was a loop at one vertex. In either case, we have reduced the sum of the degrees by 2. So assuming the result for n − 1 edges (and it certainly holds for one edge), it remains true for n edges. Example 4.5. There is no graph with vertex degrees 2, 3, 3, 5. Deﬁnition 4.6. Two graphs G1 = (V1, E1), G2 = (V2, E2) are isomorphic if there exists a bijection f ∶ V1 → V2 between their vertex sets such that the number of edges between any two vertices a, b ∈ V1 equals the number of edges between f (a), f (b) ∈ V2 . For simple graphs, this amounts to the assertion that (a, b) ∈ E1 ⇐⇒ (f (a), f (b)) ∈ E2. It is often easy to see that two graphs are not isomorphic, less easy to prove that they are. To show that two graphs are isomorphic, one must construct an isomorphism. To show that they are not isomorphic, one looks for some property which is diﬀerent in the two graphs, such as the sequence of vertex degrees (if one is lucky), or the existence of cycles of a given length (see §4.2). Example 4.7. A complete graph with n vertices is a simple graph in which any two vertices are joined by an edge, so there are › n 2” edges. Any two are isomorphic so we can speak of the complete graph with n vertices. It is denoted Kn . The ﬁgures shown are representations of K5 and K6 : 4 BASIC GRAPH THEORY 33 4.2 Connectivity Two vertices u, v of a graph G are adjacent if there is an edge uv ∈ E whose endpoints are u and v . The edge is said to join u and v . A walk from u to v is a sequence of edges v0v1, v1v2, . . . , vn−1vn with u = v0 and v = vn . The length of the walk is the number n of edges (we also allow u = v and n = 0). A walk may be written v0v1\u0005vn . Best not to write v0 → v1 → \u0005 → vn if G is not a digraph. A trail is a walk with no repeated edges. A path is a trail with no repeated vertices except possibly v0 = vn . A walk/trail/path is closed if v0 = vn, i.e. it starts and ﬁnishes at the same vertex. A cycle is a closed path with at least one edge. An n-cycle is a cycle of length n. Hence a loop is a 1-cycle and a 2-cycle only appears if there is a multiple edge. Examples. Referring to the old underground map below, consider the stations A Aldwych! C Charing Cross H Holborn O Oxford Circus P Picadilly Circus S South Kensington T Tottenham Court Road. 4 BASIC GRAPH THEORY 34 Then LTOPL is a 4-cycle (and path) SPOTHA path of length 5 SPLTOPC trail (and walk) [repeated vertex] SPLTOP trail (and walk) [P still repeated] SPLTOPLH walk [repeated edge] Deﬁnition 4.8. Two vertices u, v of a graph G are connected if one can walk from one to the other and we can write u ∼ v . (This implies there is a path between any two vertices, why?) Then ∼ is an equivalence relation, and it partitions V into one or more subsets, the components of G. The graph G itself is connected if there is just one component, in which case any two of its vertices are joined by a path. A subgraph of a graph G = G(V, E) is any graph G′ = G′(V ′, E′) such that V ′ ⊆ V and E′ ⊆ E . Note that E′ might not include all the edges in G that join vertices in V ′, though if it does one says that G′ is vertex-induced (from V ′ ). A component of a graph G is then a maximal connected subgraph G′, i.e. G′ is connected but if one more vertex or edge from G is added then the subgraph is no longer connected. A disconnecting set of a connected graph G is a set of edges whose removal makes the new graph disconnected. When an edge is removed the vertices which are its endpoints are retained. Of particular importance is the special case: 4 BASIC GRAPH THEORY 35 Deﬁnition 4.9. A cutset of a connected graph G is a disconnecting set, no proper subset of which is a disconnecting set. Thus, if any one of the edges in a cutset is retained then the graph stays connected. If a cutset consists of a single edge then this edge is called a cut-edge or a bridge. A separating set of a connected graph G is a set of vertices whose removal makes the new graph disconnected. When a vertex is removed all the edges which have it as an endpoint are also removed. If a separating set of a connected graph G consists of a single vertex that vertex is called a cut-vertex. A tree is a connected graph with no cycles, though there are alternative deﬁnitions (to be seen later). A bipartite graph is a graph in which V, the set of vertices, is the union of two disjoint non-empty sets V1 and V2 and all the edges have one endpoint in V1 and the other endpoint in V2 . (Hence no two vertices in V1 are adjacent and no two vertices in V2 are adjacent.) One can also prove the useful Theorem 4.10. A graph is bipartite if and only if there are no cycles of odd length. Example 4.11. Kn,m is the complete bipartite graph where SV1S = n, SV2S = m and every vertex in V1 is adjacent to every vertex in V2 . Here we see (m, n) = (13, 7) and (3, 3): 4.3 Eulerian graphs Recall that a cycle is a closed path. It is therefore a walk that passes through no edge twice and (apart from being joined up so that it ﬁnishes where it started) no vertex twice. Lemma 4.12. If a graph G is connected and every vertex has degree at least 2 then G contains a cycle. Proof. A loop deﬁnes a 1-cycle, and a multiple edge deﬁnes one or more 2-cycles, so we may assume that G is simple. Choose any vertex v1 . Let v2 be an adjacent vertex (meaning there is an edge v1v2 ). Since v2 has degree at least 2, it must have an adjacent vertex v3 distinct from v1 . Continuing in this way, since V is a ﬁnite set, we must eventually ﬁnd a vertex already in the list, say vj = vi with 1 D i < j, and vi+1, . . . , vj all distinct. Then vivi+1\u0005vj is the desired cycle. 4 BASIC GRAPH THEORY 36 Example 4.13. Graph theory is said to have begun with Euler’s paper solving the problem of the seven bridges of Könisberg near the Baltic Sea (now the Russian city of Kaliningrad sandwiched between Poland and Lithuania and disconnected from Moscow). The question was whether there exists a trail that crosses each bridge exactly once. There can’t be one because when one converts the map into a graph with each land mass a vertex and each bridge an edge, all the vertices have odd degree, so it is impossible to ‘come and go’ without repeating an edge. Next we’ll make this precise. Deﬁnition 4.14. A connected graph is semi-Eulerian if there is a trail which includes every edge of the graph. (Thus it passes along each edge exactly once and goes through every vertex, but some vertices can be visited more than once.) The graph is Eulerian if such a trail can be found that is closed. Deﬁnition 4.15. Suppose that G is a connected graph. Then G is Eulerian if and only if every vertex has even degree. Theorem 4.16. A connected graph is semi-Eulerian if and only if it has at most two vertices of odd degree. These results were known to Euler, who studied the bridges problem in 1736, though a formal proof was ﬁrst given in 1871 by Hierholzer (who died that year aged 31). Note. No graph can have an odd number of vertices of odd degree, because the ‘total degree’ must be even. We shall deduce the corollary from the theorem by a neat trick. Proof of the theorem. If G has a closed ‘Eulerian trail’, then follow along it from a starting vertex. At each new vertex, we can mark oﬀ the arriving edge and the departing edge. Both are traversed once and only once, so the degree of each vertex (including the start=ﬁnish one) must be even. This justiﬁes the ‘only if’ part of the theorem. The harder ‘if’ part can be proved by induction on the number of edges of G, using the lemma to ﬁrst remove a cycle and work on what is left of G. However we shall explain how one can eﬀectively construct a closed Eulerian trail using (what is now called) Hierholzer’s algorithm. Suppose then that G(V, E) is a connected graph, all of whose vertices are of even degree. Fix a vertex v0 ∈ V . A ﬁrst observation (which is a reﬁnement of the lemma) is that one always ﬁnd a trail (with at least one edge) in G that eventually returns to v0 . This is because, whichever 4 BASIC GRAPH THEORY 37 edge one chooses to walk along and whichever vertex one arrives at, there will always be an edge available to leave that vertex. (This is guaranteed by the even degree property, even if the trail has already passed through the same vertex.) But the graph is ﬁnite, so we must eventually return to v0 . With this observation, we type out the algorithm in pseudocode. Each step can in fact by implemented by instructing a computer how to process data (lists of vertices and edges) representing the graph. Let T be a closed trail of any length (even zero) starting and ending at v0 . # Hierholzer’s algorithm def aug(T): if T contains all the edges of G: return T else: walk to the first vertex v1 in T with a spare edge construct new trail T1 from v1 to v1 using spare edges insert T1 as a detour into T to form a longer trail T+ return aug(T+) A ‘spare edge’ means an edge of G that does not appear in G; there will always be one if T is not already Eulerian. The argument above shows that the new trail T1 can be found, and inserting it as a detour into T yields the ‘augmented’ trail T + . If T is empty, T1 will be a trail with at least one edge (it could be a loop) based at v0 . We could have asked the programme to output T +, but in that case it would be necessary to start afresh with T + in place of T . By calling aug(T +), we have made the function recursive by requiring the program to keep applying the ‘else’ step until it ﬁnds a trail with all the edges of G. Example 4.17. The complete graph Kn is Eulerian if and only if n is odd (for then all vertex degrees are even). Let us apply the algorithm to K5 with vertices a, b, c, d, e. Take T to be the empty trail (denoted g) at v0 = a. When it comes to spare edges, let’s always move to a vertex of least alphabetical order. This causes the program to manufacture three separate detours, illustrated below with the colours red, green, blue, and to return the Eulerian path abdaebcdeca with (as has to be) 10 edges. 4 BASIC GRAPH THEORY 38 input → T = g T1 = ³·µ abca T + = abca T = abca T1 = ³¹¹·¹¹µ bdaeb T + = abdaebca T = abdaebca T1 = ³·µ cdec T + = abdaebcdeca T = abdaebcdeca ← output Proof of the corollary assuming the theorem. Suppose that G has exactly two vertices u, v of odd degree. Add an extra edge from u to v . In this section, we are not assuming graphs are simple, so if there was already an edge from u to v (or more than one), we simply add another. The point is that the new graph has all its vertices of even degree so, by the theorem, it possesses a Eulerian trail, which must incorporate the extra edge. If the latter is removed, we end up with a Eulerian trail for G that starts at u and ﬁnishes at v . Conversely, if we had such a trail we can make it closed by again adding an extra edge, and conclude that the modiﬁed graph has all its other vertices of even degree. Now suppose that all vertices have even degree. If the graph has loops or extra edges connecting two vertices then (by ﬁrst removing loops and pairs of multiple edges) we can modify any trail on the remaining simple graph to include what we removed. So we may suppose that G is simple. Argue by induction on the number SES of edges. If this is 3, then G itself is a single 3-cycle. 4 BASIC GRAPH THEORY 39 Now take a graph with SES > 3, and assume that the result is true for graphs with less edges. By the lemma, G contains a cycle C, and we can assume this is not all of G (otherwise we are ﬁnished). Remove all its edges and any isolated vertices; the resulting graph may have a number of components, G1, . . . , Gk with k E 1. Each component Gi is simple, has less edges than G, and its vertices have even degree (because if one was on C we have removed two edges). By way of induction hypothesis, we may assume that Gi has a closed trail Ci passing along all the edges of Gi exactly once. If we now start from a vertex of C and walk along it, the ﬁrst time we encounter a component, we can insert the trail Ci, and then continue along C until we meet a vertex of the next component of G minus the cycle. And so on. This process gives a Eulerian trail for G, and the induction succeeds. A more popular way of ﬁnding an Eulerian trail is Fleury’s algorithm. It can be summed up by the slogan “do not burn bridges”. Recall that a bridge in a connected graph is an edge that when removed will cause the new graph to be disconnected. We shall mimic our description of the previous algorithm to deﬁne Fleury’s, but instead of proving that it always succeeds we make do with an example. We shall implement the algorithm to construct a semi-Eulerian trail, so this time let G be a connected graph with at most two vertices of odd degree. Fix a vertex v0 of odd degree if there is one, and a trail T starting at v0, possibly empty. # Fleury’s algorithm def fl(T): G’ = G with edges and isolated vertices of T removed w = last vertex in T choose an edge of G’ at w avoiding a bridge if possible T+ = T with the new edge added return fl(T+) Then ﬂ(T ) will be a semi-Eulerian trail for G. Example 4.18. In the graph illustrated immediately below, there are two ‘odd’ vertices, so we start top right (green), and aim to ﬁnish at the adjacent (red) vertex. After traversing 4 edges, we have constructed a trail T shown in grey. To obtain G′, we discard its 4 grey edges and 1 isolated vertex. The point then is that we must complete the ‘left wing’ before crossing the top bridge. (This is obvious to the human eye, but programming a computer to recognize a bridge would be an unwanted complication.) Thus, T + will be formed from T by adding either the left horizontal edge or the vertical one. 4 BASIC GRAPH THEORY 40 Example 4.19. We noted that Kn is Eulerian iﬀ n is odd (for then all vertex degrees are even). Each of the › 7 2” = 21 edges of K7 can be identiﬁed with a domino, and each domino with equal values side by side deﬁnes a loop at a vertex of K7 . A Eulerian trail in K7 with its 7 loops is then a ‘domino cycle’, like the smaller n = 5 version: 4.4 Hamiltonian cycles Deﬁnition 4.20. A connected graph is Hamiltonian if there is a closed path (and so, a cycle) that visits every vertex exactly once. The closed path is called a Hamiltonian cycle. A Eulerian trail must, by its very nature visit every vertex, but it is allowed to do so more than once. By contrast, a Hamiltonian cycle must visit each vertex only once, and will not in general pass along every edge. Despite the analogy with Eulerian trail, deciding when a graph is Hamiltonian is much harder and remains the subject of current research. There are no wide-ranging theorems that characterize Hamiltonian graphs, and most results that are known are rather restrictive and of limited value for the graphs encountered in this course. The following is one such result, which we state without proof. Theorem 4.21 (Gabriel Dirac’s Theorem). Let G(V, E) be a simple graph with SV S = n E 3. If d(v) E n~2 for all v ∈ V (equivalently, n D 2δ(G)) then G is Hamiltonian. 4 BASIC GRAPH THEORY 41 Example 4.22. Since δ(Kn) = n − 1, the theorem does tells us that Kn is Hamiltonian for all n E 2. A bipartite graph with an odd number of vertices cannot be Hamiltonian, because any Hamiltonian cycle would be odd. The 12 edges and 6 vertices of an octahedron form a graph that (like those arising from the other platonic solids) is Hamiltonian. The Petersen graph is not Hamiltonian, but it does have a (non-closed) path passing though every vertex. The Grötzsch graph ¨G has 11 vertices and 20 edges, and by contrast is Hamiltonian. Here are two representations of it: Exercise 4.23. For which values of n does ¨G possess an n-cycle? A celebrated example of a Hamiltonian graph is the knight’s graph N with 64 vertices, deﬁned as follows. Consider a knight moving freely on a standard (8 × 8) chess board, without other peices to get in the way. Assign a vertex to each square, and declare two vertices to be adjacent if a knight can move legitimately between them (that is, two squares one way and one sideways). If a knight starts on one of the 16 squares near the centre of the board, it has 8 squares it can move to, but this choice is reduced closer to an edge of the board. The edges of the central 6 × 6 square allow 6 moves, except its corners that allow only 4. Overall, N has 4 vertices of degree 2 8 ” 3 20 ” 4 16 ” 6 16 ” 8 It follows that the total number of edges is e = 1 2 (4 ∗ 2 + 8 ∗ 3 + 20 ∗ 4 + 16 ∗ 6 + 16 ∗ 8) = 168, and a Hamiltonian cycle uses 64 of them. Not surprisingly, it is diﬃcult to ﬁnd such a knight’s tour from scratch. The following instance was constructed (by the lecturer for Module Selection) 5 VERTEX COLOURING 42 by starting in a corner, and using Warnsdorﬀ’s (not infallible) rule: move the knight to a square from which there is the least number of successive moves possible. The knight therefore hugs the edges as long as it can; each number (from 1 to 64) labels the start of the corresponding edge of the cycle: The knight’s graph is bipartite because the vertices (squares) are divided into black and white, and a knight changes colour each move. From the remarks above, there cannot exist a knight’s tour on (for example) a 7 × 7 chequer board. 5 Vertex colouring 5.1 Chromatic number In this section, all graphs will be simple. The problem then is to assign a colour to each vertex of a graph so that no two adjacent vertices have the same colour, and to do this using the least number of colours. In mathematical language, a vertex colouring of a graph G(V, E) is a mapping c∶ V → N with the property uv ∈ E Ô⇒ c(u) ≠ c(v). This means that adjacent vertices have diﬀerent colours (values of c), and to do this with as few colours as possible means reducing the image of c. We are using positive integers to label the colours, though in examples we shall use Greek letters in their alphabetical order α, β, γ, δ, ε, ζ, . . . On screen, we can use actual colours, such as red, green, blue, and yellow. A graph is k -colourable if we can ﬁnd a vertex colouring with S Im c S = k . In this case, one of k colours can be assigned to each vertex such that no two adjacent vertices have the same colour. 5 VERTEX COLOURING 43 Deﬁnition 5.1. The chromatic number of a simple graph G, denoted χ(G), is the least value of k for which G is k -colourable. Observation. It is impossible to ﬁnd a vertex colouring if G has a loop uu. Multiple edges do not aﬀect the colouring problem, but in any case, we restrict to simple graphs. χ(G) = 1 if and only if all the vertices of G are isolated, not an interesting scenario. χ(G) = 2 if and only if G is bipartite (see the end of §4.2). This is really the deﬁnition of bipartite: one can regard the colours as ‘positive’ and ‘negative’ or (in the chess example in §4.4) white and black. If SV S = n then obviously G is n-colourable and χ(G) D n, but usually we can make do with many fewer colours because χ(G) is much more closely related to the degrees of vertices in G. In this sense, the next example is not typical. Example 5.2. Since every vertex of Kn is joined to every other, we have χ(Kn) = n. Let Cn denote the ‘cycle graph’ consisting of the n vertices and n edges of a polygon. Then χ(Cn) = œ 2 if n is even, 3 if n is odd. It follows that: G contains Kn as a subgraph ⇒ χ(G) E n G contains an odd cycle as a subgraph ⇒ χ(G) E 3. Application. Vertex colouring can be used to solve the timetabling problem with: • a set of modules (the vertices); • groups of students who have selected pairs of modules (the edges); • a limited number of time slots (the colours); • an unlimited number of lecture rooms (to simplify the problem). If no adjacent vertices have the same colour all students can attend all the lectures for the modules they have chosen! In the graph G above, a popular module has ‘degree’ 5. Four modules (left) form a complete subgraph K4, so no less than 4 colours will suﬃce. Thus χ(G) = 4. 5 VERTEX COLOURING 44 5.2 Colouring results The whole theory of vertex colouring depends on the so-called greedy algorithm. This is a natural way of colouring the vertices when they are put in order, and (in very informal language) can be exporessed as follows: label the vertices v_1,v_2,...,v_n label the colours 1,2,...,n assign to v_1 colour 1 for j in range(2,n+1): S = {colours assigned to vertices adjacent to v_j} assign to v_j the smallest colour not in S return S and the assignments Example 5.3. Diﬀerent vertex orderings can give very diﬀerent numbers of colours. The ‘cube graph’ is bipartite so χ = 2, and this is illustrated on the left. But the greedy algorithm produces 4 colours (here 1 = R, 2 = G, 3 = B, 4 = Y ) when applied to the ordering on the right: For any G, it can be shown that there always exists some vertex ordering for which the greedy algorithm gives the minimum number (namely, χ(G)) of colours. Recall that ∆(G) = max v∈V d(v), where d(v) is the degree (valency) of the vertex v . For example, ∆(Kn) = n−1 and χ(Kn) = n. Here are the main results on vertex colouring, in increasing diﬃculty. Lemma 5.4. If G is simple then χ(G) D ∆(G) + 1. Proposition 5.5. If G is simple, connected and not regular (not all vertices have degree ∆) then χ(G) D ∆(G). Theorem 5.6 (Brooks’ theorem). If G is simple, connected and neither complete nor an odd cycle (so G ~≅ Kn and G ~≅ C2k+1 ) then again χ(G) D ∆(G). Question 5.7. Why must we add ‘connected’ in the last two statements? 5 VERTEX COLOURING 45 Proof of the lemma. Let k = ∆(G), and ﬁx a set of k + 1 colours. Take the vertices in any order. Suppose we have managed to colour some (or g) of them. The next (or ﬁrst) vertex is surrounded by at most n adjacent vertices, so we can colour it without a clash, and move on to the next vertex in the list. Note that the proposition reduces the proof of the theorem to the case of regular graphs. We shall prove the former, but not the latter. Incidentally, if we assume that ∆(G) E 3, there is no need to mention the odd cycle in the theorem. 5.3 Brooks’ algorithm The proof of the proposition is accomplished by implementing a 2-step process that is sometimes called Brooks’ algorithm. This produces an ordering of the vertices (or a re-ordering if we have one already), relative to which (as we shall explain) the greedy algorithm will always succeed with at most ∆ colours. We shall illustrate it with a modiﬁcation of the dodecahedral graph with 12 vertices, in which we have removed one edge (leaving 29) so that the graph is no longer regular. We have labelled the vertices with numbers 1, 2, . . . , 20 in no special way, and the missing edge is (1, 6). Rather than type out the instructions, we shall explain the process with a table. In general, let G be a graph with ∆(G) = k but not regular. Choose a vertex with degree less than k to start, and place the vertex in a ‘queue’. At each stage, the ﬁrst element in the queue is moved to the left-hand list, and any adjacent vertices not previously queued are added at the back of the queue (in any order, but for deﬁniteness one can add them in increasing order). In our example, k = 3, and we can start with vertex 6. Later on, vertex 5 is adjacent to 1, 4, 10, but 1, 10 have already appeared in the queue, so only 4 is added (see the boxes). At the end of the process, the list of vertices must be read in reverse order. In our case, we obtain v1 = 13, v2 = 9, . . . , v20 = 6. Now apply the greedy algorithm to this (reversed) list. By construction, each vertex in the list can be adjacent to at most k − 1 previous elements in the list, because it is also adjacent 5 VERTEX COLOURING 46 to one of the vertices above it in the table. For example, there is no problem colouring v10 because it is only adjacent to v4, and without checking we know it must be adjacent to some vj with j > 10 (it ﬁrst entered the queue when 10 entered the list). This scheme shows that we can colour G with at most k colours. In our case, we recover the colouring shown. If we restore the missing edge, this is not a valid colouring, though Brooks’ theorem implies that the dodecahedral graph is also 3-colourable and has χ = 3. list Q 6 v20 = 6 11 15 v19 = 11 15 7 16 15 7 16 10 20 7 16 10 20 2 12 16 10 20 2 12 17 10 20 2 12 17 5 14 20 2 12 17 5 14 19 2 12 17 5 14 19 1 3 12 17 5 14 19 1 3 8 17 5 14 19 1 3 8 18 v10 = 5 14 19 1 3 8 18 4 14 19 1 3 8 18 4 9 19 1 3 8 18 4 9 1 3 8 18 4 9 3 8 18 4 9 8 18 4 9 13 18 4 9 13 v3 = 4 9 13 v2 = 9 13 v1 = 13 This table emphasizes the dynamic nature of the process, and how the data might be stored on a computer. Each vertex is processed separately and its ‘new neighbours’ put in the queue using the First In First Out (FIFO) principle, which contrasts with that of a stack (FILO) we saw in recursive relations. However, there is a lot of redundant information with the diagonals. In practice, one can form a long queue, ticking oﬀ the vertices as they are processed and crossing out vertices on the graph as soon as they enter the queue (as the initial one or neighbours of the current vertex). Exercise 5.8. Retain the edge (1, 6) and re-do the table starting with vertex 6 again. You will probably ﬁnd that most vertex colours are the same but that at the ﬁnal stage one requires a fourth colour. This is not a contradiction: even though the dodecahedral graph has χ = ∆ = 3, it is important to remember that Brooks’ algorithm is only guaranteed to use at most ∆ colours when G is not regular. We have not proved Brooks’ theorem! 6 PLANARITY 47 6 Planarity 6.1 The Platonic graphs We can represent K4 the ‘complete graph on four vertices’ by the edges and vertices of a tetrahedron with transparent faces: Unlike a square with its two diagonals added, this has the advantage that there are no ‘false intersections’ of its edges. Deﬁnition 6.1. A graph is called planar if it can be drawn in the plane without crossings, so that edges intersect only in vertices. When it has been drawn that way we shall call it a plane drawing. The phrase ‘can be drawn’ means ‘is isomorphic to a graph’, so ‘planar’ is a property of an isomorphic class — if it is true for one graph, it is true for any isomorphic graph. Our problem then is to understand how to decide whether such a class is planar. We shall begin with four more regular planar graphs, namely the ones determined by the vertices and edges of the remaining platonic solids. A platonic solid is a convex polyhedron (formed by intersecting a number of planes in space) with congruent faces each of which is a regular polygon. It is known that such a face must be a triangle, square or pentagon. Let n denote the number of vertices e edges f faces p edges bounding each face ∆ = q edges joined at each vertex χ chromatic number Then the ﬁve Platonic solids and properties of the associated graphs are given by the table 6 PLANARITY 48 name n e f p q χ Eulerian? Hamiltonian? tetrahedron 4 6 4 3 3 4 no yes cube 8 12 6 4 3 2 no yes octahedron 6 12 8 3 4 3 yes yes dodecahedron 20 30 12 5 3 3 no yes icosahedron 12 30 20 3 5 4 no yes The Greek preﬁxes refer to the number of faces, for example dodeca means 2 + 10 = 12. The last four come in pairs, in their data we swap n ↔ f and p ↔ q . Here is the cube (or hexahedral) graph and the octahedral graph: Recall that the cube graph is bipartite. Its 23 vertices can be labelled by their Cartesian coordinates, which in the image are listed without commas: 000, 001, 010, 011, 100, 101, 110, 111. The octahedral graph is the only one of the ﬁve whose vertices all have even degree. The dodecahedral and icosahedral graphs are more complicated: To convert f into a number for a plane graph, we must count the outside as one face. Theorem 6.2 (Euler’s formula). For any connected plane graph drawing, n − e + f = 2. 6 PLANARITY 49 Proof. This is a remarkably universal formula. It is valid when there is just 1 isolated vertex (and so 1 outside face). We can proceed by induction on n. Each time we add an edge joined to the previous graph, either its other end is ‘free’ (the vertex has degree 1) or it joins up an existing vertex. In the former case, we have added 1 edge and 1 vertex, in the latter case 1 edge and 1 face. Either way n − e + f does not change. 6.2 Detecting non-planarity Recall that: • Kn is the complete graph with n vertices and so › n 2” = 1 2 n(n − 1) edges; • Km,n is the complete bipartite graph with m + n vertices and mn edges. In particular, K5 is the ‘starred pentagon’ and K3,3 is the ‘utility’ graph representing the distribution of broadband, electricity, water to three consumers. Note that the edges and vertices of a cube form a subgraph of K4,4 obtained by removing four edges. Proposition 6.3. Neither K5 nor K3,3 is planar. It follows that no planar graph can contain K5 or K3,3 as a subgraph. Proof. This can be done by ﬁrst principles (so no theory). Consider K5; suppose it has a plane drawing with no redundant crossings. Since the abstract graph has a 5-cycle, that (taken on its own) will determine a pentagon in the plane. The positions of the other ﬁve edges in the drawing remain to be speciﬁed, and each one must either lie inside or outside the pentagon. They cannot all lie outside without a crossing, so let us suppose one lies inside. There is no way to distinguish any of the 5 remaining edges, so we pick one and assume it lies inside. After one more choice, the inside/outside positions are forced upon us, and we are stuck at the ﬁnal step. A similar argument works for K3,3, though this has a 6-cycle. (Recall that any cycle in a bipartite graph must be even.) To formulate two important results, we need two new concepts for modifying graphs, namely homeomorphism and contraction. 6 PLANARITY 50 Deﬁnition 6.4. Two graphs G1, G2 are called homeomorphic if vertices of degree 2 can be added to one or both so that the resulting graphs are isomorphic. Roughly speaking, this means ‘adding blobs’ on one or more egdes so that the two graphs correspond. Adding a single vertex of degree 2 will split one edge into two, so adding several will generate more edges. To compare G1 and G2 , one could also ‘strip’ them of vertices of degree 2 one by one provided the result is still a graph. However, this stripping operation can lead to graphs that are not simple, so the notion of isomorphism is a little more complicated. If G1 and G2 are homeomorphic then their degree sequences can only diﬀer by their numbers of entries that are ‘2’. This is important when one is looking for subgraphs that are homeomorphic to a speciﬁc graph like K5 , which has vertex sequence (4, 4, 4, 4, 4). Example 6.5. Let G be a graph with degree sequence (3, 3, 3, 3, 6, 6). Draw one that is simple. Any graph homeomorphic to G must have degree sequence (2, . . . , 2, 3, 3, 3, 3, 6, 6), with zero or more ‘2’s. If there are no ‘2’s then the graph will be isomorphic to G, which is a special case of homeomorphism. Homeomorphism deﬁnes an equivalence relation on graphs in which one disregards vertices of degree 2. In particular, all cycle graphs Cn with n E 1 are homeomorphic! But a 1-cycle (a single vertex with a loop) cannot have its vertex removed, because the result is not a graph! Adding or removing vertices of degree 2 can be regarded as a ‘trivial’ operation that does not aﬀect the essence of the graph. It is a topological notion. What we are really doing is concentrating on the set of points formed by the edges only (this set forms a ‘topological space’), pretending they are made of stretchable wire. We are only interested in whether one set can be transformed into another by bending and stretching/compressing. Theorem 6.6 (Kuratowski, 1930). A graph is planar if and only if it does not contain a subgraph homeomorphic to K5 or K3,3 . Expressed another way, non-planar ⇐⇒ ∃ subgraph homeomorphic to K5 or K3,3 , though (as remarked above) the implication ⇐ is obvious. One can think of K5 and K3,3 as ‘germs’, one of which will always be present if the original graph is non-planar. Example 6.7. Recall the Petersen graph P, which has 10 vertices and 15 edges. One guesses correctly that it is not planar. It is a regular graph with vertex degree 3, so there is no hope of ﬁnding a K5 inside, but it does contain a subgraph homeomorphic to K3,3 . One needs to remove the two edges that are horizontal in the image below, leaving four vertices of degree 2. Note that the edges of a subgraph must be edges of P, so in order to talk of a subgraph we must leave the vertices in place, since they lie on other edges. But we can remove them and unite the edges so as to form a new graph homeomorphic to the subgraph: 6 PLANARITY 51 The new graph has 6 vertices and 9 edges, since we removed 2 edges, and another four pairs of edges became four single ones. It is easy to see that the 6 vertices are partitioned into two groups of 3, with all possible edges going from one group to the other, so we are dealing with K3,3 . This is all very well, but the operation we have performed is not very natural. Staring at P, it seems much closer to K5 than K3,3, and the next approach makes this precise. Deﬁnition 6.8. If G is a graph, and uv is an edge joining vertices u and v, then the graph obtained from G by contracting uv, written G~uv, is formed by making u and v coalesce so that any edges that arrived at either of them now arrive at the new common vertex. In this process, any loops are eliminated and any multiple edge just becomes a single one. A simple example would be a triangle with 3 vertices (i.e. a 3-cycle). If we contract any edge, we simply get a single edge with its two ends as vertices. Notice that the loop and extra edge are suppressed. If we contract an edge in a plane diagram, it remains a plane diagram. One can contract a number of edges by doing one at a time. Contraction is a rough analogue of taking a quotient in other branches of mathematics. Five contractions convert the Petersen graph P to the complete graph K5 . In its pentagonal representation above, we contract the 5 spokes by joining each outer vertex to its nearest neighbour inside. This is a painless operation that does not even produce multiple edges to combine. 6 PLANARITY 52 Theorem 6.9 (Wagner, 1937). A graph is planar if and only if it does not contain a subgraph that can be contracted to (a graph isomorphic to) K5 or K3,3 . Expressed another way, non-planar ⇐⇒ ∃ subgraph contractible to K5 or K3,3 . But this time, neither of the implications is elementary. The forward direction (⇒) follows immediately from Kuratowski’s theorem, because any subgraph homeomorphic to K5 (resp. K3,3 ) can itself be contracted to K5 (resp. K3,3 ) by contracting edges so as to delete the superﬂuous vertices of degree 2. But there are complications the other way – if G is contractible to K5 then it might not contain a subgraph homeomorphic to K5, but one homeomorphic to K3,3 instead. Exercise 6.10. Identify three edges of P whose contraction yields K3,3 . It might help to use the following representation of K3,3 : 6.3 Further results Recall Euler’s formula for a plane graph drawing: n − e + f = 2. We gave an informal proof in §5.3 by showing that the left-hand side does not change when the graph is ‘grown’ by adding one edge at a time. A more rigorous proof can be given by induction on the number e of edges. We shall illustrate this for the special case of trees. Recall that a tree is a connected graph with no cycles. Proposition 6.11. If G is a tree then e = n − 1. Proof. If e = 1 then n = 2, the result is valid. Assume the result is true for e = N − 1. Let G be a graph with N edges. Now any edge uv of a tree is a bridge – its removal disconnects the graph (because if not, there must be a path from u to v which becomes a cycle with uv ). So if an edge is removed we obtain a graph with exactly two components (no more than two, because a single edge cannot connect three components). Both of the components must be trees, by deﬁnition. By assumption, e = 1 + e1 + e2 = 1 + (n1 − 1) + (n2 − 1) = n − 1, as stated. 6 PLANARITY 53 Remark 6.12. (i) We can use a similar induction argument to prove that any tree has a plane diagram (i.e. without crossings) with just an outside face. Thus f = 1, and the proposition is compatible with Euler’s formula. (ii) For a ﬁxed number of vertices n, no connected graph can have less than e − 1 edges (exercise). Using this one can show that if G is connected then e = n − 1 if and only if G is a tree. Proposition 6.13. If G is a simple planar graph with e E 3 then e D 3n − 6. Proof. Represent G by a plane diagram. Each face (even if there is only one) borders at least 3 edges. Each such edge will be counted twice if it has diﬀerent faces either side of it, otherwise counted once. Thus 2e E 3f = 3(2 + e − n), which gives the result. Lemma 6.14. Any simple planar graph has a vertex of degree at most 5, and is 6-colourable. Proof. Recall that the sum of the vertex degrees equals 2e. If all the degrees are at least 6, then 6n D Q v∈V d(v) = 2e D 6n − 12, a contradiction. We prove that χ D 6 by induction on the number of vertices, n. Obviously χ D n, so the result is true when n D 6. Suppose it is true for n = N − 1. If G now has N vertices, remove one (call it v ) of degree at most 5 and its associated edges (at most 5 of them). By assumption, the remaining graph is 6-colourable. Moreover, v only had 5 neighbouring vertices, so when we replace v and its edges we still have a 6th colour left for v . The lemma is analogous to saying that χ D ∆ + 1, and it is not too hard to reﬁne the proof to conclude that χ D 5. This is eﬀectively the ﬁve colour theorem, whose equivalent statement for maps was proved by Heawood in 1890. The four colour theorem was not resolved until almost a century later, but not without a ‘proof’ that involved substantial computer veriﬁcation of special cases: Theorem 6.15 (Appel-Haken, 1976). Any simple plane graph is 4-colourable, i.e. χ D 4. 6.4 The four-colour theorem* The last theorem is more familiar as a statement about maps – only four colours are needed in such a way that contiguous countries are distinguished. Here is the idea. A map can be deﬁned as a plane graph drawing for which removing one or two edges will not disconnect the graph. (This excludes vertices of degree 2 and an outside face reaching both sides of an edge.) Given a map M, we can form its ‘dual’, which is a plane graph diagram denoted M ∗, which has a vertex for each face of M and an edge joining two vertices whenever the corresponding faces are contiguous (and this edge crosses only that 6 PLANARITY 54 common border). Because of our assumptions, M ∗ will have no loops or multiple edges. Then a vertex colouring of M ∗ corresponds to a valid colouring of the map, so the theorem implies the map colouring result. The concept of duality is well known in the context of polyhedra – as we remarked, the cube (6 faces, 8 vertices) and octahedron (8 faces, 6 vertices) are dual pairs, as are the dodecahedron (12 faces, 20 vertices) and icosahedron (20 faces, 12 vertices). The dual of a tetrehedron is another tetrahedron (the graph being K4 with 4 faces, 4 vertices). We conclude with some comments that link this topic to Geometry of Surfaces. Planar graph diagrams can be regarded as graphs on the surface of a sphere in which one point of the sphere (say the north pole p) corresponds to inﬁnity in the plane. The outside face in the plane becomes a normal face containing p on the sphere, so this is more natural. One can show that both K5 and K3,3 can be drawn on the torus without artiﬁcial crossings. One can also try to draw graphs on more complicated surfaces, in particular on a surface of genus g, which means a ‘torus with g holes’. Provided there are no crossings, it is well known that if f now counts ‘faces’ on the surface, then n − e + f = 2 − 2g, this quantity being the so-called Euler characteristic of the surface. A graph that can be drawn on a surface of genus g but not on one of genus g − 1 is called a graph of genus g . Thus K4 is a graph of genus 0, and K5 and K3,3 are graphs of genus 1. It is also known that a map drawn on a surface of genus g can always be coloured with a maximum of k colours, where k = 7 + »1 + 48g 2 \u000f. Taking g = 0 gives the four colour theorem as a special case. Taking g = 1 shows that 7 colours suﬃce to colour the vertices of a graph inscribed on a doughnut. 7 NAVIGATION IN GRAPHS 55 7 Navigation in graphs 7.1 Adjacency data There are two common ways of representing graphs non-pictorially – by means of an adjacency matrix or an adjacency table. We consider these in turn. Without assuming that it is simple, Deﬁnition 7.1. Let G = (V, E) be a graph G, where V = {v1, . . . , vn} is the set of vertices, and E the set of edges. Let us label the vertices v1 . . . , vn . The adjacency matrix of the graph G is the matrix A = (aij)1Di,jDn , where aij is the number of edges joining vi to vj . The adjacency matrix A of a graph G = (V, E), with SV S = n is an n × n symmetric matrix. One can reconstruct G from A. Example 7.2. A = ™ Œ Œ Œ ﬂ 0 1 0 1 1 0 1 2 0 1 0 1 1 2 1 0 ﬁ Š Š Š Ł We can work out the degree of any vertex by taking the sum of the entries in the corresponding row (or column, since A is symmetric). This gives the following lemma. Lemma 7.3. Let G = (V, E) be a graph G, where V = {v1, . . . , vn} is the set of vertices, and E the set of edges. Let A be the adjacency matrix of G. Then, we have d(vi) = 2aii + Q 1DjDn i≠j aij. (2) The graph is simple if and only if aii = 0 for all i, and every other entry is 0 or 1. Example 7.4. Returning to Example 7.2, we see that d(v1) = 2 d(v2) = 4 d(v3) = 2 d(v4) = 4. Remark 7.5. Our convention in Deﬁnition 7.1 is that a loop around a vertex is counted once, thus our formula (2) for the degree of a vertex. Another common approach is to assume that a loop is counted twice. In that case, the formula for the degree of a vertex would be d(vi) = Q 1DjDn aij. (3) 7 NAVIGATION IN GRAPHS 56 Deﬁnition 7.6. Let G = (V, E) be a graph G, where V = {v1, . . . , vn} is the set of vertices, and E the set of edges. Let A be the adjacency matrix of G. Then, the Laplacian (or Riskoﬀ matrix) of G is the matrix L given by L = D − A, where D is the diagonal matrix D = ™ Œ Œ Œ ﬂ d(v1) 0 . . . 0 0 d(v2) 0 ⋮ \b ⋮ 0 . . . 0 d(vn) ﬁ Š Š Š Ł Many properties of the graph G can be determined by studying its adjacency matrix. For example, we have the following theorem. Theorem 7.7. Let G = (V, E) be a graph G, where V = {v1, . . . , vn} is the set of vertices, and E the set of edges. Let A be the adjacency matrix of G, and L its Laplacian. Let c(G) be the number of connected components of G. Then, we have c(G) = n − rank(L) = dim(ker(L)). Example 7.8. Return to Example 7.4, we obtain that the Laplacian of the graph is L = ™ Œ Œ Œ ﬂ 2 −1 0 −1 −1 4 −1 −2 0 −1 2 −1 −1 −2 −1 4 ﬁ Š Š Š Ł The echelon form of this matrix is given by L ′ = ™ Œ Œ Œ ﬂ 1 0 0 −1 0 1 0 −1 0 0 1 −1 0 0 0 0 ﬁ Š Š Š Ł . From this, we see that rank(L) = rank(L ′) = 3. Hence, c(G) = 4 − 3 = 1. This is unsurprising since by inspection we can see that G is indeed connected. Two advantages of the matrix approach are that it can be generalized: • to deal with digraphs by relaxing the condition that aij = aji, so aij = 1 means that i → j is a directed edge (with a loop now counting 1), see the next example; • to deal with simple weighted graphs or digraphs, in which each edge is assigned a non-negative number. The lower triangular part of the matrix then resembles a table of distances between cities of the type that used to be common in motoring atlases, except that only adjacent nodes have non-zero entries. Example 7.9. Below is the sparse adjacency matrix of the disconnected digraph that represents squaring modulo 13, with vertices representing the 12 non-zero residue classes: 7 NAVIGATION IN GRAPHS 57 The smaller component consists of the residue classes 1 (with the loop), 12 = −1, 5 and 8 = −5 (the last two square to −1 mod 13). If we re-label the vertices so that 1, 5, 21, 26 come at the start then the matrix will have a block form, making it obvious that there are (at least) two components. Another method of representing an ordinary graph is by its adjacency table. Deﬁnition 7.10. Let G = (V, E) be a graph G, where V is the set of vertices, and E the set of edges. The adjacency table T of G is the table T = \u0004 v Tv \t v∈V , where Tv is the set of vertices adjacent to v . We will also work with the transpose of this table when it is more convenient: \u0001 v Tv \u0006 v∈V , Example 7.11. The adjacency table of the graph G in Example 7.2 is given by 1 2 3 4 2 1 2 1 4 3 4 2 4 3 In row representation, we can write this as: 1 ∶ [2, 4] 2 ∶ [1, 3, 4] 3 ∶ [2, 4] 4 ∶ [1, 2, 3] (This is how the adjacency table is given in Sage.) One can easily adapt the matrix methods, for example to detect the existence of cycles 1 → 2 → → 4 → 1, 1 → 2 → 3 → 4 → 1 by passing from column to column. 7 NAVIGATION IN GRAPHS 58 7.2 Search trees We discuss two diﬀerent methods of searching through the vertices of graphs. These are • Depth ﬁrst search (DFS): In a depth ﬁrst search, one follows paths as far as possible: From a vertex v already reached, we proceed to any vertex w adjacent to v which was not yet visited; then we go on directly from w to another vertex not yet reached etc., as long as this is possible. (If we cannot go on, we backtrack just as much as necessary.) In this way, one constructs maximal paths starting at some initial vertex s. • Breadth ﬁrst seach (BFS): In a breadth ﬁrst search, vertices which have larger distance to the starting vertex s are examined later than those with smaller distance to s. Let G = (V, E) be a graph. We deﬁne the following functions: (a) For each w ∈ V , p(w) denotes the vertex from which w has been accessed. (b) For each w ∈ V , nr(w) denotes the numbering of w in the order in which it has been reached. (c) For each e ∈ E , u(e) = False if the edge e has not been used to connect two vertices that have already been visited, and u(e) = True otherwise. Algorithm 7.12 (Depth First Search (DFS)). Let G = (V, E) be a graph, and s ∈ V a vertex. (1) For each v ∈ V , initialize p(v) = 0 and nr(v) = 0; (2) For each e ∈ E , initialize u(e) = False; (3) Set i ← 1; v ← s; nr(v) ← 1; //The first vertex visited is s. (4) Choose a vertex w ∈ Tv , and set u(e) = True, where e = {v, w}: (a) If nr(w) = 0, then set p(w) ← v; i ← i + 1; nr(w) ← i; v ← w. (b) If nr(w) ≠ 0, then return to Step (4); (5) Set v ← p(v), and return to Step (4). The DFS algorithm can be thought of as a walk in a maze. We illustrate this with an ucoming example. Theorem 7.13. Let G = (V, E) be a graph. (a) For s ∈ V , each edge in the connected component of s is used exactly once in each direction during the execution of Algorithm 7.12. (b) If G is connected, then Algorithm 7.12 has complexity O(SES). 7 NAVIGATION IN GRAPHS 59 Example 7.14. Let us consider the following graph. a d h s c f b e g We perform a DFS search beginning at s. To make the algorithm deterministic, we visit the vertices in Step (4) according to their alphabetical order. This means that the vertices are reached in the following order: s, a, b, c, d, e, f, g . After that, the algorithm backtracks from g to f , to e, and then to d. Now h is reached and the algorithm backtracks to d, c, b, a, and ﬁnally to s. The directed tree T constructed by the DFS is given below: a d h s c f b e g Example 7.15. We can convert a maze (left) into a graph (right) by placing a vertex at each position where a choice is needed (including the start and end) and at a dead-end. A walk in this graph is a walk in the maze. This graph can be input into a computer by typing its adjacency table. In SAGE, this is 7 NAVIGATION IN GRAPHS 60 T = { 1: [4,6], 2: [3], 3: [2,7,9], 4: [1], 5: [6], 6: [1,5,10], 7: [3,12], 8: [12], 9: [1,3,14], 10: [6,15], 11: [12], 12: [7,8,11], 13: [14], 14: [9,13,15], 15: [10,14,16], 16: [15] } We create the graph using Sage using the following script: T = { 8: [12], 11: [12], 12: [7,8,11], 1: [4,6], 2: [3], 3: [2,7,9], 4: [1], 5: [6], 6: [1,5,10], 10: [6,15], 7: [3,12], 9: [1,3,14], 13: [14], 14: [9,13,15], 15: [10,14,16], 16: [15] }; G = Graph(T); H = G.plot(vertex_colors=’white’); H.show(figsize=[4, 6], dpi=700) This gives the graph We perform a DFS search starting at s = 7. In Step (4), we visit the vertices in numerical order. We proceed in the following steps: 1. We visit the vertices 3, 2 in that order; then we backtrack to 3. 2. We visit the vertices 3, 9, 1 and 4. Then, we backtrack to 1, then we go to 6 and 5. 7 NAVIGATION IN GRAPHS 61 7 3 2 9 1 4 6 5 10 15 14 13 16 12 8 11 Figure 1: DFS search tree for Example 7.15 3. We visit the vertices 10, 15, 14 and 13. (Note, from 14 we cannot visit 9, otherwise we will have a loop, which is not permitted.) Then, we backtrack to 14, then to 15. 4. We visit the vertex 16. Then, we backtrack to 15, then to 10, 6, 1, 9, 3 and 7. 5. We visit 12 and 8. Then, we backtrack to 12. 6. We visit 11, and we backtrack to 12 and 7. This gives the search tree in Figure 1. One way to carry out Algorithm 7.12 is by processing the data from the adjacency table in a stack. At each stage, the vertex being processed is the one on the right. This represents the ‘top’ of the stack, which we are allowed to ‘peek’. We seek to add or ‘push’ an adjacent vertex to the stack if there exists an adjacent vertex that has not already been processed. We implement the DFS stack approach for Example 7.15 in Table 1. We add at most one adjacent vertex to the stack (for deﬁniteness, the smallest in our list), and we keep a separate record of those vertices that have (at some point) entered the stack. If the vertex being processed has no new neighbours (either because it has degree one, or because its neighbours are in the stack), we remove or ‘pop‘ it from the stack. Each vertex can appear at most once in our stack, and each vertex appears twice in our table, once added, once removed. This means that the table must contain 2n rows, where n is the number of vertices (here n = 16). DFS abbreviated version. Use of a table is advised to gain conﬁdence in the method, but for some purposes it suﬃces to list the vertices in the order in which they are encountered, and add ‘ties’ joining neighbouring vertices that are not already adjacent in the list: ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ 7, ³¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹µ 3, 2, 9, ³¹¹·¹¹µ 1, 4, ³¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹µ 6, 5, 10, ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ 15, 14, 13, 16, ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ 12, 8, 11 . 7 NAVIGATION IN GRAPHS 62 DFS stack ← → added removed 7 7 7, 3 3 7, 3, 2 2 7, 3 2 7, 3, 9 9 7, 3, 9, 1 1 7, 3, 9, 1, 4 4 7, 3, 9, 1 4 7, 3, 9, 1, 6 6 7, 3, 9, 1, 6, 5 5 7, 3, 9, 1, 6 5 7, 3, 9, 1, 6, 10 10 7, 3, 9, 1, 6, 10, 15 15 7, 3, 9, 1, 6, 10, 15, 14 14 7, 3, 9, 1, 6, 10, 15, 14, 13 13 7, 3, 9, 1, 6, 10, 15, 14 13 7, 3, 9, 1, 6, 10, 15 14 7, 3, 9, 1, 6, 10, 15, 16 16 7, 3, 9, 1, 6, 10, 15 16 7, 3, 9, 1, 6, 10 15 7, 3, 9, 1, 6 10 7, 3, 9, 1 6 7, 3, 9 1 7, 3 9 7 3 7, 12 12 7, 12, 8 8 7, 12 8 7, 12, 11 11 7, 12 11 7 12 g 7 Table 1: DFS stack for Example 7.15 7 NAVIGATION IN GRAPHS 63 7 3 2 9 1 4 6 5 10 14 13 15 16 12 8 11 Figure 2: BFS search tree for Example 7.15 It is a consequence of the methods that all such ties are ‘nested’, with no crossings. Breadth ﬁrst search. We recall that a queue is a data structure in which items are added on the right, and removed from the left. Algorithm 7.16 (Breadth First Search (BFS)). Let G = (V, E) be a graph, and s a vertex. (1) Initialise the empty queue Q = g; (2) Set d(s) ← 0; (3) Append s to Q; (4) Remove the ﬁrst vertex v from Q; (5) For each vertex w ∈ Tv , do: If d(v) is undeﬁned, then set d(w) ← d(v) + 1; Append w to Q. (6) Return to Step (4). Theorem 7.17. Algorithm 7.16 has complexity O(SES). At the end of the algorithm, every vertex t of G satisﬁes d(s, t) = œ d(t), if d(t) is deﬁned; ∞, otherwise. Corollary 7.18. Let s be a vertex of a graph G. Then G is connected if and only if d(t) is deﬁned for each vertex t at the end of BFS search starting at s. Example 7.19. Let us consider the graph G in Example 7.14. We a BFS search on this graph starting at the vertex s. To make the algorithm deterministic, we select the vertices in 7 NAVIGATION IN GRAPHS 64 alphabetical order in Step (5) of the BFS. To make things clearer, we have drawn the vertices in levels according to their distance to s; also, we have omitted all edges leading to vertices already labelled. Thus all we see of |G| is a spanning tree, that is, a spanning subgraph of G which is a tree. This kind of tree will be studied more closely in Chap. 4. 0 1 2 3 a d h s c f b e g This is carried out by inserting data into a queue. When processing a vertex in a BFS, it is important to add all its adjacent vertices before moving on. This is exactly what we did in the 2-part algorithm to re-order vertices prior to colouring them. Although our table should show each addition and removal step by step, we can save time by using one row for each vertex being processed (having just been removed). This way, we only have n rows, excluding the ﬁrst: removed ← BFS queue ← 7 7 3, 12 3 12, 2, 9 12 2, 9, 8, 11 2 9, 8, 11 9 8, 11, 1, 14 8 11, 1, 14 11 1, 14 1 14, 4, 6 14 4, 6, 13, 15 4 6, 13, 15 6 13, 15, 5, 10 13 15, 5, 10 15 5, 10, 16 5 10, 16 10 16 16 g BFS abbreviated version. As in the 2-part colouring algorithm, some of the information can be presented by a single long list: √ √ √ √ √ √ √ √ √ vertices: 7 3 12 2 9 8 11 1 14 4 6 13 15 5 10 16 level: 0 1 1 2 2 2 2 3 3 4 4 4 4 5 5 5 7 NAVIGATION IN GRAPHS 65 Here we have processed up to and including vertex 14, in the latter case by adding 13, 15. It is a consequence of the method that vertices are added level by level, and the last row indicates their ‘distance’ to the start. Deﬁnition 7.20. Given a connected graph G, a spanning tree is a subgraph of G without cycles that includes all the vertices of G. Both searches determine such a spanning tree, although the way they do this reﬂects their names. The trees are rooted, because we have distinguished a starting point – vertex 7 is the root. We can now re-draw the tree growing (by convention) downwards, level by level. The ‘dead-end’ vertices 2, 4, 5, 8, 11, 13, 16 all deﬁne leaves of the trees, but the BFS tree (right) happens to have an extra leaf at the ﬁnish. In our example, G was not itself a tree, having a cycle (9, 1, 6, 10, 15, 14, 9) and the two trees break this in diﬀerent ways. The DFS tree omits the edge (14, 9) whereas the BFS one omits (10, 15). The DFS tree (left) has height 8, this being the maximum number of edges from root to leaf, achieved by arriving at the dead-end 13. By contrast, the BFS tree is more spread out and has height 5. 7.3 Shortest paths Deﬁnition 7.21. A weighted graph is a simple graph G = G(V, E) together with a function d ∶ E Ð→ (0, ∞) called weight function or map, which take positive integer values. (We use d for “distance” because w could be confused with a vertex.) Example 7.22. Let G = (V, E) be a simple graph. We can turn G into a weighted graph by setting d(e) = 1 for all e ∈ E . Ð→ Example 7.23. In the graph below, the vertices can be thought of as representing cities. The weight can be thought of as the time spent on the train between each two cities connected by an edge. The weight can also represent the distance in km. 7 NAVIGATION IN GRAPHS 66 Deﬁnition 7.24. Let G = (V, E) be a weighted graph. Let W = e1 @ . . . @ es be a walk (or path) in G. The length of the W is given by d(W ) = s Q i=1 d(ei). If u, v are vertices, the distance between u and v is deﬁned by by d(u, v) ∶= œ min{d(W ) ∶ W ∈ Paths(u, v)}, if v is accessible from u ∞ otherwise. We note that d(u, u) = 0. Applications: Weighted graphs have many important applications. For example, here are two such applications. 1. Train scheduling: An example consists of points on a transport network with distances or travel times between nodes. The TfL map is arguably less practical – it displays walking times between stations – but is a good illustration. 2. Projects scheduling: If we want to carry out a complex project – such as, for example, building a dam, a shopping centre or an airplane – the various tasks ought to be well coordinated to avoid loss of time and money. This kind of tools are used in operations research. In such applications, one would like to ﬁnd the shortest distance between two vertices. In practice, this could represent the shortest distance or time between to cities. The smallest cost of a project, etc. The aim of this section is to present and justify Dijkstra’s algorithm for ﬁnding shortest paths from some ﬁxed root vertex to all the others in a weighted graph. The BFS algorithm is a special case of the Dijkstra’s algorithm; in that case, one simply sets the weight of each edge to be d(e) = 1 (see Example 7.22). If G = (V, E) is a weighted graph, and u, v ∈ V , we will denote the shortest path between u and v by SP(u, v). The following is known as the Dijkstra algorithm. It is one of the most important algorithm is graph theory. 7 NAVIGATION IN GRAPHS 67 Algorithm 7.25 (Dijkstra’s algorithm). This takes as input: a weighted graph G(V, E) and a distinguished vertex v0 . Let V = {v0, v1, . . . , vn}. Its output consists of a list of the shortest distances Li = SD(v0, vi) for each vertex vi . Obviously L0 = 0. (1) Initialize Vtemp ← V , and set L0 = 0; (2) For j > 0, intialize Lj ← ∞; (3) while Vtemp ≠ g: Choose vi ∈ Vtemp with Li minimal, and set Vtemp ← Vtemp \u0013 {vi}. (4) For vj ∈ Vtemp adjacent to vi , set Lj ← min(Li + d(vi vj), Lj); (5) Return (L0, L1, . . . , Ln); Example 7.26. Let’s return to Example 7.23. We would like to ﬁnd the shortest distance between B and M , i.e. SP(B, M ). • Initial values: LB = 0, Lv = ∞, for v ∈ {K, M, O, R, T, U }, Vtemp = V ; • Iteration k = 1: v = T , Vtemp = {K, M, O, R, U }, LT = 2; LO = 4 and LR = 5; • Iteration k = 2: v = O , Vtemp = {K, M, R, U }, LO = 4; LR = 5 and LK = 7; • Iteration k = 3: v = R, Vtemp = {K, M, U }, LR = 5; LK = 7, LM = 11 and LU = 6; • Iteration k = 4: v = U , Vtemp = {K, M }, LU = 6; LM = 11; • Iteration k = 5: v = M , Vtemp = {K}, LM = 11; LK = 7; • Iteration k = 6: v = K , Vtemp = g, LK = 7; So the algorithm returns the vector (0, 7, 11, 4, 5, 2, 6). 8 OPTIMALITY 68 8 Optimality 8.1 Shortest paths The aim of this section is to prove that Dijkstra’s algorithm is ‘correct’. The next result is phrased using the notation of Subsection 7.3. Lemma 8.1. Suppose that a shortest path v0 € u € vi between two vertices v0, vi in a weighted graph passes via an intermediate vertex u. Then both subpaths v0 € u and u € vi are shortest paths between their respective vertices, and in particular SD(v0, vi) = SD(v0, u) + SD(u, vi). Proof. If one of the ‘subpaths’ is not shortest, then we could substitute it with a shorter one, giving a shorter path v0 € vi . It’s that simple! For the lemma, we are assuming that the path between v0 and vi is a shortest one. If this were not the case, we only have the triangle inequality SD(v0, vi) D SD(v0, u) + SD(u, vi). The lemma is an instance of an important argument called ‘Bellman’s optimality principle’ that crops up in diﬀerent guises in many problems in optimization theory. Let us return to Dijkstra’s algorithm. We wish to prove that all the permanent labels of vertices in Vperm are correct shortest distances: Theorem 8.2. Algorithm 7.25 determines with complexity O(SV S 2) the distances with respect to some vertex s in (G, d). More precisely, at the end of the algorithm SD(v0, vi) = Li, for all i = 0, . . . , n. Proof. We shall prove this by induction on i. The statement is certainly true when i = 0 since SD(v0, v0) = L0 = 0. Let i > 0, and assume that SD(v0, vk) = Lk for all 0 D k D i − 1. Now let vi ∈ Vtemp denote the temporary vertex chosen at a later stage because its label Li is minimal. We want to show that Li = SD(v0, vi). Suppose that v0 € vk → vm € vi is a shortest path from v0 to vi . Here we have chosen intermediate and adjacent vertices vk , with k < i and vm ∈ Vtemp ; this is clearly possible and it may be that m = i. Our inductive hypothesis is that SD(v0, vj) = Lj for all 0 D j D i − 1, so in particular Lk = SD(v0, vk). Then, 8 OPTIMALITY 69 we have Li D Lm, by minimality D Lk + d(vk, vm), by deﬁnition of Lm when vk scanned vm D Lk + SD(vk, vi), since the edge is part of the SP = SD(v0, vk) + SD(vk, vi), by hypothesis = SD(v0, vi), by the previous lemma. But Li is the distance to vi via some path, so it must equal SD(v0, vi), and (incidentally) all the inequalities are equalities. This completes the induction. Remark 8.3. The crucial technique in Dijkstra’s algorithm is the act of relabelling: once vi becomes permanent we scan its adjacent vertices and reduce their labels if passing through vi gives a shorter path: Lj = min(Li + d(vi, vj), Lj). The act of relabelling is called relaxation of the edge vivj, and its repeated use allows one to decrease the estimated shortest distances until they become correct. Dijkstra’s algorithm has the characteristic that it grows a tree of shortest paths from the root. There are other shortest path algorithms that apply relaxations in a more brute force (and thus, simpler) way, whilst still eventually achieving a shortest path. 8.2 Kruskal’s algorithm In this section, G is always a simple connected weighted graph. Let n denote the number of its vertices. We have seen a number of algorithms that, when applied to G, construct a spanning tree. This is a subgraph with the same vertex set as G, and since it is a tree, it will have n − 1 edges. In particular, Dijkstra’s algorithm does this, provided we give it a starting vertex to act as root. A diﬀerent connectivity problem concerns the construction of a minimal spanning tree or MST. Deﬁnition 8.4. Let (G, d) be a weighted graph. For any subset E′ of the set of edges E , we deﬁned the total weight of E′ to be d(E′) = Q e∈E′ d(e). Deﬁnition 8.5. Let (G, d) be a weighted simply connected graph. A minimal spanning tree or MST for G is a tree T whose total degree is minimal among those of all spanning trees. Any connected graph has a spanning tree, because whenever there is a cycle one can remove any edge in that cycle, leaving all the vertices connected, and continue until there are no more cycles. Therefore a minimal spanning tree must exist, and (if there is more than one) the total weight of any two are equal by deﬁnition. Algorithm 8.6 (Kruskal’s algorithm). Let G = (V, E) be a weighted simple connected graph, with SV S = n. We order the edges of G according to their weight, so that E = {e1, . . . , em} with d(e1) D . . . D d(em). 8 OPTIMALITY 70 (1) Initialize T ← g; (2) For k = 1, . . . , m, do: if ek does not form a cycle together with some edges of T , then append ek to T ; (3) Return to Step (2); Example 8.7. Let us consider the graph below. We want to ﬁnd a spanning tree. 1 2 5 6 4 7 3 8 28 1 2 8 10 9 26 5 8 24 1 87 7 27 We label the edges ﬁrst according to their weights, then to the numbering of their vertices. This gives the list: e1 = {1, 5}, e2 = {6, 8}, e3 = {1, 3}, e4 = {4, 5}, e5 = {4, 8}, e6 = {7, 8}, e7 = {2, 5}, e8 = {4, 7}, e9 = {6, 7}, e10 = {2, 4}, e11 = {2, 6}, e12 = {3, 6}, e13 = {5, 6}, e14 = {3, 8}, e15 = {1, 2}. Now, we apply the Kruskal algorithm (Algorithm 8.6): Iteration k T 1 {e1} 2 {e1, e2} 3 {e1, e2, e3} 4 {e1, e2, e3} 5 {e1, e2, e3, e4} 6 {e1, e2, e3, e4, e5} 7 {e1, e2, e3, e4, e5, e6} 8 {e1, e2, e3, e4, e5, e6, e7} (After the 8th iteration, every additional edge creates a cycle) 8 OPTIMALITY 71 1 2 5 6 4 7 3 8 We see that T is indeed a spanning tree for G. By Theorem 8.9, it is a minimal one. Example 8.8. Applying Dijkstra’s algorithm with root bottom left in the following graph gives a tree of weight 41. Applying Kruskal’s algorithm gives a MST with weight 34. The vertices are drawn with large circles to allow one to record the labels (temporary and permanent) in applying Dijkstra’s algorithm 8 OPTIMALITY 72 Kruskal’s is a prototype ‘greedy algorithm’ since it executes what seems to be the optimal choice at each step. One could imagine (for example) that at each stage one should only add edges that are connected to ones already chosen, so that the MST is ‘grown’ branch by branch. (It turns out that this procedure is also valid – it is called Prim’s algorithm and works well when the graph is deﬁned by an adjacency matrix.) In any case, it is far from obvious that Kruskal’s procedure works, but this is what the next result assures us: Theorem 8.9. Algorithm 8.6 outputs a spanning tree T that is a minimal spanning tree. We need some lemmas before we can prove this theorem. Lemma 8.10. Let G be a graph. Then, the following are equivalent: (a) G is a tree; (b) G does not have any cycles, but adding any further edge yields a unique cycle; (c) Any two vertices of G are connected by a unique path. (d) G is connected, and every edge of G is a bridge. Proof. Give proof for ﬁrst 3 items. Remark 8.11. Let G = (V, E) be a graph and T ⊂ G a tree. Let e be an edge of G not contained in T . Then, by Lemma 8.10, the graph T ∪ {e} contains a unique cycle. We will denote this cycle by CT (e). Proof of Theorem 8.9. Note that at each intermediate stage, F is a ‘forest’ consisting of one or more trees, and SF S < n. Since ST S = n − 1 it can have only one connected component, and must include all n vertices. We need to prove that its total weight is minimal amongst all spanning trees. Let T ′ be a minimal spanning tree (one certainly exists!) such that T ′ ∩ T is as large as possible. Let e be an edge in T \u0013T ′ of least weight. Let CT ′(e) be the unique cycle given by Lemma 8.10 and Remark 8.11. The cycle CT ′(e) must have an edge e′ ∈ T ′ \u0013 T . Otherwise, we would 8 OPTIMALITY 73 have CT ′(e) ⊂ T . Again, by Lemma 8.10 and and Remark 8.11, there is unique cycle CT (e′) contained in T ∪{e′}. The same argument as above implies that CT (e ′) has an edge e′′ ∈ T \u0013T ′ . Assume that e = ej , e′ = ek and e′′ = el , with 1 D j, k, l D m. By assumption on e, we have d(e) D d(e ′′), hence j D l . We claim that j D k , meaning that d(e) D d(e′). Otherwise, we would have k < j , and d(e′) < d(e). So e′ would have been added to F ⊆ T before e: At the stage the algorithm is applied to any edges of weight d(e ′), the edge e′′ would not have been part of F since d(e′) < d(e) D d(e′′), nor could adding e′ have created a cycle in F , because in that case there would already be a path from e ′ + to e′ − in T preventing the later addition of e′′ . So e ′ would have been added to F ⊆ T . But this is a contradiction. Finally, consider (T ′ \u0013 {e′}) @ {e}; this is an MST with a larger intersection with T , which is a contradiction. 8.3 Back to matrices The aim of this section is to link our study of spanning trees to some matrix algebra. We’ll be using the preﬁx ‘ADJ’ for both the ‘ADJacency’ matrix and the ‘ADJugate’ (sometimes called ‘ADJoint’) matrix. Revision on matrix algebra. Let A = (aij) be a square n × n matrix, and recall the notion of cofactor, used in the computation of inverses. Namely, deﬁne cij = (−1)i+j(sub-determinant formed from A by deleting row i and col j ). The transpose CŠ is the so-called adjugate matrix ̃A = adj A: ̃Aij = cji, and A ̃A = (det A)I = ̃AA. Of course, A = „a b c d‚ Ô⇒ adj A = „ d −b −c a ‚ . If A is invertible then A−1 = 1 det A ̃A. However, ̃A can still be useful when A is not invertible; here’s an enticing example: A = ™ Œ ﬂ 1 2 3 4 5 6 7 8 9 ﬁ Š Ł Ô⇒ ̃A = −3 ™ Œ ﬂ 1 −2 1 −2 4 −2 1 −2 1 ﬁ Š Ł . Let G be a simple graph with n vertices. Consider the following n × n matrices: A = the adjacency matrix of G D = the diagonal matrix of vertex degrees L = D − A. 8 OPTIMALITY 74 L is called the Laplacian matrix of th graph G. It is obviously symmetric, and all its rows (or columns) add up to zero. In fact, the rank of L equals n minus the number of components of G. Example 8.12. Consider this graph with 4 vertices and 5 edges: A = ™ Œ Œ Œ ﬂ 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 ﬁ Š Š Š Ł Ô⇒ L = ™ Œ Œ Œ ﬂ 2 −1 0 −1 −1 3 −1 −1 0 −1 2 −1 −1 −1 −1 3 ﬁ Š Š Š Ł . Exercise 8.13. Compute A2 and check that its (i, j)th entry is the number of walks of length 2 from vertex i to vertex j . Explain why, more generally, (An)ij is the number of walks of length n from i to j . 8.4 The graph Laplacian* Now we turn attention to the matrix L. Consider the characteristic polynomial det(L − xI) = (λ1 − x)(λ2 − x)\u0005(λ4 − x). Since L is not invertible, at least one eigenvalue must vanish, say λ4 = 0. Then det(L − xI) = x 4 − 10x 3 + (λ1λ2 + λ2λ3 + λ3λ1)x2 − λ1λ2λ3x. One the other hand, this equals det ™ Œ Œ Œ ﬂ 2 − x −1 0 −1 −1 3 − x −1 −1 0 −1 2 − x −1 −1 −1 −1 3 − x ﬁ Š Š Š Ł = det ™ Œ Œ Œ ﬂ 2 − x −1 0 −1 −1 3 − x −1 −1 0 −1 2 − x −1 −x −x −x −x ﬁ Š Š Š Ł = det ™ Œ Œ Œ ﬂ 2 − x −1 0 −x −1 3 − x −1 −x 0 −1 2 − x −x −x −x −x −4x ﬁ Š Š Š Ł = −4x c44 + O(x2), where O(x2) gathers all the terms in x 2, x3, x4 . (The ﬁrst step above was to add the ﬁrst three rows to the last one to give a row of −x’s, the second was to add the ﬁrst three columns to the last one.) Therefore, λ1λ2λ3 = 4c44 = 32. 9 NETWORKS AND FLOWS 75 More to the point, by crossing out other rows/columns, we can see that all the cofactors of L are are equal! The same argument gives Lemma 8.14. For a simple connected graph, all the cofactors of L are equal (to 1~n times the product of its non-zero eigenvalues). Theorem 8.15 (Kirchoﬀ’s matrix tree theorem). This number equals the number of spanning trees in the simple connected graph G. Sketch of proof. This is based on another matrix associated to a graph, its incidence matrix. Or rather, the incidence matrix M associated to a digraph. First, we need to ‘orient’ the edges of G arbitrarily, as in the picture on the previous page. Then rows of M represent vertices, columns edges, and a 1 (resp. −1) in a column means that the edge leaves (resp. enters) the vertex associated to that row. With vertices labelled 1, 2, 3, 4 and edges labelled a, b, c, d, e, this gives M = ™ Œ Œ Œ ﬂ 1 0 0 −1 0 −1 1 0 0 −1 0 −1 1 0 0 0 0 −1 1 1 ﬁ Š Š Š Ł . It is easy to understand that L = M M Š. The sum of the rows of M is also zero. In general, for a connected graph, the rank of M equals n − 1, and (this is the key point) one can show that a subset of n − 1 edges forms a tree if and only if the determinant of that submatrix is non-zero. We do not lose information by deleting any row of M, say the last, to deﬁne the reduced incidence matrix R. The proof of Kirchoﬀ’s theorem is now a matter of computing cnn(L) = det(RRŠ) as a sum of products of sub-determinants of R. A generalization of the usual rule for det(AB) says how to do that. Example 8.16. The complete graph K3 obviously has 3 spanning trees, and K4 has › 6 3” − 4 = 16. Using Kirchoﬀ’s theorem (and the trick of adding rows to simplify the cofactor calculation), one quickly obtains Corollary 8.17. The complete graph Kn has nn−2 spanning trees. Actually, we can forget about Kn, and nn−2 counts labelled trees with n vertices. This fact was known to Cayley, and Prüfer explained how such trees can be described by sequences of numbers (a1, . . . , an−2) with ai ∈ {1, . . . , n}. 9 Networks and ﬂows In this section, we discuss ﬂows in networks: How much can be transported in a network from a source s to a sink t if the capacities of the connections are given? Such a network might model a system of pipelines, a water supply system, or a system of roads. The theory of ﬂows has many applications, and is one of the most important parts of combinatorial optimization. 9 NETWORKS AND FLOWS 76 9.1 Network ﬂow Throughout this section, we work with directed graph. We recall some of the terminology that will be needed. Let G = (V, E) is a directed graph. For an edge e ∈ E , we denote the start, source or origin of e by e− , and the end, sink or target of e by e+ . We say that W ⊂ E is a path in G if W an undirected path is a path in the undirected graph G. In that case, we can write W = v0 X v1 X v2 X \u0005 X vk We say that ei = vi X vi+1 is a forward edge if ei = vi → vi+1 , and a backward edge if ei = vi ← vi+1 . We say that W is a directed path or an aligned path if each edge ei is a forward edge, so that W = v0 → v1 → v2 → \u0005 → vk. Deﬁnition 9.1. Let G = (V, E) be a directed graph. A capacity on G is a map c ∶ E → RE0 . The capacity of an edge e ∈ E is the quantity c(e). Let s, t ∈ V be two vertices such that t is accessible from s. We call N = (G, c, s, t) a network with source s and sink t. A ﬂow on N is a map f ∶ E → RE0 which satisﬁes the following conditions: (F1) For each edge e ∈ E , we have 0 D f (e) D c(e). We call e saturated if f (e) = c(e), and void if f (e) = 0. (F2) For each vertex v ∈ V \u0013 {s, t}, we have Q e+=v f (e) = Q e−=v f (e), where e− is the source of e and e+ its target. This is known as Kirchoﬀ’s Law. Example 9.2. The diagram illustrates a ﬂow of value 8 on a network whose capacities are indicated by the ringed numbers. There are three saturated edges: s → a, a → c, c → t. The edge b → t is void. Example 9.3. As we explained in the introduction, networks can be used to represent of one-way roads (like motorway carriageways and sliproads) carrying traﬃc, pipes with carrying ﬂuid or gas at pressure, or a national electricity grid. 9 NETWORKS AND FLOWS 77 Lemma 9.4. Let N = (G, c, s, t) be a network with a ﬂow f ∶ E → RE0 . Then, we have Q s=e− f (e) − Q s=e+ f (e) = Q t=e+ f (e) − Q t=e− f (e). (4) Proof. We have Q s=e− f (e) + Q t=e− f (e) + Q v≠s,t v=e− f (e) = Q e∈V f (e) = Q s=e+ f (e) + Q t=e+ f (e) + Q v≠s,t v=e+ f (e) The result then follows from Condition (F2). Deﬁnition 9.5. Let N = (G, c, s, t) be a network with a ﬂow f ∶ E → RE0 . (a) The quantity given by (4) is called the value of the ﬂow f , and is denoted by w(f ). (b) We say that f is maximal if for any ﬂow f ′ on N , we have w(f ′) D w(f ). Problem. Given a network with source and sink, ﬁnd a ﬂow with the maximum possible value, a so-called maximum ﬂow. 9.2 The max ﬂow, min cut theorem The main result of this section is known as the Max Flow, Min Cut Theorem. It states that the maximal value of a ﬂow always equals the minimal capacity of a cut. We ﬁrst start with some deﬁnition, then we give a characterization of maximal ﬂows. Deﬁnition 9.6. Let N = (G, c, s, t) be a network. (a) A cut is a partition of the set V of vertices of into two disjoint subsets S and T , one of which contains s and the other t: V = S @ T, s ∈ S, t ∈ T. We denote such a cut by (S, T ). (b) The capacity of a cut (S, T ) is the quantity c(S, T ) ∶= Q e−∈S,e+∈T c(e). (5) (c) We say that the cut (S, T ) is minimal if for any cut (S′, T ′), we have c(S, T ) D c(S′, T ′). Remark 9.7. A cut is completely speciﬁed by S since T = V \u0013 S . It is also speciﬁed by the arcs that need to be removed to separate S from T . If we remove these arcs, we are assuming that the resulting subgraphs are both connected. An obvious special case is always S = {s}. Example 9.8. A cut can be visualized by means of a line or curve cutting through the edges joining S to T . Returning to Example 9.2, we see that the red and the green lines represent two distinct cuts. The red one has capacity 34, and the green one only 10. 9 NETWORKS AND FLOWS 78 Lemma 9.9. Let N = (G, c, s, t) be a network, with a ﬂow f ∶ E → RE0 . Let (S, T ) be a cut of N . Then, we have w(f ) = Q e−∈S,e+∈T f (e) − Q e+∈S,e−∈T f (e). (6) In particular, we have w(f ) D c(S, T ). Equality holds if and only if • Every edge e ∈ E , with e− ∈ S and e+ ∈ T , is saturated; • Every edge e ∈ E , with e− ∈ T and e+ ∈ S , is void. Proof. By summing Equation (4) in Condition (F2), we have w(f ) = Q v∈S „ Q v=e− f (e) − Q v=e+ f (e)‚ = Q e−∈S,e+∈S f (e) + Q e−∈S,e+∈T f (e) − Q e+∈S,e−∈S f (e) − Q e+∈S,e−∈T f (e) = Q e−∈S,e+∈T f (e) − Q e+∈S,e−∈T f (e). The last equality follows from the fact that the ﬁrst and second terms of the second equality cancel out. Deﬁnition 9.10. Let N = (G, c, s, t) be a network, with a ﬂow f ∶ E → RE0 . A path W from s to t is called an augmenting path with respect to f if (i) f (e) < c(e) holds for every forward edge e ∈ W ; (ii) f (e) > 0 for every backward edge e ∈ W . Example 9.11. In Example 9.2, the path s → c ← b → t is an augmenting path. The following theorem is a characterisation of maximal ﬂows on networks. Theorem 9.12 (Augmenting path theorem). Let N = (G, c, s, t) be a network and f ∶ E → RE0 a ﬂow on N . Then, f is maximal if and only if there are no augmenting paths with respect to f . 9 NETWORKS AND FLOWS 79 Proof. First, assume that f is a maximal ﬂow. Suppose there is an augmenting path W . Let d be the minimum of all values c(e) − f (e) (taken over all forward edges e in W ) and all values f (e) (taken over the backward edges in W ). Then d > 0, by deﬁnition of an augmenting path. Now we deﬁne a mapping f ′ ∶ E → RE0 as follows: f ′(e) = ¢¨¨¨ ¦ ¨¨¨¤ f (e) + d if e is a forward edge in W, f (e) − d if e is a backward edge in W, f (e) otherwise. It is easily checked that f ′ is a ﬂow on N with value w(f ′) = w(f ) + d > w(f ), contradicting the maximality of f . Conversely, suppose there are no augmenting paths in N with respect to f . Let S be the set of all vertices v such that there exists an augmenting path from s to v (including s itself), and put T = V \u0013 S . By hypothesis, (S, T ) is a cut of N . Note that each edge e = uv with e− = u ∈ S and e+ = v ∈ T has to be saturated: otherwise, it could be appended to an augmenting path from s to u to reach the point v ∈ T , a contradiction. Similarly, each edge e with e− ∈ T and e+ ∈ S has to be void. Then Lemma 9.9 gives w(f ) = c(S, T ), so that f is maximal. Theorem 9.13 (Integral ﬂow theorem). Let N = (G, c, s, t) be a network where all capacities c(e) are integers. Then there is a maximal ﬂow f on N such that all values f (e) are integral. Proof. By setting f0(e) = 0 for all e, we obtain an integral ﬂow f0 on N with value 0. If this trivial ﬂow is not maximal, then there exists an augmenting path with respect to f0 . In that case the number d appearing in the proof of Theorem 9.12 is a positive integer, and we can construct an integral ﬂow f1 of value d as in the proof of Theorem 9.12. We continue in the same manner. As the value of the ﬂow is increased in each step by a positive integer and as the capacity of any cut is an upper bound on the value of the ﬂow (by Lemma 9.9), after a ﬁnite number of steps we reach an integral ﬂow f for which no augmenting path exists. By Theorem 9.12, this ﬂow f is maximal. Corollary 9.14. Let f be a ﬂow on a ﬂow network N = (G, c, s, t), denote by Sf the set of all vertices accessible from s on an augmenting path with respect to f , and put Tf = V \u0013 Sf . Then f is a maximal ﬂow if and only if t ∈ Tf . In this case, (Sf , Tf ) is a minimal cut: w(f ) = c(Sf , Tf ). Theorem 9.15 (Max-ﬂow min-cut). Let N = (G, c, s, t) be a network, and f ∶ E → RE0 be a ﬂow on N . The maximal value of f is equal to the minimal capacity of a cut for N . Proof. The assertion follows from Theorem 9.13 and Corollary 9.14. 9.3 Labelling Algorithm In this section, we describe more carefully the algorithm that provides an infallible method for increasing the ﬂow through a network, if such an increase is possible. The proof of Theorem 9.13 suggests the following rough outline of such an algorithm: 9 NETWORKS AND FLOWS 80 Algorithm 9.16 (Naive Labelling algorithm). Let N = (G, c, s, t) a network. The algorithm output a maximal ﬂow f on N . (1) f (e) ← 0 for all edges e; (2) while there exists an augmenting path with respect to f do: (3) let W = (e1, . . . , ek) be an augmenting path from s to t, and set d ← min({c(ei) − f (ei) ∶ ei is a forward edge in W } ∪ {f (ei) ∶ ei is a backward edge in W }); f (ei) ← f (ei) + d for each forward edge ei; f (ei) ← f (ei) − d for each backward edge ei; (4) od Example 9.17. Returning to Example 9.2, we want to ﬁnd a maximal ﬂow starting with the ﬂow f above. In the ﬁrst step, we use the augmenting path: s → c ← b → t. The possible increment for the ﬂow f is d0 = min{c(s → c) − f (s → c), f (c → b), c(b → t) − f (b → t)} = min{12 − 3, 1, 8 − 0} = 1. So the augmented ﬂow f1 only changes the values of the ﬂow f along the edges s → c, c ← b and b → t. We get that f1(e) = ¢¨¨¨ ¦ ¨¨¨¤ f (e) + 1 if ∈ {s → c, b → t}; f (e) − 1 if ∈ {c ← b}; f (e) if e ∉ {s → c, c ← b, b → t}. The values of the ﬂow f1 are given by the second entry (in red) above the capacity of each edge in the diagram below. Next, the path s → c ← a → b → t is an augmenting path with respect to f1 . The new increment with respect to that path is d1 = min{c(s → c) − f1(s → c), f1(c → a), c(a → b) − f1(a → b), c(b → t) − f1(b → t)} = min{12 − 4, 2 − 1, 8 − 0} = 1. 9 NETWORKS AND FLOWS 81 We get that f2(e) = ¢¨¨¨ ¦ ¨¨¨¤ f1(e) + 1 if ∈ {s → c, a → b, b → t}; f1(e) − 1 if ∈ {c ← a}; f1(e) if e ∉ {s → c, c ← a, a → b, b → t}. The values of the ﬂow f2 are given by the third entry (in red) above the capacity of each edge in the diagram above. There is no augmenting path with respect to the ﬂow f2 since all the forward edges from a and c are saturated, and all the backward ones have ﬂow 0. Therefore, by Theorem 9.12, f2 is maximal. The vertices that are accessible from s on an augmenting path with respect to f2 are a and c. So setting S2 = {s, a, c} and T2 = V \u0013 S2 = {b, t}, we get a minimal cut by Corollary 9.14. We verify that c(S2, T2) = 10 = w(f2). A more reﬁned version of Algorihm 9.16 is the Ford and Fulkerson Labelling algorithm given below. It uses an analogue of the BFS algorithm to ﬁnd a maximal ﬂow. Algorithm 9.18 (Labelling algorithm). Let N = (G, c, s, t) a network. The algorithm output a maximal ﬂow f on N . The algorithm iterates the following subroutine. (1) ε = 0; (2) Q = (s); (3) L(s) = ∞; (4) Add (s, L(s)) to the table; (5) while Q is non-empty and has ﬁrst element x do: (6) while there exists y adjacent to x not already in Q do: (7) if (x, y) ∈ E and f (x, y) < c(x, y) then: (8) add y to Q; (9) L(y) = min{L(x), c(x, y) − f (x, y)}; (10) add (y, L(y), x, +) to the table; (11) if y = t then: (12) ε = L(t); (13) stop; (14) elif (y, x) ∈ E and f (y, x) > 0: (15) add y to Q; (16) L(y) = min{L(x), f (y, x)}; 9 NETWORKS AND FLOWS 82 (17) add (y, L(y), x, −) to the table; (18) remove x from Q; (19) return ε and the table Example 9.19. We rework in Example 9.17, using the Relabelling Algorithm 9.18. Again, we want to ﬁnd a maximal ﬂow starting with the ﬂow f0 = f . We’ll apply the labels in a table to avoid further complicating the diagram s a b c t 1st iteration: ∞ 4c − 1c − 9s+ 1b + Queue is scabt Below each vertex is a number indicating the amount of ﬂow that can be transferred towards that vertex, from which vertex it was transferred, and in which direction. We can now update the ﬂow by d = 1 (the amount reaching t) along the augmenting path s → c ← b → t, which is remembered with the aid of the symbols in the last row. Forward edges have the ﬂow increased by d, backwards ones have it reduced by d. We can now remove all the labels and apply the same procedure to the updated ﬂow to perform a second iteration: s a b c t 2nd iteration: ∞ 4c − 1a+ 8s+ 1b + Q is scabt 3rd iteration: ∞ 3c − 7s+ Q is sca In the third iteration, we cannot augment any edges beyond a or c because forward ones are saturated and backward ones have ﬂow 0. This means that the second iteration produced a maximum ﬂow. The diagram shows the all the ﬂow numbers after n = 0, 1, 2 iterations, and the maximum ﬂow has value 10. With hindsight, it was obvious that our network admits a ﬂow with value 10 – we can send 2 units along the path sabt and 8 units along the path sct. However, the Labelling Algorithm has the advantage that it can be applied to any any ﬂow, including the one with all numbers 0, which would have produced the more obvious maximum ﬂow. 9 NETWORKS AND FLOWS 83 s a b c d e f t 38 1 2 8 10 13 26 2 8 24 1 17 7 27 0 0 0 0 0 0 0 0 0 0 0 00 0 0 k x y ∈ Ax \u0013 Q Edge Label L(y) Queue 1 s a + min{L(s), c(s → a) − f0(s → a)} = 38 b + min{L(s), c(s → b) − f0(s → b)} = 1 sabf f + min{L(s), c(s → f ) − f0(s → f )} = 2 a c + min{L(a), c(a → c) − f0(a → c)} = 10 sabf cd d + min{L(a), c(a → c) − f0(a → c)} = 13 b {} sabf cd f t + min{L(f ), c(f → t) − f0(f → t)} = 2 sabf cdt Figure 3: First iteration: w(f0) = 0 Remark 9.20. Algorithm 9.18 also returns the cut associated to the maximal ﬂow by Corol- lary 9.14. In Example 9.19, we obtain the cut S = {s, a, c} and T = {b, t}. The capacity of this cut is c(S, T ) = c(a → b) + c(c → t) = 2 + 8 = 10. Example 9.21. We want to ﬁnd a maximal ﬂow on the network in Figure 3 using Algo- rithm 9.18. The details of the calculations are given in the Figures 3, 4, 5, 6, 7, 8, 9, 10, 11 and 12, and the summary in Table 2. In the column Edge, we indicate whether an edge is forward by +, and backward by −; and by s that it is saturated. The maximal ﬂow is obtained after 9 iterations. At each step, one determines the augmenting path (winning path) by using the second and third columns of the table and working from bottom to top. For example, in the ﬁrst step, we see that the augmenting path is s → f → t–it is shown in blue. We use the data of the k -th iteration to obtain the k -th row of Table 2. For example, in the ﬁrst step, we see that the ﬂow into a comes from s, so the label for a is 38s+ , 9 NETWORKS AND FLOWS 84 s a b c d e f t 38 1 2 8 10 13 26 2 8 24 1 17 7 27 0 0 2 0 0 0 0 0 0 0 0 00 0 2 k x y ∈ Ax \u0013 Q Edge Label L(y) Queue 2 s a + min{L(s), c(s → a) − f1(s → a)} = 38 b + min{L(s), c(s → b) − f1(s → b)} = 1 sab f +/s a c + min{L(a), c(c → a) − f1(c → a)} = 10 sabcd d + min{L(a), c(d → a) − f1(d → a)} = 13 b {} sabcd c t + min{L(c), c(c → t) − f1(c → t)} = 1 sabcdt e, f + Figure 4: Second iteration: w(f1) = 2 9 NETWORKS AND FLOWS 85 s a b c d e f t 38 1 2 8 10 13 26 2 8 24 1 17 7 27 1 0 2 0 1 0 0 0 0 0 1 00 0 2 k x y ∈ Ax \u0013 Q Edge Label L(y) Queue 3 s a + min{L(s), c(s → a) − f2(s → a)} = 37 b + min{L(s), c(s → b) − f2(s → b)} = 1 sab f +/s a c + min{L(a), c(a → c) − f2(a → c)} = 9 sabcd d + min{L(a), c(a → d) − f2(a → d)} = 13 b {} sabcd c e + min{L(c), c(c → e) − f2(c → e)} = 8 sabcdeff + min{L(c), c(c → f ) − f2(c → f )} = 9 t +/s d t + min{L(d), c(d → t) − f2(d → t)} = 7 sabcdef t Figure 5: The third iteration: w(f2) = 3 9 NETWORKS AND FLOWS 86 s a b c d e f t 38 1 2 8 10 13 26 2 8 24 1 17 7 27 8 0 2 0 1 7 0 0 0 0 1 07 0 2 k x y ∈ Ax \u0013 Q Edge Label L(y) Queue 4 s a + min{L(s), c(s → a) − f3(s → a)} = 30 b + min{L(s), c(s → b) − f3(s → b)} = 1 sab f +/s a c + min{L(a), c(a → c) − f3(a → c)} = 9 sabcd d + min{L(a), c(a → d) − f3(a → d)} = 6 b {} sabcd c e + min{L(c), c(c → e) − f3(c → e)} = 8 sabcdeff + min{L(c), c(c → f ) − f3(c → f )} = 9 t +/s d t +/s sabcdef e t + min{L(e), c(e → t) − f3(e → t)} = 7 sabcdef t Figure 6: The fourth iteration: w(f3) = 10 9 NETWORKS AND FLOWS 87 s a b c d e f t 38 1 2 8 10 13 26 2 8 24 1 17 7 27 15 0 2 0 8 7 0 0 0 7 1 07 7 2 k x y ∈ Ax \u0013 Q Edge Label L(y) Queue 5 s a + min{L(s), c(s → a) − f4(s → a)} = 23 b + min{L(s), c(s → b) − f4(s → b)} = 1 sab f +/s a c + min{L(a), c(a → c) − f4(a → c)} = 2 sabcd d + min{L(a), c(a → d) − f4(a → d)} = 6 b {} sabcd c e + min{L(c), c(c → e) − f4(c → e)} = 1 sabcdeff + min{L(c), c(c → f ) − f4(c → f )} = 2 t +/s d t +/s sabcdef e t +/s sabcdef f t + min{L(f ), c(f → t) − f4(f → t)} = 2 sabcdef t Figure 7: The ﬁfth iteration: w(f4) = 17 9 NETWORKS AND FLOWS 88 s a b c d e f t 38 1 2 8 10 13 26 2 8 24 1 17 7 27 17 0 2 0 10 7 0 0 2 7 1 07 7 4 k x y ∈ Ax \u0013 Q Edge Label L(y) Queue 6 s a + min{L(s), c(s → a) − f5(s → a)} = 21 b + min{L(s), c(s → b) − f5(s → b)} = 1 sab f +/s a d + min{L(a), c(d → a) − f5(d → a)} = 6 sabd c +/s b c + min{L(b), c(b → c) − f5(b → c)} = 1 sabdc c e + min{L(c), c(c → e) − f5(c → e)} = 1 sabdceff + min{L(c), c(c → f ) − f5(c → f )} = 1 t +/s e t +/s sabdcef d t +/s sabdcef f t + min{L(f ), c(f → t) − f5(f → t)} = 1 sabdcef t Figure 8: The sixth iteration: w(f5) = 19 9 NETWORKS AND FLOWS 89 s a b c d e f t 38 1 2 8 10 13 26 2 8 24 1 17 7 27 17 1 2 0 10 7 1 0 3 7 1 07 7 5 k x y ∈ Ax \u0013 Q Edge Label L(y) Queue 7 s a + min{L(s), c(s → a) − f6(s → a)} = 21 b +/s sa f +/s a b + min{L(a), c(b → a) − f6(b → a)} = 8 sabdd + min{L(a), c(a → d) − f6(a → d)} = 6 c +/s b c + min{L(c), c(b → c) − f6(b → c)} = 8 sabdc d e min{L(f ), c(d → e) − f6(d → e)} = 1 sabdce t +/s c e + min{L(c), c(c → e) − f6(c → e)} = 1 sabdceff + min{L(c), c(c → f ) − f6(c → f )} = 8 t +/s e t +/s sabdcef f t + min{L(f ), c(f → t) − f6(f → t)} = 8 sabdcef t Figure 9: The seventh iteration: w(f6) = 20 9 NETWORKS AND FLOWS 90 s a b c d e f t 38 1 2 8 10 13 26 2 8 24 1 17 7 27 25 1 2 8 10 7 9 0 11 7 1 07 7 13 k x y ∈ Ax \u0013 Q Edge Label L(y) Queue 8 s a + min{L(s), c(s → a) − f7(s → a)} = 13 b +/s sa f +/s a d + min{L(a), c(d → a) − f7(d → a)} = 6 sad b, c +/s d e min{L(d), c(d → e) − f7(d → e)} = 1 sade t +/s e c − min{L(e), f7(c → e)} = 1 sadec t +/s c f + min{L(c), c(c → f ) − f7(c → f )} = 1 sadcef t +/s f t + min{L(f ), c(f → t) − f7(f → t)} = 1 sadcef t Figure 10: The eighth iteration: w(f7) = 28 9 NETWORKS AND FLOWS 91 s a b c d e f t 38 1 2 8 10 13 26 2 8 24 1 17 7 27 27 1 2 8 10 9 11 2 13 7 1 07 7 15 k x y ∈ Ax \u0013 Q Edge Label L(y) Queue 9 s a + min{L(s), c(s → a) − f8(s → a)} = 11 b +/s sa f +/s a d + min{L(a), c(d → a) − f8(d → a)} = 4 sad b, c +/s d e min{L(d), c(d → e) − f8(d → e)} = 1 sade t +/s e c − min{L(e), f8(c → e)} = 1 sadec t +/s c f + min{L(c), c(c → f ) − f8(c → f )} = 1 sadcef t +/s f t + min{L(f ), c(f → t) − f8(f → t)} = 1 sadcef t Figure 11: The ninth iteration: w(f8) = 30 9 NETWORKS AND FLOWS 92 s a b c d e f t 38 1 2 8 10 13 26 2 8 24 1 17 7 27 28 1 2 8 10 10 11 2 14 6 1 17 7 16 k x y ∈ Ax \u0013 Q Edge Label L(y) Queue 10 s a + min{L(s), c(s → a) − f9(s → a)} = 10 b +/s sa f +/s a d + min{L(a), c(d → a) − f9(d → a)} = 3 sad b, c +/s d e +/s sad t +/s Figure 12: The tenth iteration: w(f9) = 31 9 NETWORKS AND FLOWS 93 k s a b c d e f t Q 1 ∞ 38s+ 1s+ 10a + 13a+ 2s + 2f + sabcdt 2 ∞ 38s+ 1s+ 10a + 13a+ 1c + sabcdt 3 ∞ 37s+ 1s+ 9a+ 13a+ 8c + 9c + 7d + sabcdef t 4 ∞ 30s+ 1s+ 9a+ 6a+ 8c + 9c + 7e+ sabcdef t 5 ∞ 23s+ 1s+ 2a+ 6a+ 1c + 2c + 2f + sabdcef t 6 ∞ 21s+ 1s+ 1b + 6a+ 1c + 1c + 1f + sabdcef t 7 ∞ 21s+ 8a + 8b + 6a+ 1c + 8c + 8f + sadcef t 8 ∞ 13s+ 1e − 6a+ 1d + 1c + 1f + sadcef t 9 ∞ 11s+ 1e − 4a+ 1d + 1c + 1f + sadcef t 10 ∞ 3a+ sad Table 2: Summary of the Labelling algorithm for Example 9.23 u a b c e f g v u a c f g v Figure 13: The sets {e, a} and {e, b} are vertex separator for u and v the ﬂow into b also comes from s, hence the label 1b + . But the ﬂow into c comes from a, hence the label 10a + . And so on. The cut associate to the maximal ﬂow is S = {s, a, d} and T = {b, c, e, f, t}. The value of the ﬂow is w(f9) = 31. 9.4 Menger’s theorem* Deﬁnition 9.22. Let G = (V, E) be a connected graph. Let u, v ∈ V be non-adjacent, and X \u0013 {u, v}. We say that X is a vertex separator for u and v (or a (u, v)-separating set) if we can disconnect both G, and u and v , by deleting the vertices in X . We say that X is a minimal vertex separator for u and v or (a minimal (u, v)-separating set) if it is not properly contained in another (u, v)-vertex separator. Example 9.23. In the graph in Figure 13, the two sets {e, a} and {e, b} are (u, v)-separators. Both sets have minimal size as (u, v)-separating sets. Example 9.24. In the graph in Figure 14, we see that we can disconnected both G and u, v by deleting v1 , v2 and v3 . This is a minimal size for a (u, v)-separating set. 9 NETWORKS AND FLOWS 94 u v1 v2 v3 v4 v5 v6 v7 v8 v9 v u v4 v5 v6 v7 v8 v9 v Figure 14: A (u, v)-separating set for the graph is {v1, v2, v3} Deﬁnition 9.25. Let G = (V, E) be a connected graph. Let u, v ∈ V be non-adjacent. A set of (u, v)-path S = {P1, . . . , Pk} is called an edge separator (or internally disjoint) if no two paths in S have a common vertex other than u and v . Example 9.26. The paths P1 = v1v3v5v7v6, P2 = v1v2v8v6, P3 = v1v4v7v6 are not internally disjoint since P1 and P3 have the vertex v7 in common. However, the paths Q1 = v1v4v10v2, Q2 = v1v6v8v9v5v2, Q3 = v1v3v7v2 are internally disjoint. Theorem 9.27 (Menger’s theorem). Let G = (V, E) be a connected graph and u, v ∈ V be non-adjacent. The size of a minimal (u, v)-separating set is equal to the maximum number of internally disjoint (u, v)-paths in G. Example 9.28. In Example 9.24, {v4, v5} is a minimal (u, v)-separating set. So by Menger’s theorem, this is the maximal number of internally disjoint (u, v)-paths in G. Example 9.29. Let A and B two Earth’s locations, and G = (V, E) be a network of satellites relaying communications between A and B . Assume there is a magnetic storm from the sun. Question: How many satellites have to be disabled by the storm in order to totally disrupt communications between A and B ? This example pertains to a recent news article about the lost of 40 satellites by Starlink https://www.bbc.co.uk/news/world-60317806. 9 NETWORKS AND FLOWS 95 u a b c v u a b c v Figure 15: Replacing every edge xy with directed arcs x → y and y → x The Falcon 9 rocket launch on 3 February 2022 which carried 49 Starlink satellites, most of which were caught by the storm Before we prove Menger’s theorem, we will do some preparation. We saw the notion of cut for networks. Now we extend this to graph. We construct a graph G∗ in the following two steps: (a) Replace every edge xy with directed arcs x → y and y → x (see Figure 15). (b) Let G∗ = (V ∗, E∗) be the directed graph obtained as follows (see Figure 16): (i) If x ∉ {u, v}, we split x into two vertices x− and x+ , and put a new directed edge x− → x + between the two. We call x− → x+ and internal arc. (ii) If x → y is an edge for the graph obtained in (a), we replace it by • x+ → v if x, y ∉ {u, v}; • x → y− if x ∈ {u, v}; • x+ → y if y ∈ {u, v}. 9 NETWORKS AND FLOWS 96 u a − a + b − b + c− c+ v Figure 16: Splitting vertices into internal arcs Deﬁnition 9.30. Let G = (V, E) be a connected simple graph, and u, v ∈ V two distinct vertices. We call G∗ = (V ∗, E∗) the augmented directed graph with source u and sink v obtained from G. Lemma 9.31. Let G = (V, E) be a connected simple graph, and u, v ∈ V two distinct vertices. Let X ⊂ V be a (u, v)-vertex separator for G. Then there exists a (u, v)-cut (S∗, T ∗) for G∗ such that every arc that connects S∗ to T ∗ is an internal arc x− → x+ for some x ∈ X . Proof. If P is a path in G, we denote by V (P ) the vertices supporting P . Similarly, if Q is a path in G∗ , we denote by V (Q) the vertices supporting Q. We deﬁne S∗ and T ∗ by S∗ ∶= {w ∈ V ∗ ∶ w ∈ V (Q) where Q is a path between u and x− for x ∈ X} ; T ∗ ∶= V ∗ \u0013 S∗. Since X is a (u, v)-separator, every path P that connects u to v can be written as P = u Ð x1 Ð x2 Ð \u0005 Ð xk Ð x Ð y1 Ð y2 Ð \u0005 Ð ym Ð v, with x ∈ X , and such that {x1, x2, . . . xk} ∩ {y1, y2, . . . ym} = g. We can convert P into a directed path Ð→ P = Ð→ P u,x− @ Ð→ P x+,v in G∗ , where Ð→ P u,x− = u → x − 1 → x+ 1 → x− 2 → x+ 2 → \u0005 → x − k → x+ k → x−; Ð→ P x+,v = x+ → y− 1 → y+ 1 → y− 2 → y+ 2 → \u0005 → y− m → y+ m → v. This decomposition is unique. Proof of Menger’s theorem. Let c(u, v) be the size of a minimal (u, v)-separating set, and λ(u, v) the maximum number of an internally disjoint (u, v)-paths in G. We want to show that c(u, v) = λ(u, v). 9 NETWORKS AND FLOWS 97 u a − a + b − b + c− c+ v 1 1 1 5 5 5 555 5 5 5 5 5 5 Figure 17: Adding capacities to the network Let U be a minimal (u, v)-separating set for G. By deﬁnition of this set, every (u, v)-path must go through U . u v × × × × λ paths U So the number of internally disjoint paths is at most SU S; hence we must have λ(u, v) D SU S = c(u, v). To complete the proof of the theorem, we must show that c(u, v) D λ(u, v). We deﬁne a network N ∗ = (G∗, c∗, u, v), with source u, sink v and capacity c ∗ . The capacities are assigned as follows (see Figure 17): (a) If e ∈ E∗ is an internal arc, we let c ∗(e) = 1; (b) If e ∈ E∗ is that is not an internal arc, we let c ∗(e) = n, where n = SV S. Goal: We want to show that • max-ﬂow of N ∗ D λ(u, v); • c(u, v) D min-cut of N ∗ . 9 NETWORKS AND FLOWS 98 Combining these two facts, with the Max-Flow Min-Cut Theorem (Theorem 9.15), we obtain c(u, v) D min-cut of N ∗ = max-ﬂow of N ∗ D λ(u, v). To show that max-ﬂow of N ∗ D λ(u, v), let f be a maximum ﬂow, with value w(f ) = m. If there is a ﬂow into x− , the value of that ﬂow has to be 1 since there is only one directed arc from x− . This unit ﬂow travels from u to v . The m unit ﬂows will transform into an internally disjoint set of (u, v)-paths, hence max-ﬂow of N ∗ D λ(u, v). To show that c(u, v) D min-cut of N ∗ , take a (u, v)-cut (S, T ) for the network N ∗ , assume for a contradiction that there is a non-internal directed arc from S to T . Then, by construction of N ∗ , we have c(S, T ) E n. u v v1 v2 vt v′ 1 v′ 2 v′ t S T 1 1 1 1 Now consider the ﬂow with (u, v)-cut (S∗, T ∗), where S∗ = {u} ∪ {x− ∶ ux ∈ E} ; T ∗ = V \u0013 S∗. Every path that crosses from S∗ to T ∗ contains an internal arc. u v x − 1 x − 2 x − t x + 1 x + 2 x + t S∗ T ∗ 1 1 1 1 This implies that c(S∗, T ∗) D n − 2 < n. However, this is a contradiction. Therefore, all arcs must be internal arcs. 9 NETWORKS AND FLOWS 99 Let (S, T ) be a minimal cut for N ∗ , and consider the set U = {x ∈ V ∶ x− ∈ S, x+ ∈ T } . Since all the arcs in G∗ are internal and (S, T ) is a minimal cut for N ∗ , we see that SU S = c(S, T ). We will show that U is a minimal (u, v)-separating set for G. From this it follows that c(u, v) D SU S = c(S, T ) = min-cut for N ∗. To prove the claim, consider a path P = u Ð v1 Ð \u0005 Ð vk Ð v in G. We can convert it into a path in N ∗ , given by Ð→ P = u → v− 1 → v+ 1 → \u0005 → x− → x+ → \u0005 → v− k → v+ k → v, with x− ∈ S and x+ ∈ T . u vx − x − 1 x − 2 x − t x + x + 1 x + 2 x + t S T × × × × −→ P Deleting these internal arcs disconnects the graph N ∗ . But the internal arcs come from the vertices in U . So U is a (u, v)-separating set for G. 9.5 Dynamic programming* In this section, we shall once again encounter the optimality principle that underlies Dijkstra’s algorithm for ﬁnding shortest paths in a weighted graph, and the labelling of an activity network to ﬁnd critical paths. Example 9.32. Deborah has £50K to invest in multiples of £10K in three companies C1, C2, C3, and wants to maximise the return. All the money is to be used, but she is not allowed to invest more than once in any company. The following table shows the expected returns on investment, with the amount invested along the top row. All numbers are in units of £10K. 0 1 2 3 4 5 C1 0 3 5 6 7 8 C2 0 2 4 8 10 11 C3 0 0 2 10 11 11 9 NETWORKS AND FLOWS 100 The problem amounts to ﬁnding an aligned path in the following network that runs from bottom left to top right with maximum total weight. Although only three arrows are shown, all the arcs are oriented from left to right, and this is akin to the activity network, in which duration is replaced by ﬁnancial return. The ﬁrst stage consists of deciding how to invest in C1, the second how much to invest in C2 . The diﬀerence has to be invested in C3 : if x1 units are invested in C1 and x2 units in C2 then 0 D x2 D 5 − x1, and x3 = 5 − x1 − x2 units are invested in C3 : With reference to the graph, each event has coordinates (i, yi) where 0 D i D 3 and 0 D yi D 5. At this event, one has concluded a total investment of y = yi in companies up to and including Ci . There are 6 + 5 + 4 + 3 + 2 + 1 = 1 2 ∗ 6 ∗ (6 + 1) = 21 aligned paths from start (0, 0) to ﬁnish (3, 5), but the problem can easily be generalized to more companies and investment choices. Our solution below reveals that there are in fact two longest paths (shown red with a ﬁnal arc in common) realizing a total return of £150K. The special feature of this network is that although there are 6 + 21 + 6 = 33 arcs, there are only 18 diﬀerent weights. These are determined by the functions ri(x) = return on investment of x units in Ci from the table, with 0 D x D 5. From these, we shall construct functions fi(y) = best return at the ith stage for a total investment y , where ‘best’ is taken over all possible investment strategies x1, x2, . . . , xi up to the ith stage, and y denotes the sum x1 + \u0005 + xi . Our aim is to ﬁnd f3(5). Fortunately we do not need to consider all choices. The optimality principle implies that the best investment (which is a longest path) at each stage will necessarily arise from a best investment (longest path) at all previous stages. Hence the Corollary. The ‘best return’ function satisﬁes fi(y) = max 0DxDy ı fi−1(y − x) + ri(x) \u0000 or fi(yi) = max xi ı fi−1(yi−1) + ri(xi) \u0000, 9 NETWORKS AND FLOWS 101 with f0 = 0. In practice, it may help to use the second equation in which the values of x = xi and y = x1 + \u0005 + xi at each stage make the discrete nature of y more explicit. The underlying logic of this formula is identical to that of the expression E(u) = max x→u ıE(x) + d(x, u)\u0000 in §9.1 for ﬁnding latest start times. This is in turn a version of relabelling formula Lj = min(Li + d(vi, vj), Lj) in §8.1 to relax the temporary labels in Dijkstra’s algorithm, ‘min’ here because we were seeking a shortest path. In dynamic programming, the graph overcomplicates the situation, so the work is best organized into a table, which is headed by the values of y in (traditionally) reverse order. There is really a separate table for each stage, which applies the corollary to determine fi(yi), though the tables can be joined together. Strictly speaking, every stage needs a triangular table to cater for all the combinations of x = xi and yi = x1 + \u0005 + xi . However, the ﬁrst and last are simpler, and in our example only the second one illustrates the general technique (this is apparent from the structure of the graph!). 5 4 3 2 1 0 ← y 8 7 6 5 3 0 ← f1(y1) x2 ↓ r2(x2)↓ 5 11 11 4 10 13 10 3 8 13 11 8 ← f1(y1) + r2(x2) 2 4 10 9 7 4 1 2 9 8 7 5 2 0 0 8 7 6 5 3 0 13 11 8 5 3 0 ← f2(y2) = max in ∆ 13 11 10 15 14 11 ← f2(y2) + r3(x3) 15 ← f3(y3) = max The ﬁrst stage is straightforward: we set x = x1 = y and f1(y1) = r1(x1). At the second stage, for each value of y, we are allowed to choose any value of x = x2 with y + x2 D 5, and for each such value we add r2(x2) to our return. The total investment y + x2 is constant along each diagonal ∆, which we scan in order to ﬁnd the maximum return. For example, the diagonal returns 7, 8, 9, 11, 10 is associated to a total inestment of 4 units, and its best return is f2(4) = max 0DxD4 ıf1(4 − x) + r2(x)\u0000 = 11. 9 NETWORKS AND FLOWS 102 There are no entries above the main diagonal since those would represent current total investments of over £50K. As we pass to the next stage, we associate the maximum return to the new value yi = yi−1 + xi, and place it under the bottom-left entry of the diagonal. In theory, we are ready for another triangular table, but in our example, there is no choice left since x3 is determined by x1 + x2 . Thus, there would only be one useful diagonal, which we have shown horizontally to save space. The best return has value 15 coming from x3 = 3, which in turn comes from an earlier best return of 5 with either x2 = 2 or x2 = 1. (These entries have been shown in bold.) Conclusion. Wise investment produces a best possible return of £150K, arising in two ways: (i) £20K in C1 plus £30K in C3, or (ii) £10K in C1 and C2 plus £30K in C3 . These two solutions are the red paths, but the return is not good enough for our Dragon. Dynamic programming is really a BFS with ‘pruning’ – one can forget results from previous stages. Here is an example in which one can compare the technique to Dijkstra’s algorithm, though in this case the latter is probably quicker. Example 9.33. Find the shortest path from a to h in the weighted digraph below: Dijkstra’s algorithm will work provided one takes account of the fact that the edges now directed; for example, d is adjacent to b but not vice versa, so one only scans from left to right. This problem is therefore layered like the investment one – for each vertex all paths from the start have the same length. The solution can be again be obtained in ‘vertical’ stages. The key point in problems like these is that the best solution at the (i + 1)th stage must arise from the best solutions at the ith stage. The shortest path can be obtained from the following table: 10 CRYPTOGRAPHY 103 start at: a dist=0 to get to: b c dist =7 from a dist =4 from a to get to: d e best dist =10 from c best dist =10 from b to get to: f g best dist =11 from e best dist =12 from d to get to: h dist=15 from g Exercise 9.34. Suppose that the weights in the above network now represent capacities, a is the source and h is the sink. Show that the value of a maximum ﬂow is 7 and identify a minimum cut. 10 Cryptography 10.1 Euler’s totient function Let n E 1 be an integer, and recall the set of residue classes R = Z~nZ = {¯x ∶ x ∈ Z} = {x + nZ ∶ x ∈ Z}. In Section 1.3, we saw that R is a ring under the following rules: (a) ¯x + ¯y = x + y , i.e. (x + nZ) + (y + nZ) = (x + y) + nZ; (b) ¯x ∗ ¯y = x ∗ y , i.e. (x + nZ) ∗ (y + nZ) = (x ∗ y) + nZ. Deﬁnition 10.1. Let n E 1 be an integer, and x ∈ Z. We say that the residue class ¯x is unit in Z~nZ if there exists y ∈ Z such that ¯x ∗ ¯y = ¯1. We denote the group of units in Z~nZ by (Z~nZ)∗ (or simply (Z~nZ)× . The following result characterizes units in residue rings. Theorem 10.2. Let n E 1 be an integer and x ∈ Z. Then, ¯x is a unit if and only if there exists x is coprime with n, which is the same as saying that gcd(x, n) = 1. Proof. Take x ∈ Z~nZ. Then, we have ¯x is a unit in Z~nZ ⇐⇒ ∃y ∈ Z such that x ∗ y ≡ 1 mod n ⇐⇒ ∃y ∈ Z such that n S (x ∗ y − 1) ⇐⇒ ∃y ∈ Z such that x ∗ y − 1 = a ∗ n for some a ∈ Z ⇐⇒ ∃y ∈ Z such that x ∗ y − a ∗ n = 1 for some a ∈ Z ⇐⇒ ∃y ∈ Z such that gcd(x, n) = 1. 10 CRYPTOGRAPHY 104 Deﬁnition 10.3. The function ϕ ∶ N → N, deﬁned by ϕ(n) ∶= S(Z~nZ)∗S, is called Euler’s totient function. Theorem 10.4. The Euler totient function satisﬁes the following properties: (a) For every positive integer n ∈ Z, we have ϕ(n) = n M pSn ‰1 − 1 p ’ . (b) For every positive integers m, n such that gcd(m, n) = 1, we have ϕ(m ∗ n) = ϕ(m) ∗ ϕ(n). Example 10.5. ϕ(39) = 39 ‰1 − 1 3 ’ ‰1 − 1 13 ’ = 24; ϕ(100) = 100 ‰1 − 1 2 ’ ‰1 − 1 5 ’ = 40. Theorem 10.6 (Euler). Let n E 1 be an integer, and x ∈ Z. Then, we have gcd(x, n) = 1 Ô⇒ xϕ(n) = 1 mod n. Proof. The set (Z~nZ)∗ is a group of order ϕ(n) by deﬁnition of Euler’s totient function. By Lagrange’s theorem, the order of any element divides the size of this group, so xϕ(n) is the identity in Z~nZ. 10.2 Introduction Human beings have always had a constant desire to protect information or secrets from others. There are plenty of famous historical examples where people tried to keep secrets from adversaries. Kings and generals have communicated with their troops using basic cryptographic methods to prevent enemies from getting access to sensitive military information. In fact, it is often reported that Julius Caesar used a simple cipher, which now carries his name. As society has evolved, the need for more sophisticated methods to protect data has increased. Now, with the information era at hand, the need is more pronounced than ever. As the world becomes more connected, the demand for more information and electronic services is growing, and with increased demand comes increased dependency on electronic systems. Already, the exchange of sensitive information, such as credit card numbers, over the Internet is common practice. Protecting data and electronic systems is crucial to our ways of life. The techniques needs to protect data belong to the ﬁeld of cryptography. 10 CRYPTOGRAPHY 105 Encryption Key Decryption Key Alice Encryption Decryption Bob Eve Plaintext Ciphertext Plaintext Figure 18: Basic communication scenario for cryptography 10.3 Basic setup We have the following basic scenario: Alice wants to send a secret message, the plaintext, to Bob. She does this by encrypting the message into a ciphertext using a secret key, and transmitting the message over a non-secure channel. When Bob receives the message, he decrypts it using the key. Alice and Bob must agree on their keys (in secret). There is an eavesdropper, called Eve, attempting to ﬁnd the plaintext. To formalise the basic scenario in Figure 18, let P be the plaintext alphabet, the set from which the plaintext is written. E.g. P = {A, B, . . . , Z}. Let C denote the ciphertext alphabet; sometimes we have C = P . Let K denotes the set of possible keys. The encryption function is a function E ∶ K × P → C (k, x) ( Ek(x). Likewise, the decryption function is a function D ∶ K × C → P (k, x) ( Dk(x). For all x ∈ P and all k ∈ K , we want that Dk(Ek(x)) = x. P C P Ek id Dk Example 10.7 (Caesar’s cipher). One of the earliest ciphers is due to Julius Caesar. To encrypt a message using Caesar’s cipher, we simply shift each letter by three places forward. To decrypt, we simply shift three letter back. We view this mathematically as follows. Let P = {0, 1, . . . , 25} = Z~26Z, and put P in correspondence with {A, B, . . . , Z} by 0 ↔ A, 1 ↔ B, . . . , 25 ↔ Z. 10 CRYPTOGRAPHY 106 Take C = Z~26Z. Then the encryption function for the Caesar cipher simply adds 3 in Z~26Z: E ∶ P → C x ( x + 3, and the decryption function is D ∶ C → P y ( y − 3. We want to encrypt the message “VENI, VIDI, VICI” with the Caesar cipher. First, we get rid of spaces (as they yield so much information on the structure of the message that decryption becomes easier) to get “VENIVIDIVICI”. Looking up these letters in our plaintext alphabet P , we get the sequence 21, 4, 13, 8, 21, 8, 3, 8, 21, 8, 2, 8. By adding 3 in Z~26Z, we get 24, 7, 16, 11, 24, 11, 6, 11, 24, 11, 5, 11. By identifying this numbers with the corresponding letters, we get “YHQLYLGLYLFL”. 10.4 Vigenère cipher The Vigenère cipher is a variation of the shift cipher. It is attributed to the sixteenth century French cryptographer Vigenère. However, many believe that Vigenère own ciphers were more sophisticated. This cipher was thought to be unbreakable until the nineteenth century: “Le chiﬀre indéchiﬀrable.” First, choose a key length m = 6, say. Then choose a vector of length m from Z~26Z, e.g. k = (21, 4, 2, 19, 14, 17). To encrypt the message, we shift the ﬁrst letter by 21, the second by 4, and so on, then the seventh by 21, and so on. Here the key space for length m is K = (Z~26Z)m = {(k1, . . . , km) ∶ ki ∈ Z~26Z}. We also have P = C = K . The encryption map is Ek ∶ P → C (x1, . . . , xm) ( (x1 + k1, . . . , xm + km). The decryption map is Dk ∶ C → P (y1, . . . , ym) ( (y1 − k1, . . . , ym − km). Example 10.8. The encryption of the sentence “Divert troops East” using the key word “White” is as follows: Plain text DIVERTTR OOPSEAST 3 8 21 4 17 19 19 17 14 14 15 18 4 0 18 19 Keyw ord WHITEWHITEWHITEW 22 7 8 19 4 22 7 8 19 4 22 7 8 19 4 22 Ciphertext ZPD XVPAZHSLZMTWP 25 15 3 23 21 15 0 25 7 18 11 25 12 19 22 15 10 CRYPTOGRAPHY 107 The security of the Vigenère cipher depends on the fact that the key and its length are unknown. There is also another additional advantage over a simple shift cipher in that a letter can be encoded in diﬀerent ways. In Example 10.8, both the letters “I” and “T” encrypted as “P”. Up until the 20th century, one needed a Vigenère table in order to do the encryption. 10.5 The RSA algorithm Until the 1970’s encrypting messages required both sender and receiver to use the same key (‘codebook’) to encrypt and decrypt. Such crypto systems are known as symmetric key crypto systems. The use of such crypto systems is not practical for interchange of secret data on the internet. The concept that led to the introduction of all modern forms of cryptography is that of an asymmetrical key crypto systems based on trapdoors, in the terminology of a famous paper by Diﬃe & Hellman (1976). A trapdoor is a mathematical function f ∶ X → Y such that it is easy to compute y = f (x), given x ∈ X , but hard to solve y = f (x) for x ∈ X , given y ∈ Y . In other words, it is hard to invert f without using extra information. This idea was successively implemented in the celebrated RSA algorithm, named after Rivest, Shamir & Adleman, who discovered it in April 1977 and patented it that year. Years afterwards, it was revealed that the same algorithm had been described by Cliﬀord Cocks in a GCHQ memo in 1973, working with James Ellis, who had already conceived of the trapdoor mechanism. In RSA, one chooses: • Two large primes p ≠ q and form n = pq , the modulus; • An integer 2 D e < ϕ(n) such that gcd(e, ϕ(n)) = 1 and compute 2 D s < ϕ(n) such that es = 1 mod ϕ(n); e is called the encryption or public exponent and s the decryption or secret exponent. The pair kp = (n, e) is called the public key, and the pair ks = (n, s) is called the secret key. One must keep p, q and s secret at all time. The plaintext and ciphertext alphabets are the same P = C = Z~nZ, and the key space is K = (Z~ϕ(n)Z)× . The encryption function is E ∶ P × K → C (m, e) ( Ee(m) = me mod n. The decryption function is D ∶ C × K → P (c, s) ( Ds(c) = c s mod n. Proposition 10.9. Ds(Ee(m, e), s) = m for all m ∈ P and e ∈ K . Proof. This follows from Fermat’s little Theorem (Theorem 1.16) or Euler’s Theorem 10.6. From Proposition 10.9, to send the plaintext m to Bob, Alice does the following: 1. Compute c = m e mod n; 10 CRYPTOGRAPHY 108 2. Send the ciphertext c to Bob. To decipher the message c sent by Alice, Bob simply computes c s = (m e)s = m es = m mod n, by Fermat’s little Theorem or Euler’s Theorem. And hence, Bob recovers the plaintext m sent by Alice. Bob can do these computations since he’s the only one to know s, p and q . Eve also knows e, c = me , and n, but doesn’t know m. Her challenge is to recover m from c and kp = (n, e). Example 10.10. Suppose that p = 29, q = 53 and n = 29 ⋅ 53 = 1537, and Bob’s public key is kp = (n, e) = (1537, 3). Then, ϕ(n) = (p − 1)(q − 1) = 1456. The decryption key is ks = (n, s), where s is the multiplicative inverse of e modulo ϕ(n). The Extended Euclid Algorithm yields s = 971 and (n, s) = (1537, 971). The public key (1537, 3) is published by Bob. Assume that Alice wants to send the message m = 101 to Bob. She can use Bob’s public key to encrypt it as c = Ee(m) = m e mod n = 1013 mod 1537 = 511 mod 1537. She then sends c = 511 to Bob. Bob decrypts as Ds(c) = c s mod n = 511 971 mod 1537 = 101 = m. Example 10.11. In our second example, Bob chooses p = 1999, q = 2029, so the ‘key length’ equals n = pq = 4 055 971, and ϕ = ϕ(n) = (p − 1)(q − 1) = 4 051 944. Bob takes e = 5 so as to make it easy for Alice for work out xe with her primitive calculator. It is obviously coprime to ϕ; indeed choosing e to be a prime obviates the need to check that gcd(e, ϕ) = 1. In addition ϕ + 1 is a multiple of 5, and in fact ϕ + 1 = 5 ∗ 810389, so Bob’s private key is s = 810389. He considers swapping s with e but decides against it. Alice’s message x is in fact only 3 digits long, nonetheless xe is about 996 ∗ 10 9, just less than one trillion. But she did the modular calculation almost by hand: c = 2515 = 251 ∗ (251 2)2 mod n = 251 ∗ (63001)2 mod n = 251 ∗ (3 969 126 001) mod n = 251 ∗ (2 386 363) mod n = 2 749 376 mod n. Bob now uses a computer to discover that c s mod n = 251, so Alice had encrypted her module code. 10 CRYPTOGRAPHY 109 10.6 Security of RSA The security of RSA relies on the assumption that factoring (big) composite integers is hard. If Eve knows p and q , then she knows ϕ(n) = (p − 1)(q − 1), and she can easily compute the private exponent s such that se ≡ 1 mod ϕ(n). Moreover, given n = pq and a multiple of lcm(p − 1, q − 1), one can compute p and q . Indeed, suppose we are given ϕ(n) = (p − 1)(q − 1) = pq − (p + q) + 1. Then, we can recover p, q as the roots of the quadratic polynomial (X − p)(X − q) = X 2 − (n + 1 − ϕ(n))X + n. So one must keep ϕ(n) secret as well. In practice, p and q should have a similar length but diﬀer by a few powers of 10. Prime numbers can be found using primality tests, like a probabilistic version of one we shall consider brieﬂy in §10.6. The eﬀectiveness of the algorithm in this respect cannot be proved mathematically, and there is a serious concern from experts that within a couple of decades quantum computers could crack the current public keys. 10.7 The eﬃciency of RSA In current RSA schemes, n has 2048 bits typically, i.e. more that 600 decimal digits. So how do we compute ae mod n eﬃciently? If n ≈ 10 200 and e ≈ 10 50 , a = 2, we couldn’t even write down ae = 2 1050 . Even working in Z~nZ (i.e. modulo n) would still require e multiplications. So to compute ae , we use fast exponentiation as we saw in previous sections. When e = 2i , then a e = a 2i can be computed using i squaring in Z~nZ; this requires C(log n)2i = C(log e)(log2 n) basic arithmetic operations, where C > 0 is a constant. For general e, write e = (bkbk−1 . . . b0)2 and k = log2 e\u000f. Then, we have ae = k M i=0 bi=1 a 2i. The number of squaring is k = log2 e\u000f = C log e, and the number of multiplications required is #{i ∶ bi = 1} − 1 = C log2 e. So to compute a e mod n requires C(log e)(log2 n) basic arithmetic operations, where C > 0. In practice, one choose the public exponent e to be short, and the private exponent s has full length. In fact, a short exponent s would be insecure. To perform the decryption, one combines fast exponentiation with the Chinese Remainder Theorem. A popular choice of e is the Fermat prime 216 + 1 = 65537. In practice, the numbers p, q, e, s will be converted to base 2, and then divided into 64-bit blocks. These are then displayed using the 64 characters A...Z, a...z, 0...9, + / as well as = and a series of check digits. The RSA algorithm can be also be used to authenticate the sender of ciphertext by providing a digital signature linked to the message, and to enable non-repudiation – Alice can’t deny she was the author of a command send to Bob. A list of public keys in a typical known _hosts folder reveals a mix of ‘ssh-rsa’ and ‘ecdsa- sha2-nistp256’ algorithms. The latter are all based on the elliptic curve 10 CRYPTOGRAPHY 110 y2 = x3 − 3x + 41058363725152142129326129780047268409114441015993725554835256314039467401291 whose study belongs to the realm of number theory and geometry. 10.8 Miller’s test We have seen that the RSA is based on the principle that factoring large composite integers is hard. Current RSA algorithms use integers n that that have around 600 decimals. So, it is natural to ask, how does one ﬁnd big prime numbers? How does one know whether a big number is prime? To determine whether a given integer n is prime, one uses a primality test. There are deterministic and probabilistic primality tests. A deterministic primality test is an algorithm which decides whether a given number n is a prime, while a probabilistic primality test is an algorithm decides whether there is a high probability that a given number is prime. In practice, probabilistic primality tests are faster than deterministic ones, and for everyday cryptographic purposes probabilistic tests suﬃce: if the probability that my banking application is corrupted is less than 10 −1000 , I shouldn’t be worried. In this section we present the so-called Miller-Rabin primality test. It is a probabilistic one. There are other primality tests but we will not see them. Let n = 217. It is patently obvious that n is divisible by 7. Indeed, n = 7 ∗ 31. Observe that n − 1 is divisible by 23, so set k = 2j ∗ 27 with 0 D j D 3. The following table displays the values of a k mod n, for 1 D a D 10 in the range [−108, 108] ∶ a 1 2 3 4 5 6 7 8 9 10 k = 27 1 -27 -8 78 -92 -1 -77 64 64 97 k = 54 1 78 64 8 1 1 70 -27 -27 78 k = 108 1 8 -27 64 1 1 -91 78 78 8 k = 216 1 64 78 -27 1 1 35 8 8 64 If n were prime, by Fermat’s little theorem we would have a n−1 = 1 mod n if 0 < a < n and so the last row would be all 1’s. Moreover, if n is prime and n − 1 = 2m then x = am satisﬁes x2 = 1 mod n, i.e. (x − 1)(x + 1) = 0 mod n, and n must divide x − 1 or x + 1 so either a m = 1 mod n or a m = −1 mod n. Continuing in this way establishes one implication of the following proposition. Proposition 10.12. Let n > 1 be an odd integer. Write n − 1 = 2k ∗ u with u odd. Then the following statements are equivalent. (i) n is a prime number. (ii) For all a ∈ (Z~nZ)∗ one of the following statements holds: • au ≡ 1 mod n, or • there is 0 D i D k − 1 such that a2i∗u ≡ −1 mod n. 10 CRYPTOGRAPHY 111 Proof. Since n − 1 = 2ku with u > 1 odd, k E 1, we can write an − a = a(an−1 − 1) = a(a(n−1)~2 + 1)(a (n−1)~2 − 1) = a(a(n−1)~2 + 1)(a (n−1)~4 − 1)(a (n−1)~4 + 1) \u0005 = a(au − 1) k−1 M i=0(a2iu + 1). So, by forgetting the intermediate equalities, we get a n − a = a(an−1 − 1) = a(a u − 1) k−1 M i=0(a 2iu + 1). (7) If n is an odd prime, then by Fermat’s Little Theorem, a n − a = 0 ∈ Z~nZ, so we have at least one of the following: a ≡ 0 mod n, a u ≡ 1 mod n, or a2i∗u ≡ −1 mod n, for i = 0, 1, . . . , k − 1. From this, it follows that (i) ⇒ (ii). Example 10.13. Let n = 561. Then, n − 1 = 560 = 16 ∗ 35 = 24 ∗ 5 ∗ 7. Let a = 2. Then b0 = 235 = 263 mod 561, b1 = b 2 0 = 166 mod 561, b2 = b 2 1 = 67 mod 561, b3 = b 2 2 = 1 mod 561. Since b3 ≡ 1 mod 561, we conclude that 561 is composite. Moreover, gcd(b2 − 1, 561) = 33 is a non-trivial factor of 561. Indeed, one can verify that 561 = 3 ∗ 11 ∗ 17. Proposition 10.12 leads to the following algorithm. Algorithm 10.14 (Miller-Rabin primality test). Input: n E 3 an integer. Output: probably prime or composite. (1) If n is divisible by 2, 3 or 5, then return composite and stop. (2) Compute n − 1 = 2k ∗ u with u ∈ N odd. (3) Choose a ∈ (Z~nZ)∗ (a ≠ 1). (4) Calculate b = a u mod n. (5) If b = 1 mod n, return probably prime. 10 CRYPTOGRAPHY 112 (6) Loop i = 0, . . . , k − 1: • If b 2 = −1 mod n, return probably prime. • Otherwise, set b = b 2 mod n. (7) Increment i, and return to Step 6. (8) Return composite. We need the following deﬁnition in order to discuss the accuracy of the Miller-Rabin primality test. Deﬁnition 10.15. Let n > 1 be an odd integer such that n − 1 = 2k ∗ u, with u odd. An odd integer n > 1 such that gcd(a, n) = 1 is called a (Miller-Rabin) witness to the compositeness of n, if the following two conditions are true: (a) a u ~≡ 1 mod n; or (b) a 2i∗u ~≡ −1 mod n, for i = 0, 1, . . . , k − 1. The following theorem states that the Miller-Rabin test has a very high probability of deciding whether a given integer is composite. Theorem 10.16. Let n ∈ Z>1 be odd and composite. Then, we have #{a ∶ 1 D a D n − 1, a is a witness for n} n − 1 E 3 4 . So a random a has E 75% chance predicting that n is composite. So if you run the compositeness test with r random choices for a, and n passes, the probability that n is prime is E 1 − 1 4r . In most circumstances, we are happy with such a probable prime. So the Miller-Rabin test is a very good probabilistic primality test. Example 10.17. Let n = 172947529. Then n − 1 = 2 3 ∗ 21618441. Let a = 17. Then, we ﬁnd that 17 21618441 = 1 mod 172947529. So 17 is not a Miller-Rabin witness. Next, letting a = 3, we have 321618441 = −1 mod 172947529. At this point, one might suspect that n is prime. But if we try another value, say a = 23, we ﬁnd that 23 21618441 = 40063806 mod 172947529, 23 2∗21618441 = 2257065 mod 172947529, 23 4∗21618441 = 1 mod 172947529. Thus 23 is a Miller-Rabin witness and n is actually a composite. In fact, n is called a Carmichael number, but it is not easy to factor by hand: n = 307 ∗ 613 ∗ 919. Exercise 10.18. Determine whether 353 passes Miller’s test to base 5. [Answer: Yes, since 352 = 2 ∗ 176 = 2 5 ∗ 11 and one ﬁnds that 5 176 = −1 mod 353. In fact, 353 is prime so it must pass to any base!] 11 CODES* 113 11 Codes* When sending a message, one might wish to: • transmit it eﬃciently – achieved with error-correcting codes; • keep the message private and authenticate the sender – the role of cryptography. Here is the set-up to keep in mind: ALICE BOB message message × × × × Ö encode/encrypt Õ × × × × decode/decrypt codewords € SS € received words We shall consider codes ﬁrst. Bob might receive 104727 IS A MEMORABLE QRIME but the underlined characters are not what Alice wrote. The second error needs both a dictionary (or spell-checker) and a realization that the number is not salty, not criminal, not dirty, nor is it 3 cents. The ﬁrst needs an analysis of primes that diﬀer ‘minimally’ from 104727 (which is itself divisible by 3) 11.1 Check digits The idea here is to assign a check digit or digits to each block of numerical data: The check should involve the whole block, but ideally be small compared to it and easy to compute. It should be able to detect when a common error has occurred, even if it will not correct the error. Examples of some common systems follow. Parity bit. Each block might consist of 7 bits, to which one check bit is added. For example 1010100 ? where ‘ ?’ is chosen so that the overall number of 1’s is even. Here it is 1. More generally x1x2x3x4x5x6x7 x8 must satisfy ∑8 i=1xi = 0 mod 2. This system deﬁnes a set of 27 = 128 code words requiring 8 bits for transmission. It will succeed in detecting an error if exactly one digit x1, . . . , x7 is transcribed wrongly, but it does not ‘see’ the transposition of two bits. 11 CODES* 114 ISBN 10 (International Standard Book Number, pre 2007). This is a 10-digit number, which we can type as x1x2x3x4x5x6x7x8x9x10, in which the last digit x10 is a check. For a number to be valid x1 + 2x2 + 3x3 + \u0005 + 9x9 + 10x10 = 0 mod 11. The check digit is easily determined by the formula x10 = −10x1 = x1 + 2x2 + 3x3 + \u0005 + 9x9 mod 11. If x10 = 10 mod 11, the character ‘X’ is used. For example, the ISBN 10 number of Iris Murdoch’s novel “The Sea, the Sea” (the 1978 Booker prizewinner) is 014118616X. ISBN 10 detects the two commonest errors: (i) a single wrong digit, like 3491234287 instead of 3491242287. (ii) a single adjacent transposition, like 3491224287 instead 3491242287. Let’s verify (i). Suppose that the original 10-digit ‘word’ is x = x1x2x3\u0005x10, but that this is received as y with a error in (say) the second position: y = x1y2x3\u0005x10 . Set f (x) = 10 Q i=1 i xi, so that f (x) = 0 mod 11. Then f (y) = f (x) + 2(y2 − x2) = 2(y2 − x2) mod 11 can’t be zero because 11 is prime. IBAN (International Bank Account Number). The IBANs of a given country have the same number of digits: for example, the UK and Germany have 22, whilst France and Italy have 27. Here is a UK example: GB27 LOYD 3011 2700 1268 86 The two digits (here, 27) after the country code form the check. In general, these two digits represent an integer c satisfying 2 D c D 98 (with c = 2 giving 02 etc). The next four characters obviously indicates the bank (here Lloyds). What follows is the sort-code (30-11-27) and account number (00126886). Digression. The sort-code and account number must pass a separate validation called ‘modulus checking’ that varies from bank to bank (and whether the account it a sterling or euro one!). Lloyds uses a system akin to ISBN 10 with (in our case) the weights shown in the ﬁrst row: 0 0 3 2 9 8 5 2 6 5 4 3 2 1 3 0 1 1 2 7 0 0 1 2 6 8 8 6 0 0 3 2 18 56 0 0 6 10 24 24 16 6 11 CODES* 115 The sum of the numbers in the last row is 165, which is a mltiple of 11, so the account number is valid. Returning to the IBAN, move the ﬁrst four characters to the back and remove spaces: LOYD30112700126886GB76 Now replace any alphabetic letters by its position in the alphabet plus 9 (so A→10,. . . , L→11,. . . , Y→34). This gives the 26-digit number 2124341330112700126886161176 which (as it stands, expressed to base 10) must equal 1 modulo 97, which it does! This condition uniquely speciﬁes the check digits, with the assumption 2 D c D 98. Because 97 is prime, any one of these 97 possibilities can occur. The drawback is that validation requires a computer. ISBN 13 (post 2007). First, some general theory. Suppose that we want to add a single check digit xn to a string x1x2\u0005xn−1, using the rule c0 + c1x1 + \u0005 + cnxn = 0 mod N. In order that xn is determined, we need cn to be coprime to N . For single errors to be detected we also need ci to be coprime to N for all for all i < n. For transpositions to be detected, we need ci − cj coprime to N for all i ≠ j . ISBN 13 is validated by the ‘check function’ f (x) = x1 + 3x2 + x3 + 3x4 + \u0005 + 3x12 + x13 mod 10, but this fails to detect transpositions in which the digits diﬀer by 5, like 27 ↔ 72. For 3xi + xi+1, 3xi+1 + xi, xi + 3xi+1, xi+1 + 3xi are all equal modulo 10, and the check digit will be the same. Luhn algorithm. This is used to determine the ﬁnal digit of credit card numbers. First deﬁne Ã2x = œ 2x if x ∈ {0, 1, 2, 3, 4}, 2x − 9 if x ∈ {5, 6, 7, 8, 9}. The map x ( Ã2x is the permuation (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) z→ (0, 2, 4, 6, 8, 1, 3, 5, 7, 9) with ﬁxed points 0 and 9 (so Ã2x is not quite the obvious residue of 2x modulo 9). For a 16-digit number x = x1\u0005x16 we deﬁne f (x) = Ä2x1 + x2 + Ä2x3 + x4 + \u0005 + Ä2x15 + x16. We then require that f (x) = 0 mod 10. Without the ‘hats’, the function f would not detect transcriptions diﬀering by 5. But with the hats its detects all single transcriptions, and all adjacent transpositions except for 09 ↔ 90 (thought to be less of a problem since 0 and 9 are far apart on the numerical keypad). It also corrects most twin errors ii ↔ jj, but not 22 ↔ 55, 33 ↔ 66 or 44 ↔ 77, since (e.g.) 2 + ̂2 = 5 + ̂5. 11 CODES* 116 11.2 Binary codes These are based on the alphabet B = {0, 1}. Later, it will be important to realize that (equipped with addition modulo 2 and multiplication) this set becomes a ﬁeld. It is commonly denoted Z2, Z~2Z or Z~(2). The problem with the ﬁrst notation is that Z2 also stands for the (inﬁnite) set of p-adic integers with prime p = 2. The other notations are clumsy, so we shall use B or (maybe later) F2 . The Cartesian product B n = B × \u0005 × B is a vector space over the ﬁeld B of dimension n with an obvious basis. We abbreviate (x1, . . . , xn) to x1x2\u0005xn . Deﬁnition 11.1. A binary code C is a set of strings of 0’s and 1’s of length n, i.e. it is a subset of B n . We shall call an element of B n a string or word, and each element of C a codeword. We regard x ∈ B n as ‘valid’ if x belongs to C, which we can think of (for the moment) as a set of valid account numbers expressed in binary. Example 11.2. Take n = 4 and deﬁne C = {0000, 0101, 1010, 1111}. You receive 0111. This is not in <C, so there must be an error. You can compare it to each element of C : received codeword # erroneous digits 0111 0000 3 0111 0101 1 0111 1010 3 0111 1111 1 The original message was likely to have been 0101 or 1111. But that is still two choices – we want to design codes so that there is only one choice. Deﬁnition 11.3. The Hamming distance between two words x, y ∈ B n is the number of bits by which they diﬀer. It is denoted ∂(x, y). This function satisﬁes the properties of a metric in the sense of metric space, including the triangle inequality (for a proof of the latter, see §10.3): ¢¨¨¨¨ ¦ ¨¨¨¨¤ ∂(x, y) = 0 ⇔ x = y ∂(x, y) = ∂(y, x) ∂(x, y) D ∂(x, z) + ∂(z, y). We shall always adopt the Minimum distance (MD) or nearest neighbour principle. If an invalid word x is received, assume that the codeword y transmitted was one for which ∂(x, y) is as small as possible. Example 11.4. Let C = {a = 01101, b = 10110, c = 00011}. Then ∂(b, c) = 3, ∂(c, a) = 3, ∂(a, b) = 4. 11 CODES* 117 If we receive x = 01011, we test ∂(x, a) = 2, ∂(x, b) = 4, ∂(x, c) = 1, so the MD principle tells us to assume that c was transmitted. We want to design C so that each codeword has a unique nearest neighbour (as measured with ∂ ). One might expect to achieve this if the codewords are well dispersed, which amounts to requiring that the distance between any two is suﬃciently large: Deﬁnition 11.5. Let C be a binary code. Its minimum distance is given by δ = min{∂(x, y) ∶ x, y ∈ C, x ≠ y}. Now suppose that δ E 2e + 1. If x ∈ B n and y, y′ ∈ C then ∂(x, y) D e, ∂(x, y′) D e Ô⇒ y = y′. This is an immediate consequence of the triangle inequality: ∂(y, y′) D ∂(y, x) + ∂(x, y′) D 2e < δ, so the deﬁnition of δ implies that y = y′ . As a consequence, we obtain the well-known Lemma 11.6. A binary code with δ E 2e + 1 will correct e errors using the MD principle. Example 11.7. In Example 1, δ = 2 and this is not enough to correct any errors. In Example 2, δ = 3 so one can detect and correct single errors. Lemma 11.8. Let C ⊂ B n be a binary code with δ E 2e + 1. Then SCS−1 + n + › n 2” + \u0005 + › n e”‘ D 2n. Proof. The expression in parentheses on the left-hand side equals the number of elements in B n that are within distance e of a given codeword y . For example, there are n words that diﬀer from y by exactly one digit, and ›n 2” that diﬀer by exactly two digits. If we surround each codeword y by the ‘ball’ or neighbourhood Ne(y) = {y ∈ B n ∶ ∂(x, y) D e}, no two balls can intersect, for Lemma 1 tells us that Ne(y) ∩ Ne(y′) = g. In the next section, we shall study the case e = 1 of ‘1-error correcting codes’ in more detail, for which we need to assume that δ E 3. Lemma 2 implies that SCS(1 + n) D 2n, and equality here would imply that both SCS and n + 1 are powers of 2. We shall show (in §10.4) that such codes do in fact exist. 11 CODES* 118 11.3 Binary linear codes We now specialize the set-up of the previous section to the case in which C is a subspace of B n . This condition makes sense because B n is a vector space over the ﬁnite ﬁeld B = {0, 1} = F2, with coordinate-wise addition. Remember that a word like 010101 really stands for the vector (0, 1, 0, 1, 0, 1). We need not worry about scalar multiplication since 2 = 0 and −1 = 1 in B! So we just need to verify that x, y ∈ C Ô⇒ x + y ∈ C. Any linear code must contain the zero vector 0 = 00\u00050. Moreover, it has a dimension k with k D n, and a basis {x1, \u0005, xk} consisting of k elements. It then follows that C = ı k Q i=1 akxk ∶ ak ∈ B\u0000 has 2k elements. The space B n itself has dimension n and a basis consisting of the vectors {ei} where ei is the vector or word with a 1 in the ith position and zeros elsewhere. Example 11.9. Let C = {000, 111} ⊂ B 3 . The two codewords can be visualized as the opposite vertices of a cube. Notice that δ = 3 and B 3 = N1(000) @ N1(111) is partitioned into two subsets of size 4. This is an example of a repeat code in which each of two messages (0 and 1) is repeated twice to enable correction of 1 error. Words in B n can be thought of as vertices of an n-dimensional hypercube, but this is hard to visualize (at least for n > 4!). Here is a linear code with (n, k, δ) = (5, 2, 3) that we shall return to: C = {00000, 10110, 01011, 11101}. Any two of the nonzero elements form a basis of C . Deﬁnition 11.10. The weight of a word x ∈ B n equals the number of 1’s it has: x = x1\u0005xn Ô⇒ w(x) = n Q i=1 xi. 11 CODES* 119 Recall the previous deﬁnition (of δ ). Lemma 11.11. Given a linear code C, δ = min{w(x) ∶ x ∈ C, x ≠ 0}. So the minimum distance is also minimum nonzero weight in C . Proof. Denote (temporarily) the right-handf side by δ′ . The point is that ∂(x, y) = ∂(x − y, 0) = w(x − y), which holds for any x, y ∈ B n (we could equally well write + in place of −). If the latter belong to C then so does x − y, so δ E δ′ . But w(z) is itself the distance of z from the zero vector 0 ∈ C, so δ′ E δ . We can also use w to prove the triangle inequality for ∂ . If x, y ∈ B n then w(x + y) = n Q i=1 Åxi+yi D n Q i=1 (xi + yi) = w(x) + w(y), where Åxi+yi ∈ {0, 1} stands for the reduction of xi + yi modulo 2. Thus w behaves like a norm on a real vector space, and ∂(x, y) = w(x − y) = w(x − z + z − y) D w(x − z) + w(z − y) = ∂(x, z) + ∂(z, y). Key example to illustrate the theory. The ﬁrst two nonzero elements of the previous example C ⊂ B 5 correspond to the columns of the matrix E = ™ Œ Œ Œ Œ Œ Œ ﬂ 1 0 0 1 1 0 1 1 0 1 ﬁ Š Š Š Š Š Š Ł = ™ Œ ﬂ I2 A ﬁ Š Ł , where In denotes the n × n identity matrix. Our convention is that matrices always act on the left on column vectors, so this deﬁnes a linear transformation E∶ B 2 Ð→ B 5. It follows that C = Im E, and each element of C has the form Ev = ™ ﬂ v Av ﬁ Ł , where v is one of west = „ 0 0‚ , north = „ 1 0‚ , south = „ 0 1‚ , east = „ 1 1‚ . 11 CODES* 120 We shall freely transpose from rows to columns, using the latter when we need to act on them by matrices. With this confusion, Ev = v Av . Seen this way, Av plays the role of a check block for each of the four possibilities for v, which might be commands for a robot to move. Observe that here the check is longer than the original message. This is to enable error correction rather than mere detection: since δ = 3, the block ‘protects’ the direction in the event it is corrupted by 90 degrees. We can describe C in an equivalent way, using the matrix H = ™ Œ ﬂ 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 ﬁ Š Ł = − A U I 3 ‘. For let x = „ b c ‚ ∈ B 5, with b ∈ B 2 and c ∈ B 3 . Then Hx = 0 ⇔ Ab + c = 0 ⇔ Ab = c ⇔ x ∈ Im E = C. The matrix H is called the parity-check or check matrix of the linear code C . Deﬁnition 11.12. Suppose that r < n. Let H be a matrix of size r × n and entries in B (one can express this by writing H ∈ B r,n and r is the number of rows). Then the subspace ker H = {x ∈ B n ∶ Hx = 0} of B n is called the linear code with check matrix H . We shall always assume that r =⇒ nkH, since if not we can delete one or more rows without aﬀecting the kernel. Example 11.13. Take r = 1 and H to be the single row with all 1’s. Then C consists of all the elements of B n of even weight. We could regard the ﬁrst n − 1 bits of x ∈ Bn as the ‘message’, and the ﬁnal bit xn as a parity check digit, as in §10.1. 11.4 Correcting one error We have already used three parameters to help describe a linear code: n = number of bits in transmission k = n − r = dimension, so SCS = 2k δ = minimum distance between codewords. Suppose that we need to correct one error in a transmitted message block. This requires a code (linear or not) with δ E 2e + 1 = 3, and by Lemma 2 from §10.2, s(1 + n) D 2n, 11 CODES* 121 where s = SCS (s for size). A big question is Given s, n satisfying this inequality, does there exist C ⊂ B n with δ = 3 and SCS = s? Easy exercise. There is no code (linear or not) with SCS = 3, n = 4 and δ = 3. At the risk of repetition, let’s summarize the deﬁnition of linear codes using matrices. Such a code is often deﬁned by a check matrix H of size r × n with r < n. Then the set of codewords is C = {xŠ ∶ Hx = 0} ⊂ B n. We naturally regard a word as a string written as a row, but it is always transposed to a column vector for the check matrix to test it. We shall usually omit the transpose symbol Š, since context makes it clear whether one is dealing with a row or a column. So C = ker H, i.e. C is the kernel of the linear transformation B n Ð→ B r x z→ Hx. We assume that ⇒ nkH = r, so that dim C = n − r . We call this dimension k, so that there are 2k codewords. To make clearer the analogy with check digits, one often takes H = − A U I r ‘ so that the last block is the identity matrix. In this case, H is said to be in standard form, but this is not always convenient. Note that A has n − r = k columns. We can deﬁne an ‘encoding matrix’ E = ™ ﬂ Ik A ﬁ Ł . By multiplying the blocks, we see that HE = AIk + IrA = A + A = 0 is the zero matrix (of size r × k ). This means that H annihilates the k columns of E, which must therefore lie in C . But these k columns are linearly independent because they include the columns of Ik, and they span the image of E∶ B k → B n . Conclusion. C = ker H = Im E, so as a row any codeword can be written v Av . Some authors would (correctly) express this as (r, rAŠ) having preferred to make explicit the row vector r = vŠ and having choden to use EŠ instead of E . Exercise. Suppose that C = ker H, where 11 CODES* 122 H = ™ Œ Œ Œ Œ Œ Œ Œ Œ ﬂ 1 0 1 0 1 0 1 1 0 1 0 1 0 1 RRRRRRRRRRRRRRRRRRRRRRRRRRRRR I 7 ﬁ Š Š Š Š Š Š Š Š Ł . What is the size of C ? How many errors does it correct?. Lemma 11.14. Let H be the check matrix of a linear code C . Then δ E 3 (so C corrects at least one error) provided no column of H is zero and no two columns are equal. Moreover, if x diﬀers from a codeword y by just one bit in the ith position (i.e. x = y + ei ), then Hx is the ith column of H . Proof. We need to ensure that C has no words of weight 1 or 2. A word of weight one means it is ei for some i, and Hi = Hei is the ith column of H . So this must be nonzero. Similarly, a word x of weight 2 must equal ei + ej with i ≠ j, and so Hx = Hei + Hej = Hi + Hj = Hi − Hj must be nonzero. Finally, if x = y + ei with y ∈ C then Hx = Hi . Example 11.15. The matrix H = ™ Œ ﬂ 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 ﬁ Š Ł obviously has rank 3, so deﬁnes a linear code of dimension 7 − 3 = 4. Its parameters are (n, k, δ) = (7, 4, 3). If x diﬀers from a codeword only in the ith position then Hx (transposed to a row) is conveniently the binary expansion of i! If Hx is nozero, it is called the syndrome of the word x. The best way to modify H so that the identity matrix appears on the right is to perform row operations (as for echelon form) because this will not change the kernel of the matrix. We take ¢¨¨¨ ¦ ¨¨¨¤ r′ 1 = r1 + r2 r′ 2 = r1 + r3 r′ 3 = r1 + r2 + r3 to form H ′ = ™ Œ ﬂ 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 ﬁ Š Ł . 11 CODES* 123 The encoding matrix associated to H ′ is E = ™ Œ Œ Œ Œ Œ Œ Œ Œ Œ Œ Œ ﬂ 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 ﬁ Š Š Š Š Š Š Š Š Š Š Š Ł , and any codeword then has the form v Av ∈ B 4+3 . This time, the check block is smaller than the original message v . To quantify this fact, one deﬁnes the information rate of the code as ρ = k n „= lg SCS n if C is not linear‚ . Here we have ρ = 4~7 ∼ 0.57. Exercise 11.16. Explain in what sense the code in the previous example can detect up to two errors, if it does not have to correct one! Deﬁnition 11.17. Let H be a matrix whose columns are all 2 r−1 nonzero words formed from k bits. The linear code C = ker H is called a Hamming code; it has parameters (2 r −1, 2 r −r−1, 3). Any two Hamming codes of the same size (SCS = 22r−r−1 ) are essentially equivalent, because permuting the columns will merely permute all the bits in C . When r = 2, we can take H = „ 1 1 0 1 0 1 ‚ , E = ™ Œ ﬂ 1 1 1 ﬁ Š Ł , so as to recover the repeat code C = {000, 111} ⊂ B 3 . Hamming codes are perfect, meaning that we have equality in Lemma 2 in §10.2. Another way of saying this is that the balls Ne(y) = {x ∈ B n ∶ ∂(x, y) D e} partition C as y ranges over C . For the Hamming code in B n, we have 2 k balls each of size 1 + n = 2 r . This was alluded to at the end of §10.2. The information rate of a Hamming code is ρ = k n = 2r − r − 1 2r − 1 = 1 − (r + 1)2−1 1 − 2−r → 1 as r → ∞. Already for r = 6 (n = 63) we have ρ > 0.9. Apart from the Hamming codes, codes of size 1 and repeat codes {0\u00050, 1\u00051} ⊂ B n of size 2 with n odd, there is just one other perfect code. This is the mysterious Golay code G23, a binary linear code with parameters (23, 12, 7).","libVersion":"0.2.2","langs":""}